[
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html",
    "href": "weekly-notes/weekotes qmd.html",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We observe data such as counties, income, population, education, etc.\nWe believe there’s some relationship between these variables.\nStatistical learning = a set of approaches for estimating that relationship."
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#the-general-problem",
    "href": "weekly-notes/weekotes qmd.html#the-general-problem",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We observe data such as counties, income, population, education, etc.\nWe believe there’s some relationship between these variables.\nStatistical learning = a set of approaches for estimating that relationship."
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#formalizing-the-relationship",
    "href": "weekly-notes/weekotes qmd.html#formalizing-the-relationship",
    "title": "Week 5 Notes",
    "section": "Formalizing the Relationship",
    "text": "Formalizing the Relationship\nFor any quantitative response Y and predictors X₁, X₂, … Xₚ:\n\\[\nY = f(X) + \\epsilon\n\\]\nWhere:\n\nf = the systematic information X provides about Y\n\nε = random error (irreducible)"
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#what-is-f",
    "href": "weekly-notes/weekotes qmd.html#what-is-f",
    "title": "Week 5 Notes",
    "section": "What is f?",
    "text": "What is f?\nf represents the true relationship between predictors and the outcome.\n\nIt’s fixed but unknown — what we’re trying to estimate.\n\nDifferent X values produce different Y values through f.\n\nExample:\n\nY = median income\n\nX = population, education, poverty rate\n\nf = the way these factors systematically relate to income"
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#why-estimate-f",
    "href": "weekly-notes/weekotes qmd.html#why-estimate-f",
    "title": "Week 5 Notes",
    "section": "Why Estimate f?",
    "text": "Why Estimate f?\nTwo main reasons:\n\nPrediction\n\nEstimate Y for new observations\n\nFocus on accuracy, not interpretation\n\nInference\n\nUnderstand how X affects Y\n\nIdentify which predictors matter\n\nFocus on interpretation"
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#how-do-we-estimate-f",
    "href": "weekly-notes/weekotes qmd.html#how-do-we-estimate-f",
    "title": "Week 5 Notes",
    "section": "How Do We Estimate f?",
    "text": "How Do We Estimate f?\n\nParametric Methods\n\nAssume a specific functional form (e.g., linear)\nEasier to interpret and compute\nFocus of this week’s session\n\n\n\nNon-Parametric Methods\n\nMake fewer assumptions\n\nMore flexible but require more data and are harder to interpret\n\nKey difference:\n- Parametric (blue): Assume f is linear → estimate coefficients\n- Non-parametric (green): Let data determine the shape of f"
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#parametric-approach-linear-regression",
    "href": "weekly-notes/weekotes qmd.html#parametric-approach-linear-regression",
    "title": "Week 5 Notes",
    "section": "Parametric Approach: Linear Regression",
    "text": "Parametric Approach: Linear Regression\nAssumption: Relationship between X and Y is linear\n\\[\nY ≈ β₀ + β₁X₁ + β₂X₂ + … + βₚXₚ\n\\]\nWe estimate the β coefficients using Ordinary Least Squares (OLS)."
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#why-linear-regression",
    "href": "weekly-notes/weekotes qmd.html#why-linear-regression",
    "title": "Week 5 Notes",
    "section": "Why Linear Regression?",
    "text": "Why Linear Regression?\nAdvantages - Simple and interpretable\n- Foundation for many other models\n- Performs well in practice\nLimitations - Assumes linearity\n- Sensitive to outliers\n- Relies on several assumptions (we’ll check them)"
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#prediction-vs-inference",
    "href": "weekly-notes/weekotes qmd.html#prediction-vs-inference",
    "title": "Week 5 Notes",
    "section": "Prediction vs Inference",
    "text": "Prediction vs Inference\nInference - “Does education affect income?”\n- Focus: coefficients, significance, mechanisms\nPrediction - “What’s County Y’s income?”\n- Focus: accuracy, prediction intervals\nToday: we’ll do both, but emphasize prediction."
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#example-prediction-policy",
    "href": "weekly-notes/weekotes qmd.html#example-prediction-policy",
    "title": "Week 5 Notes",
    "section": "Example: Prediction (Policy)",
    "text": "Example: Prediction (Policy)\nGovernment use case:\n\nCensus may miss people in hard-to-count areas\n\nPredict income or population for planning resources\n\nEven if the model doesn’t explain why, good predictions help policy."
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#example-inference-research",
    "href": "weekly-notes/weekotes qmd.html#example-inference-research",
    "title": "Week 5 Notes",
    "section": "Example: Inference (Research)",
    "text": "Example: Inference (Research)\nResearch use case:\n- Understanding gentrification\n- Which neighborhood characteristics explain income change?\n- How much does education matter vs. proximity to downtown?\n- Are policy interventions associated with outcomes?"
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#connection-to-week-2-algorithmic-bias",
    "href": "weekly-notes/weekotes qmd.html#connection-to-week-2-algorithmic-bias",
    "title": "Week 5 Notes",
    "section": "Connection to Week 2: Algorithmic Bias",
    "text": "Connection to Week 2: Algorithmic Bias\nRemember the healthcare algorithm that discriminated?\n\nIt predicted healthcare needs using costs as a proxy.\n\nTechnically accurate (good R²) but ethically biased.\n\nLesson: A good fit ≠ a fair model."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "[List main concepts from lecture] Repository (repo): Folder containing your project files Commit: Snapshot of your work at a point in time Push: Send your changes to GitHub cloud Pull: Get latest changes from GitHub cloud\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "[List main concepts from lecture] Repository (repo): Folder containing your project files Commit: Snapshot of your work at a point in time Push: Send your changes to GitHub cloud Pull: Get latest changes from GitHub cloud\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches]\n[Quarto features learned]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n[What I didn’t fully understand]\n[Areas needing more practice]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week’s content applies to real policy work]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting]\n[How I’ll apply this knowledge]"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes",
    "section": "",
    "text": "[List main concepts from lecture] Repository (repo): Folder containing your project files Commit: Snapshot of your work at a point in time Push: Send your changes to GitHub cloud Pull: Get latest changes from GitHub cloud\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 Notes",
    "section": "",
    "text": "[List main concepts from lecture] Repository (repo): Folder containing your project files Commit: Snapshot of your work at a point in time Push: Send your changes to GitHub cloud Pull: Get latest changes from GitHub cloud\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches] Powerful for spatial analysis and policy-focused statistics\n[Quarto features learned] Quarto is the “next generation” Better website creation Works with multiple programming languages Same basic concept, improved features"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n[What I didn’t fully understand]\n[Areas needing more practice]"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week’s content applies to real policy work] Free and open source Excellent for spatial data Strong statistical capabilities Large community in urban planning/policy Reproducible research workflows"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting]\n[How I’ll apply this knowledge]"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes",
    "section": "",
    "text": "[List main concepts from lecture] – Anscombe’s Quartet and the limits of summary statistics – Visualization in policy context – Connection to algorithmic bias and data ethics – ggplot2 fundamentals – Aesthetic mappings and geoms – Live demonstration – EDA workflow and principles – Understanding distributions and relationships – Critical focus: Data quality and uncertainty\n[Technical skills covered] ggplot(data = your_data) + aes(x = variable1, y = variable2) + geom_something() + additional_layers()"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "title": "Week 3 Notes",
    "section": "",
    "text": "[List main concepts from lecture] – Anscombe’s Quartet and the limits of summary statistics – Visualization in policy context – Connection to algorithmic bias and data ethics – ggplot2 fundamentals – Aesthetic mappings and geoms – Live demonstration – EDA workflow and principles – Understanding distributions and relationships – Critical focus: Data quality and uncertainty\n[Technical skills covered] ggplot(data = your_data) + aes(x = variable1, y = variable2) + geom_something() + additional_layers()"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches]\n[Quarto features learned]"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n[What I didn’t fully understand]\n[Areas needing more practice]"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week’s content applies to real policy work] – Summary statistics can hide critical patterns – Outliers may represent important communities – Relationships aren’t always linear – Visual inspection reveals data quality issues"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3 Notes",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting]\n[How I’ll apply this knowledge]"
  },
  {
    "objectID": "Assignments/Midterm/final_slides.html#which-features-matter-most",
    "href": "Assignments/Midterm/final_slides.html#which-features-matter-most",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Which Features Matter Most?",
    "text": "Which Features Matter Most?\n\n\n\n\n\n\n\n\nFeature\nDirection\nInterpretation\n\n\n\n\nLiving area\n↑\nStrongest driver of housing price\n\n\nAge + Age²\n↓ then ↑\nU-shaped pattern — older historic homes regain value\n\n\nExterior good\n↑\nMaintenance condition positively impacts price\n\n\nMedian income / Education\n↑\nSocioeconomic context drives demand\n\n\nPoverty rate / Crime\n↓\nNegative neighborhood effects\n\n\nInteraction: Living area × Wealthy neighborhood\n↓\nLarger homes add less"
  },
  {
    "objectID": "Assignments/Midterm/final_slides.html#thank-you-for-listening",
    "href": "Assignments/Midterm/final_slides.html#thank-you-for-listening",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Thank you for listening",
    "text": "Thank you for listening\nAny questions?"
  },
  {
    "objectID": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html",
    "href": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#assignment-overview",
    "href": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data: - Pennsylvania county boundaries - Pennsylvania hospitals (from lecture data) - Pennsylvania census tracts\nYour Task:\n\n# Load required packages\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(units)\nlibrary(stringr)\nlibrary(knitr)\nlibrary(scales)\nlibrary(readr)\nlibrary(kableExtra)\n\noptions(tigris_use_cache=TRUE)\noptions(tigris_progress=FALSE)\n# Load spatial data\npa_counties=st_read(\"/Users/cathy/GitHub/MUSA-5080-Fall-2025/lectures/week-04/data/Pennsylvania_County_Boundaries.shp\")\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `/Users/cathy/GitHub/MUSA-5080-Fall-2025/lectures/week-04/data/Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\npa_hospital=st_read(\"/Users/cathy/GitHub/MUSA-5080-Fall-2025/lectures/week-04/data/hospitals.geojson\")\n\nReading layer `hospitals' from data source \n  `/Users/cathy/GitHub/MUSA-5080-Fall-2025/lectures/week-04/data/hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\ncensus_tracts &lt;- tracts(state=\"PA\", cb=TRUE)\n\n# Check that all data loaded correctly\nhead(pa_counties)\n\nSimple feature collection with 6 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8905670 ymin: 4862594 xmax: -8317930 ymax: 5161279\nProjected CRS: WGS 84 / Pseudo-Mercator\n  OBJECTID MSLINK COUNTY_NAM COUNTY_NUM FIPS_COUNT COUNTY_ARE COUNTY_PER\n1      336     46 MONTGOMERY         46        091       &lt;NA&gt;       &lt;NA&gt;\n2      337      8   BRADFORD         08        015       &lt;NA&gt;       &lt;NA&gt;\n3      338      9      BUCKS         09        017       &lt;NA&gt;       &lt;NA&gt;\n4      339     58      TIOGA         58        117       &lt;NA&gt;       &lt;NA&gt;\n5      340     59      UNION         59        119       &lt;NA&gt;       &lt;NA&gt;\n6      341     60    VENANGO         60        121       &lt;NA&gt;       &lt;NA&gt;\n  NUMERIC_LA COUNTY_N_1 AREA_SQ_MI SOUND SPREAD_SHE IMAGE_NAME NOTE_FILE VIDEO\n1          5         46   487.4271  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n2          2          8  1161.3379  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n3          5          9   622.0836  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n4          2         58  1137.2480  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n5          2         59   319.1893  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n6          3         60   683.3676  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n  DISTRICT_N PA_CTY_COD MAINT_CTY_ DISTRICT_O                       geometry\n1         06         46          4        6-4 MULTIPOLYGON (((-8398884 48...\n2         03         08          9        3-9 MULTIPOLYGON (((-8558633 51...\n3         06         09          1        6-1 MULTIPOLYGON (((-8367360 49...\n4         03         59          7        3-7 MULTIPOLYGON (((-8558633 51...\n5         03         60          8        3-8 MULTIPOLYGON (((-8562865 49...\n6         01         61          5        1-5 MULTIPOLYGON (((-8870781 50...\n\nhead(pa_hospital)\n\nSimple feature collection with 6 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.27907 ymin: 39.80913 xmax: -75.17005 ymax: 40.24273\nGeodetic CRS:  WGS 84\n                 CHIEF_EXEC              CHIEF_EX_1\n1             Peter J Adamo               President\n2          Autumn DeShields Chief Executive Officer\n3              Shawn Parekh Chief Executive Officer\n4               DIANE HRITZ Chief Executive Officer\n5            Tim Harclerode Chief Executive Officer\n6 Richard McLaughlin MD MBA Chief Executive Officer\n                      FACILITY_U LONGITUDE       COUNTY\n1   https://www.phhealthcare.org -79.91131   Washington\n2      https://www.malvernbh.com -75.17005 Philadelphia\n3 https://roxboroughmemorial.com -75.20963 Philadelphia\n4     https://www.ashospital.net -80.27907   Washington\n5      https://www.conemaugh.org -79.02513     Somerset\n6        https://towerhealth.org -75.61213   Montgomery\n                               FACILITY_N                         STREET\n1               Penn Highlands Mon Valley         1163 Country Club Road\n2               MALVERN BEHAVIORAL HEALTH 1930 South Broad Street Unit 4\n3            Roxborough Memorial Hospital              5800 Ridge Avenue\n4              ADVANCED SURGICAL HOSPITAL       100 TRICH DRIVE\\nSUITE 1\n5 DLP Conemaugh Meyersdale Medical Center             200 Hospital Drive\n6                 Pottstown Hospital, LLC          1600 East High Street\n    CITY_OR_BO LATITUDE   TELEPHONE_ ZIP_CODE                   geometry\n1  Monongahela 40.18193 724-258-1000    15063 POINT (-79.91131 40.18193)\n2 Philadelphia 39.92619 610-480-8919    19145  POINT (-75.17005 39.9262)\n3 Philadelphia 40.02869 215-483-9900    19128 POINT (-75.20963 40.02869)\n4   WASHINGTON 40.15655   7248840710    15301 POINT (-80.27907 40.15655)\n5   Meyersdale 39.80913 814-634-5911    15552 POINT (-79.02513 39.80913)\n6    Pottstown 40.24273   6103277000    19464 POINT (-75.61213 40.24273)\n\nhead(census_tracts)\n\nSimple feature collection with 6 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -78.42478 ymin: 39.79351 xmax: -75.93766 ymax: 40.54328\nGeodetic CRS:  NAD83\n  STATEFP COUNTYFP TRACTCE              GEOIDFQ       GEOID   NAME\n1      42      001  031101 1400000US42001031101 42001031101 311.01\n2      42      013  100400 1400000US42013100400 42013100400   1004\n3      42      013  100500 1400000US42013100500 42013100500   1005\n4      42      013  100800 1400000US42013100800 42013100800   1008\n5      42      013  101900 1400000US42013101900 42013101900   1019\n6      42      011  011200 1400000US42011011200 42011011200    112\n             NAMELSAD STUSPS   NAMELSADCO   STATE_NAME LSAD   ALAND AWATER\n1 Census Tract 311.01     PA Adams County Pennsylvania   CT 3043185      0\n2   Census Tract 1004     PA Blair County Pennsylvania   CT  993724      0\n3   Census Tract 1005     PA Blair County Pennsylvania   CT 1130204      0\n4   Census Tract 1008     PA Blair County Pennsylvania   CT  996553      0\n5   Census Tract 1019     PA Blair County Pennsylvania   CT  573726      0\n6    Census Tract 112     PA Berks County Pennsylvania   CT 1539365   9308\n                        geometry\n1 MULTIPOLYGON (((-77.03108 3...\n2 MULTIPOLYGON (((-78.42478 4...\n3 MULTIPOLYGON (((-78.41661 4...\n4 MULTIPOLYGON (((-78.41067 4...\n5 MULTIPOLYGON (((-78.40836 4...\n6 MULTIPOLYGON (((-75.95433 4...\n\n\n\n\nCRS summary — Counties: WGS 84 / Pseudo-Mercator ; Hospitals: WGS 84 ; Tracts: NAD83 \n\n\nQuestions to answer: - How many hospitals are in your dataset? 219 - How many census tracts? 3445 - What coordinate reference system is each dataset in? Counties: WGS 84 / Pseudo-Mercator ; Hospitals: WGS 84 ; Tracts: NAD83 —\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables: - Total population - Median household income - Population 65 years and over (you may need to sum multiple age categories)\nYour Task:\n\n# Get demographic data from ACS\ndata=c(over65=\"DP05_0024\",income=\"DP03_0062\",population=\"DP02_0018\")\nd1=get_acs(\n  state = \"PA\",\n  geography = \"tract\",\n  year = 2023,\n  survey = \"acs5\",\n  variables = data,\n  geometry = TRUE,\n  output=\"wide\"\n)\n\n# Join to tract boundaries\nd1 &lt;- left_join(census_tracts, st_drop_geometry(d1), by = \"GEOID\")\n\nhead(d1)\n\nSimple feature collection with 6 features and 20 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -78.42478 ymin: 39.79351 xmax: -75.93766 ymax: 40.54328\nGeodetic CRS:  NAD83\n  STATEFP COUNTYFP TRACTCE              GEOIDFQ       GEOID NAME.x\n1      42      001  031101 1400000US42001031101 42001031101 311.01\n2      42      013  100400 1400000US42013100400 42013100400   1004\n3      42      013  100500 1400000US42013100500 42013100500   1005\n4      42      013  100800 1400000US42013100800 42013100800   1008\n5      42      013  101900 1400000US42013101900 42013101900   1019\n6      42      011  011200 1400000US42011011200 42011011200    112\n             NAMELSAD STUSPS   NAMELSADCO   STATE_NAME LSAD   ALAND AWATER\n1 Census Tract 311.01     PA Adams County Pennsylvania   CT 3043185      0\n2   Census Tract 1004     PA Blair County Pennsylvania   CT  993724      0\n3   Census Tract 1005     PA Blair County Pennsylvania   CT 1130204      0\n4   Census Tract 1008     PA Blair County Pennsylvania   CT  996553      0\n5   Census Tract 1019     PA Blair County Pennsylvania   CT  573726      0\n6    Census Tract 112     PA Berks County Pennsylvania   CT 1539365   9308\n                                           NAME.y over65E over65M incomeE\n1 Census Tract 311.01; Adams County; Pennsylvania     995     261   64985\n2   Census Tract 1004; Blair County; Pennsylvania     255     101   68929\n3   Census Tract 1005; Blair County; Pennsylvania     447      89   50241\n4   Census Tract 1008; Blair County; Pennsylvania     243      75   73625\n5   Census Tract 1019; Blair County; Pennsylvania     578     121   16547\n6    Census Tract 112; Berks County; Pennsylvania     496     111   61974\n  incomeM populationE populationM                       geometry\n1   10752        4865         412 MULTIPOLYGON (((-77.03108 3...\n2   17235        1660         247 MULTIPOLYGON (((-78.42478 4...\n3    5060        3360         562 MULTIPOLYGON (((-78.41661 4...\n4   18161        1490         333 MULTIPOLYGON (((-78.41067 4...\n5    1547        1247         184 MULTIPOLYGON (((-78.40836 4...\n6    6269        4123          38 MULTIPOLYGON (((-75.95433 4...\n\nst_geometry_type(d1)\n\n   [1] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n   [6] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [11] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [16] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [21] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [26] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [31] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [36] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [41] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [46] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [51] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [56] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [61] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [66] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [71] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [76] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [81] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [86] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [91] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [96] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [101] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [106] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [111] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [116] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [121] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [126] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [131] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [136] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [141] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [146] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [151] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [156] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [161] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [166] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [171] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [176] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [181] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [186] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [191] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [196] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [201] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [206] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [211] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [216] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [221] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [226] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [231] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [236] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [241] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [246] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [251] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [256] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [261] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [266] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [271] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [276] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [281] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [286] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [291] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [296] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [301] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [306] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [311] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [316] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [321] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [326] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [331] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [336] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [341] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [346] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [351] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [356] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [361] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [366] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [371] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [376] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [381] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [386] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [391] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [396] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [401] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [406] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [411] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [416] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [421] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [426] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [431] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [436] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [441] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [446] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [451] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [456] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [461] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [466] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [471] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [476] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [481] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [486] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [491] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [496] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [501] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [506] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [511] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [516] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [521] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [526] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [531] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [536] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [541] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [546] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [551] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [556] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [561] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [566] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [571] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [576] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [581] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [586] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [591] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [596] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [601] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [606] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [611] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [616] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [621] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [626] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [631] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [636] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [641] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [646] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [651] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [656] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [661] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [666] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [671] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [676] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [681] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [686] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [691] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [696] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [701] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [706] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [711] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [716] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [721] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [726] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [731] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [736] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [741] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [746] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [751] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [756] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [761] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [766] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [771] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [776] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [781] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [786] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [791] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [796] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [801] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [806] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [811] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [816] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [821] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [826] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [831] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [836] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [841] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [846] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [851] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [856] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [861] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [866] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [871] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [876] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [881] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [886] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [891] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [896] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [901] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [906] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [911] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [916] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [921] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [926] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [931] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [936] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [941] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [946] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [951] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [956] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [961] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [966] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [971] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [976] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [981] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [986] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [991] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [996] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1001] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1006] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1011] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1016] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1021] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1026] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1031] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1036] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1041] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1046] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1051] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1056] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1061] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1066] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1071] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1076] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1081] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1086] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1091] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1096] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1101] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1106] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1111] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1116] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1121] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1126] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1131] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1136] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1141] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1146] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1151] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1156] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1161] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1166] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1171] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1176] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1181] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1186] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1191] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1196] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1201] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1206] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1211] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1216] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1221] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1226] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1231] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1236] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1241] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1246] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1251] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1256] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1261] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1266] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1271] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1276] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1281] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1286] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1291] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1296] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1301] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1306] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1311] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1316] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1321] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1326] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1331] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1336] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1341] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1346] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1351] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1356] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1361] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1366] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1371] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1376] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1381] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1386] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1391] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1396] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1401] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1406] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1411] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1416] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1421] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1426] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1431] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1436] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1441] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1446] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1451] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1456] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1461] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1466] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1471] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1476] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1481] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1486] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1491] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1496] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1501] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1506] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1511] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1516] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1521] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1526] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1531] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1536] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1541] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1546] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1551] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1556] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1561] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1566] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1571] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1576] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1581] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1586] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1591] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1596] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1601] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1606] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1611] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1616] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1621] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1626] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1631] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1636] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1641] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1646] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1651] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1656] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1661] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1666] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1671] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1676] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1681] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1686] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1691] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1696] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1701] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1706] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1711] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1716] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1721] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1726] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1731] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1736] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1741] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1746] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1751] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1756] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1761] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1766] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1771] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1776] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1781] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1786] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1791] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1796] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1801] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1806] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1811] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1816] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1821] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1826] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1831] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1836] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1841] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1846] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1851] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1856] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1861] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1866] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1871] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1876] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1881] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1886] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1891] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1896] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1901] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1906] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1911] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1916] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1921] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1926] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1931] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1936] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1941] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1946] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1951] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1956] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1961] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1966] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1971] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1976] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1981] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1986] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1991] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1996] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2001] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2006] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2011] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2016] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2021] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2026] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2031] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2036] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2041] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2046] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2051] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2056] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2061] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2066] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2071] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2076] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2081] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2086] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2091] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2096] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2101] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2106] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2111] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2116] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2121] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2126] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2131] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2136] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2141] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2146] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2151] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2156] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2161] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2166] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2171] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2176] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2181] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2186] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2191] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2196] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2201] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2206] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2211] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2216] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2221] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2226] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2231] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2236] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2241] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2246] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2251] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2256] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2261] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2266] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2271] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2276] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2281] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2286] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2291] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2296] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2301] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2306] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2311] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2316] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2321] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2326] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2331] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2336] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2341] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2346] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2351] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2356] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2361] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2366] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2371] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2376] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2381] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2386] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2391] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2396] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2401] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2406] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2411] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2416] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2421] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2426] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2431] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2436] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2441] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2446] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2451] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2456] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2461] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2466] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2471] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2476] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2481] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2486] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2491] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2496] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2501] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2506] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2511] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2516] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2521] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2526] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2531] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2536] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2541] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2546] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2551] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2556] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2561] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2566] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2571] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2576] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2581] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2586] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2591] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2596] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2601] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2606] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2611] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2616] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2621] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2626] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2631] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2636] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2641] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2646] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2651] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2656] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2661] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2666] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2671] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2676] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2681] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2686] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2691] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2696] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2701] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2706] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2711] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2716] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2721] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2726] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2731] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2736] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2741] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2746] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2751] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2756] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2761] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2766] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2771] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2776] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2781] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2786] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2791] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2796] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2801] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2806] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2811] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2816] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2821] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2826] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2831] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2836] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2841] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2846] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2851] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2856] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2861] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2866] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2871] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2876] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2881] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2886] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2891] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2896] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2901] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2906] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2911] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2916] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2921] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2926] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2931] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2936] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2941] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2946] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2951] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2956] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2961] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2966] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2971] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2976] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2981] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2986] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2991] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2996] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3001] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3006] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3011] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3016] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3021] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3026] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3031] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3036] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3041] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3046] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3051] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3056] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3061] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3066] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3071] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3076] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3081] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3086] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3091] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3096] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3101] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3106] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3111] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3116] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3121] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3126] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3131] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3136] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3141] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3146] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3151] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3156] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3161] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3166] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3171] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3176] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3181] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3186] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3191] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3196] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3201] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3206] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3211] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3216] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3221] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3226] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3231] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3236] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3241] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3246] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3251] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3256] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3261] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3266] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3271] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3276] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3281] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3286] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3291] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3296] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3301] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3306] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3311] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3316] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3321] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3326] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3331] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3336] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3341] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3346] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3351] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3356] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3361] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3366] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3371] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3376] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3381] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3386] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3391] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3396] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3401] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3406] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3411] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3416] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3421] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3426] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3431] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3436] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3441] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\n\n\n# Check for missing income values\nsum(is.na(d1$incomeE))\n\n[1] 65\n\n# Calculate the median income across all tracts\nmedian_income &lt;- median(d1$incomeE, na.rm = TRUE)\nmedian_income\n\n[1] 72943.5\n\n\nQuestions to answer: - What year of ACS data are you using? 2023 ACS 5-Year Estimates. - How many tracts have missing income data? 65 - What is the median income across all PA census tracts? 72943.5 —\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria: 1. Low median household income (choose an appropriate threshold) 2. Significant elderly population (choose an appropriate threshold)\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\nsummary(d1)\n\n   STATEFP            COUNTYFP           TRACTCE            GEOIDFQ         \n Length:3445        Length:3445        Length:3445        Length:3445       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    GEOID              NAME.x            NAMELSAD            STUSPS         \n Length:3445        Length:3445        Length:3445        Length:3445       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  NAMELSADCO         STATE_NAME            LSAD               ALAND          \n Length:3445        Length:3445        Length:3445        Min.   :7.224e+04  \n Class :character   Class :character   Class :character   1st Qu.:1.320e+06  \n Mode  :character   Mode  :character   Mode  :character   Median :4.183e+06  \n                                                          Mean   :3.364e+07  \n                                                          3rd Qu.:2.858e+07  \n                                                          Max.   :1.024e+09  \n                                                                             \n     AWATER            NAME.y             over65E          over65M     \n Min.   :       0   Length:3445        Min.   :   0.0   Min.   :  3.0  \n 1st Qu.:       0   Class :character   1st Qu.: 429.0   1st Qu.: 94.0  \n Median :   13905   Mode  :character   Median : 665.0   Median :138.0  \n Mean   :  433440                      Mean   : 718.8   Mean   :156.3  \n 3rd Qu.:  240806                      3rd Qu.: 945.0   3rd Qu.:199.0  \n Max.   :29792870                      Max.   :2541.0   Max.   :826.0  \n                                                                       \n    incomeE          incomeM        populationE     populationM    \n Min.   : 13307   Min.   :   472   Min.   :    0   Min.   :   7.0  \n 1st Qu.: 57864   1st Qu.:  9324   1st Qu.: 2508   1st Qu.: 247.0  \n Median : 72944   Median : 13780   Median : 3524   Median : 375.0  \n Mean   : 80731   Mean   : 16219   Mean   : 3647   Mean   : 402.5  \n 3rd Qu.: 96691   3rd Qu.: 20141   3rd Qu.: 4658   3rd Qu.: 527.0  \n Max.   :250001   Max.   :208341   Max.   :10388   Max.   :2092.0  \n NA's   :65       NA's   :73                                       \n          geometry   \n MULTIPOLYGON :3445  \n epsg:4269    :   0  \n +proj=long...:   0  \n                     \n                     \n                     \n                     \n\nvulnerable=d1%&gt;%\n  filter(over65E&gt;945|incomeE&lt;32150\n  )\nnum_vulnerable &lt;- nrow(vulnerable)\ntotal_tracts &lt;- nrow(d1)\npercent_vulnerable &lt;- round((num_vulnerable / total_tracts) * 100, 1)\n\ncat(\"Low income threshold: $32,150\\n\")\n\nLow income threshold: $32,150\n\ncat(\"Elderly population threshold: 945 people\\n\")\n\nElderly population threshold: 945 people\n\ncat(\"Number of vulnerable tracts:\", num_vulnerable, \"\\n\")\n\nNumber of vulnerable tracts: 974 \n\ncat(\"Total tracts:\", total_tracts, \"\\n\")\n\nTotal tracts: 3445 \n\ncat(\"Percentage of vulnerable tracts:\", percent_vulnerable, \"%\\n\")\n\nPercentage of vulnerable tracts: 28.3 %\n\n\nQuestions to answer: - What income threshold did you choose and why? I chose an income threshold of $32,150, based on the 2023 U.S. Federal Poverty Guideline. This provides a policy-relevant definition of low-income households.\n\nWhat elderly population threshold did you choose and why? I selected 945 residents aged 65 and over as the threshold, which represents roughly the top 25% of tracts by elderly population size. These areas are likely to face higher demand for healthcare and mobility support.\nHow many tracts meet your vulnerability criteria? 974 tracts meet at least one of the criteria.\nWhat percentage of PA census tracts are considered vulnerable by your definition? Approximately 28.3% of all Pennsylvania census tracts are considered vulnerable. —\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n# Transform to appropriate projected CRS\nproj_crs &lt;- 3365\n\nvulnerable_proj &lt;- vulnerable %&gt;%\n  st_transform(proj_crs)\n\nhosp_proj &lt;- pa_hospital %&gt;%\n  st_transform(proj_crs)\n\nvulner_centroid &lt;- st_centroid(vulnerable_proj)\n\ndist &lt;- vulner_centroid %&gt;%\n  mutate(\n    dist_meters = apply(st_distance(vulner_centroid, hosp_proj), 1, min),\n    dist_miles = dist_meters * 0.000621371\n  )\n\navg_dist &lt;- mean(dist$dist_miles, na.rm = TRUE)\nmax_dist &lt;- max(dist$dist_miles, na.rm = TRUE)\nnum_over15 &lt;- sum(dist$dist_miles &gt; 15)\n\ncat(\"Average distance to nearest hospital (miles):\", round(avg_dist, 2), \"\\n\")\n\nAverage distance to nearest hospital (miles): 13.72 \n\ncat(\"Maximum distance (miles):\", round(max_dist, 2), \"\\n\")\n\nMaximum distance (miles): 85.17 \n\ncat(\"Number of vulnerable tracts &gt;15 miles from a hospital:\", num_over15, \"\\n\")\n\nNumber of vulnerable tracts &gt;15 miles from a hospital: 314 \n\n\nRequirements: - Use an appropriate projected coordinate system for Pennsylvania - Calculate distances in miles - Explain why you chose your projection\nQuestions to answer: - What is the average distance to the nearest hospital for vulnerable tracts? The average distance to the nearest hospital for vulnerable tracts is approximately 13.72 miles. - What is the maximum distance? The maximum distance from a vulnerable tract to the nearest hospital is 85.18 miles. - How many vulnerable tracts are more than 15 miles from the nearest hospital? There are 314 tracts located more than 15 miles from the nearest hospital.\nProjection used: EPSG:3365 – NAD83 / Pennsylvania South. It was chosen because it preserves local distances and areas for Pennsylvania, ensuring accurate measurement results in meters rather than degrees. —\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n#### Step 5: Identify Underserved Areas \n\nunderserved &lt;- dist %&gt;%\n  mutate(underserved = ifelse(dist_miles &gt; 15, TRUE, FALSE))\n\nnum_underserved &lt;- sum(underserved$underserved, na.rm = TRUE)\ntotal_vulnerable &lt;- nrow(underserved)\nperc_underserved &lt;- round((num_underserved / total_vulnerable) * 100, 1)\n\ncat(\"Number of underserved tracts:\", num_underserved, \"\\n\")\n\nNumber of underserved tracts: 314 \n\ncat(\"Percentage of vulnerable tracts that are underserved:\", perc_underserved, \"%\\n\")\n\nPercentage of vulnerable tracts that are underserved: 32.2 %\n\n\nQuestions to answer: - How many tracts are underserved? 314 tracts are considered underserved. - What percentage of vulnerable tracts are underserved? 32.2% of all vulnerable tracts are underserved. - Does this surprise you? Why or why not? This result is not surprising. Many underserved tracts are likely located in rural or remote counties, particularly in central and northern Pennsylvania, where hospitals are sparse and travel distances are long. In contrast, urban areas such as Philadelphia and Pittsburgh have higher hospital densities, providing better healthcare accessibility. —\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\npa_counties_proj &lt;- st_transform(pa_counties, st_crs(underserved))\ntracts_with_county &lt;- st_join(underserved, pa_counties_proj, join = st_intersects, left = FALSE)\n\nst_crs(underserved)$input\n\n[1] \"EPSG:3365\"\n\nst_crs(pa_counties_proj)$input\n\n[1] \"EPSG:3365\"\n\ntracts_with_county &lt;- st_join(underserved, pa_counties_proj, join = st_intersects, left = FALSE)\n\nnames(pa_counties)\n\n [1] \"OBJECTID\"   \"MSLINK\"     \"COUNTY_NAM\" \"COUNTY_NUM\" \"FIPS_COUNT\"\n [6] \"COUNTY_ARE\" \"COUNTY_PER\" \"NUMERIC_LA\" \"COUNTY_N_1\" \"AREA_SQ_MI\"\n[11] \"SOUND\"      \"SPREAD_SHE\" \"IMAGE_NAME\" \"NOTE_FILE\"  \"VIDEO\"     \n[16] \"DISTRICT_N\" \"PA_CTY_COD\" \"MAINT_CTY_\" \"DISTRICT_O\" \"geometry\"  \n\ncounty_summary &lt;- tracts_with_county %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarise(\n    vulnerable_tracts = n(),\n    underserved_tracts = sum(underserved, na.rm = TRUE),\n    perc_underserved = round((underserved_tracts / vulnerable_tracts) * 100, 1),\n    avg_distance_miles = round(mean(dist_miles, na.rm = TRUE), 2),\n    total_vulnerable_pop = sum(populationE, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(perc_underserved))\n\nhead(county_summary, 10)\n\n# A tibble: 10 × 6\n   COUNTY_NAM     vulnerable_tracts underserved_tracts perc_underserved\n   &lt;chr&gt;                      &lt;int&gt;              &lt;int&gt;            &lt;dbl&gt;\n 1 CLARION                        1                  1              100\n 2 GREENE                         1                  1              100\n 3 JUNIATA                        1                  1              100\n 4 MCKEAN                         1                  1              100\n 5 NORTHUMBERLAND                 5                  5              100\n 6 PIKE                           2                  2              100\n 7 SNYDER                         5                  5              100\n 8 TIOGA                          4                  4              100\n 9 VENANGO                        3                  3              100\n10 WARREN                         1                  1              100\n# ℹ 2 more variables: avg_distance_miles &lt;dbl&gt;, total_vulnerable_pop &lt;dbl&gt;\n\n\nRequired county-level statistics: - Number of vulnerable tracts - Number of underserved tracts\n- Percentage of vulnerable tracts that are underserved - Average distance to nearest hospital for vulnerable tracts - Total vulnerable population\nQuestions to answer: - Which 5 counties have the highest percentage of underserved vulnerable tracts? NORTHUMBERLAND,SNYDER,TIOGA,VENANGO,PIKE\n\nWhich counties have the most vulnerable people living far from hospitals? ALLEGHENY,PHILADELPHIA,BUCKS,MONTGOMERY,LANCASTER\nAre there any patterns in where underserved counties are located? Underserved counties are mainly located in rural northern and central Pennsylvania, where low population density and limited hospital infrastructure create significant barriers to healthcare access. —\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n# Create and format priority counties table\n\npriority_counties &lt;- county_summary %&gt;%\narrange(desc(perc_underserved), desc(avg_distance_miles)) %&gt;%\nslice_head(n = 10) %&gt;%\nmutate(\nperc_underserved = paste0(perc_underserved, \"%\"),\navg_distance_miles = round(avg_distance_miles, 2),\ntotal_vulnerable_pop = formatC(total_vulnerable_pop, format = \"d\", big.mark = \",\")\n)\n\nlibrary(kableExtra)\nkable(\n  priority_counties,\n  col.names = c(\"County\", \"Vulnerable Tracts\", \"Underserved Tracts\", \"% Underserved\", \n                \"Avg Distance (miles)\", \"Total Vulnerable Population\"),\n  caption = \"Table 1. Top 10 Pennsylvania Counties with the Greatest Need for Healthcare Investment\",\n  align = \"c\"\n) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    full_width = FALSE,\n    position = \"center\",\n    font_size = 13\n  ) %&gt;%\n  row_spec(0, bold = TRUE, background = \"darkgrey\", color = \"white\") %&gt;% \n  row_spec(1:10, background = \"#f8f9fa\") %&gt;%                             \n  add_header_above(c(\" \" = 1, \"Tract Counts\" = 2, \"Accessibility & Population\" = 3)) \n\n\n\nTable 1. Top 10 Pennsylvania Counties with the Greatest Need for Healthcare Investment\n\n\n\n\n\n\n\n\n\n\n\n\nTract Counts\n\n\nAccessibility & Population\n\n\n\nCounty\nVulnerable Tracts\nUnderserved Tracts\n% Underserved\nAvg Distance (miles)\nTotal Vulnerable Population\n\n\n\n\nPIKE\n2\n2\n100%\n77.36\n8,286\n\n\nWYOMING\n1\n1\n100%\n63.59\n4,185\n\n\nMCKEAN\n1\n1\n100%\n46.55\n5,054\n\n\nSNYDER\n5\n5\n100%\n44.30\n27,456\n\n\nNORTHUMBERLAND\n5\n5\n100%\n35.49\n27,608\n\n\nGREENE\n1\n1\n100%\n30.53\n5,359\n\n\nJUNIATA\n1\n1\n100%\n28.96\n5,560\n\n\nVENANGO\n3\n3\n100%\n28.69\n12,686\n\n\nTIOGA\n4\n4\n100%\n28.15\n17,895\n\n\nWARREN\n1\n1\n100%\n27.56\n5,066\n\n\n\n\n\n\nRequirements: - Use knitr::kable() or similar for formatting - Include descriptive column names - Format numbers appropriately (commas for population, percentages, etc.) - Add an informative caption - Sort by priority (you decide the metric)"
  },
  {
    "objectID": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#part-2-comprehensive-visualization",
    "href": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(knitr)\ntmap_mode(\"plot\")\ntm_shape(pa_counties_proj %&gt;% \n           left_join(county_summary, by = c(\"COUNTY_NAM\" = \"COUNTY_NAM\"))) +\n  tm_polygons(\n    col = \"avg_distance_miles\",     \n    palette = \"Blues\",                \n    style = \"quantile\",\n    n = 5,\n    title = \"Avg. Distance to Hospital (miles)\"\n  ) +\n  tm_borders(col = \"gray50\", lwd = 0.5) +\n  tm_layout(\n    legend.outside = TRUE,\n    frame = FALSE,\n    bg.color = \"white\"\n  )\n\n\n\n\n\n\n\n\nRequirements: - Fill counties by percentage of vulnerable tracts that are underserved - Include hospital locations as points - Use an appropriate color scheme - Include clear title, subtitle, and caption - Use theme_void() or similar clean theme - Add a legend with formatted labels\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)\n\ncounty_map &lt;- pa_counties %&gt;%\n  st_transform(3365) %&gt;%\n  left_join(county_summary, by = c(\"COUNTY_NAM\" = \"COUNTY_NAM\"))\n\nhospital_points &lt;- pa_hospital %&gt;%\n  st_transform(3365)\n\nggplot() +\n  geom_sf(data = county_map, aes(fill = perc_underserved), color = \"white\", size = 0.3) +\n  geom_sf(data = hospital_points, color = \"pink\", size = 1, alpha = 0.7) +\n  scale_fill_gradient(\n    name = \"% Underserved\\nVulnerable Tracts\",\n    low = \"lightgrey\", high = \"black\",\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  labs(\n    title = \"Healthcare Accessibility Across Pennsylvania Counties\",\n    subtitle = \"Percentage of vulnerable tracts underserved and hospital locations\",\n    caption = \"Data source: 2023 ACS 5-Year Estimates & Pennsylvania Department of Health\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, margin = margin(b = 10)),\n    plot.caption = element_text(size = 9, color = \"gray40\"),\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nRequirements: - Show underserved vulnerable tracts in a contrasting color - Include county boundaries for context - Show hospital locations - Use appropriate visual hierarchy (what should stand out?) - Include informative title and subtitle\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create a clean dataframe of distances\ndist_df &lt;- dist %&gt;%\n  st_drop_geometry() %&gt;%\n  select(dist_miles)\n\nggplot(dist_df, aes(x = dist_miles)) +\n  geom_histogram(\n    bins = 30,\n    fill = \"lightpink\",\n    color = \"white\",\n    alpha = 0.8\n  ) +\n  labs(\n    title = \"Distribution of Distances to the Nearest Hospital (Vulnerable Tracts)\",\n    x = \"Distance to Nearest Hospital (miles)\",\n    y = \"Number of Vulnerable Census Tracts\",\n    caption = \"Most vulnerable tracts are within 15 miles of a hospital, but a right tail shows rural access challenges.\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(size = 15, face = \"bold\"),\n    axis.title = element_text(face = \"bold\"),\n    plot.caption = element_text(size = 10, color = \"gray40\")\n  )\n\n\n\n\n\n\n\n\nSuggested chart types: - Histogram or density plot of distances - Box plot comparing distances across regions - Bar chart of underserved tracts by county - Scatter plot of distance vs. vulnerable population size\nRequirements: - Clear axes labels with units - Appropriate title - Professional formatting - Brief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\nNote these are just loose suggestions to spark ideas - follow or make your own as the data permits and as your ideas evolve. This analysis should include bringing in your own dataset, ensuring the projection/CRS of your layers align and are appropriate for the analysis (not lat/long or geodetic coordinate systems). The analysis portion should include some combination of spatial and attribute operations to answer a relatively straightforward question\n\nSchool Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: “Are school zones safe for walking/biking, or are they crime hotspots?” - Operations: Buffer schools (1000ft safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/ - Most datasets available as GeoJSON, Shapefile, or CSV with coordinates - Always check the Metadata for a data dictionary of the fields.\nAdditional Sources: - Pennsylvania Open Data: https://data.pa.gov/ - Census Bureau (via tidycensus): Demographics, economic indicators, commute patterns - TIGER/Line (via tigris): Geographic boundaries\n\n\nRecommended Starting Points\nIf you’re feeling confident: Choose an advanced challenge with multiple data layers. If you are a beginner, choose something more manageable that helps you understand the basics\nIf you have a different idea: Propose your own question! Just make sure: - You can access the spatial data - You can perform at least 2 spatial operations\n\n\nYour Analysis\nYour Task:\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load your additional dataset\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tmap)\n\nschools &lt;- st_read(\"/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Assignment_2/Schools.geojson\")\n\nReading layer `Schools' from data source \n  `/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Assignment_2/Schools.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 495 features and 14 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.2665 ymin: 39.90781 xmax: -74.97057 ymax: 40.12974\nGeodetic CRS:  WGS 84\n\ncrime &lt;- st_read(\"/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Assignment_2/incidents_part1_part2/incidents_part1_part2.shp\")\n\nReading layer `incidents_part1_part2' from data source \n  `/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Assignment_2/incidents_part1_part2/incidents_part1_part2.shp' \n  using driver `ESRI Shapefile'\nreplacing null geometries with empty geometries\nSimple feature collection with 120567 features and 13 fields (with 4991 geometries empty)\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.27421 ymin: 5.684342e-14 xmax: 5.684342e-14 ymax: 40.13683\nGeodetic CRS:  WGS 84\n\nbike_network &lt;- st_read(\"/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Assignment_2/PhiladelphiaBikeNetwork_SupportingDatasets201209/BikeNetwork_SupportingDatasets201209/PhiladelphiaBikeNetwork201204.shp\")\n\nReading layer `PhiladelphiaBikeNetwork201204' from data source \n  `/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Assignment_2/PhiladelphiaBikeNetwork_SupportingDatasets201209/BikeNetwork_SupportingDatasets201209/PhiladelphiaBikeNetwork201204.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3719 features and 7 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 2663613 ymin: 207711.1 xmax: 2747362 ymax: 299020.2\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\n# Convert all to same projected CRS\nschools &lt;- st_transform(schools, 2272)\ncrime &lt;- st_transform(crime, 2272)\nbike_network &lt;- st_transform(bike_network, 2272)\n\n\nst_crs(schools)\n\nCoordinate Reference System:\n  User input: EPSG:2272 \n  wkt:\nPROJCRS[\"NAD83 / Pennsylvania South (ftUS)\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"SPCS83 Pennsylvania South zone (US survey foot)\",\n        METHOD[\"Lambert Conic Conformal (2SP)\",\n            ID[\"EPSG\",9802]],\n        PARAMETER[\"Latitude of false origin\",39.3333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-77.75,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",40.9666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",39.9333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1968500,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United States (USA) - Pennsylvania - counties of Adams; Allegheny; Armstrong; Beaver; Bedford; Berks; Blair; Bucks; Butler; Cambria; Chester; Cumberland; Dauphin; Delaware; Fayette; Franklin; Fulton; Greene; Huntingdon; Indiana; Juniata; Lancaster; Lawrence; Lebanon; Lehigh; Mifflin; Montgomery; Northampton; Perry; Philadelphia; Schuylkill; Snyder; Somerset; Washington; Westmoreland; York.\"],\n        BBOX[39.71,-80.53,41.18,-74.72]],\n    ID[\"EPSG\",2272]]\n\nst_crs(crime)\n\nCoordinate Reference System:\n  User input: EPSG:2272 \n  wkt:\nPROJCRS[\"NAD83 / Pennsylvania South (ftUS)\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"SPCS83 Pennsylvania South zone (US survey foot)\",\n        METHOD[\"Lambert Conic Conformal (2SP)\",\n            ID[\"EPSG\",9802]],\n        PARAMETER[\"Latitude of false origin\",39.3333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-77.75,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",40.9666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",39.9333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1968500,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United States (USA) - Pennsylvania - counties of Adams; Allegheny; Armstrong; Beaver; Bedford; Berks; Blair; Bucks; Butler; Cambria; Chester; Cumberland; Dauphin; Delaware; Fayette; Franklin; Fulton; Greene; Huntingdon; Indiana; Juniata; Lancaster; Lawrence; Lebanon; Lehigh; Mifflin; Montgomery; Northampton; Perry; Philadelphia; Schuylkill; Snyder; Somerset; Washington; Westmoreland; York.\"],\n        BBOX[39.71,-80.53,41.18,-74.72]],\n    ID[\"EPSG\",2272]]\n\nst_crs(bike_network)\n\nCoordinate Reference System:\n  User input: EPSG:2272 \n  wkt:\nPROJCRS[\"NAD83 / Pennsylvania South (ftUS)\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"SPCS83 Pennsylvania South zone (US survey foot)\",\n        METHOD[\"Lambert Conic Conformal (2SP)\",\n            ID[\"EPSG\",9802]],\n        PARAMETER[\"Latitude of false origin\",39.3333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-77.75,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",40.9666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",39.9333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1968500,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United States (USA) - Pennsylvania - counties of Adams; Allegheny; Armstrong; Beaver; Bedford; Berks; Blair; Bucks; Butler; Cambria; Chester; Cumberland; Dauphin; Delaware; Fayette; Franklin; Fulton; Greene; Huntingdon; Indiana; Juniata; Lancaster; Lawrence; Lebanon; Lehigh; Mifflin; Montgomery; Northampton; Perry; Philadelphia; Schuylkill; Snyder; Somerset; Washington; Westmoreland; York.\"],\n        BBOX[39.71,-80.53,41.18,-74.72]],\n    ID[\"EPSG\",2272]]\n\nnrow(schools); nrow(crime); nrow(bike_network)\n\n[1] 495\n\n\n[1] 120567\n\n\n[1] 3719\n\n\nQuestions to answer: - What dataset did you choose and why? I selected three datasets, Philadelphia Schools, Crime Incidents, and the Philadelphia Bike Network for analyzing school safety environments. This combination allows an examination of whether schools are located in areas with high crime density and limited bicycle infrastructure, which is relevant for understanding walkability and student safety.\n\nWhat is the data source and date? All datasets were obtained from OpenDataPhilly. Schools: Data includes points identifying public, charter, private, and archdiocesan schools, as well as school annexes, athletic fields, and facilities. It supports the Streets Department’s school signage and crosswalk initiatives. Last updated February 2, 2024.\n\nCrime Incidents: Provided by the Philadelphia Police Department (2025), including Part I crimes such as aggravated assault, rape, and arson.\nBike Network – Supporting Datasets: Compiled by the Philadelphia City Planning Commission (April 2012), integrating data from multiple city departments to support bike network routing and documentation.\n\nHow many features does it contain? Schools: 495 point features Crime incidents: 120,567 point features Bike network: 3,719 line features\nWhat CRS is it in? Did you need to transform it? All datasets were standardized to EPSG:2272 – NAD83 / Pennsylvania South (ftUS). The Schools dataset was originally in WGS 84 (EPSG:4326) and was transformed to EPSG:2272 to ensure consistent spatial analysis and accurate distance or buffer measurements in feet.\n\n\n\nPose a research question\n\nResearch Question:Are schools in high-crime areas lacking nearby bicycle infrastructure?\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+): - Buffers - Spatial joins - Spatial filtering with predicates - Distance calculations - Intersections or unions - Point-in-polygon aggregation\nYour Task:\n\n# Create 500 ft buffers around schools\nschool_buffer &lt;- st_buffer(schools, dist = 500)\n\n# Create 300 ft buffers around bike lanes\nbike_buffer &lt;- st_buffer(bike_network, dist = 300)\n\n# Quick visualization\nlibrary(tmap)\n\ntmap_mode(\"plot\")\n\ntm_shape(bike_buffer) + \n  tm_borders(col = \"steelblue\", lwd = 1) +\n  tm_shape(school_buffer) + \n  tm_borders(col = \"red\", lwd = 1) +\n  tm_shape(crime) + \n  tm_dots(col = \"black\", size = 0.02) +\n  tm_layout(\n    title = \"School and Bike Lane Buffers with Crime Points\",\n    frame = FALSE,\n    legend.outside = TRUE\n  )\n\n\n\n\n\n\n\n\n\n# 1. Buffer\nschool_buffer &lt;- st_buffer(schools, dist = 500)\nbike_buffer &lt;- st_buffer(bike_network, dist = 300)\n\n# 2. Spatial join: count crimes in each school buffer\ncrime_in_school &lt;- st_join(crime, school_buffer, join = st_within)\n\ncrime_summary &lt;- crime_in_school %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(school_name) %&gt;%\n  summarise(crime_count = n())\n\n# 3. Identify whether each school has nearby bike lanes\nschool_bike_join &lt;- school_buffer %&gt;%\n  mutate(has_bike = if_else(\n    rowSums(st_intersects(., bike_buffer, sparse = FALSE)) &gt; 0, 1, 0\n  ))\n\n# 4. Merge crime + bike data\nschool_analysis &lt;- school_bike_join %&gt;%\n  left_join(crime_summary, by = \"school_name\") %&gt;%\n  mutate(crime_count = replace_na(crime_count, 0),\n         at_risk = if_else(crime_count &gt; 50 & has_bike == 0, 1, 0))\n\n# 5. Filter and visualize\n\nat_risk_schools &lt;- school_analysis %&gt;%\n  filter(at_risk == 1)\n\n# Optional: check how many schools\nnrow(at_risk_schools)\n\n[1] 52\n\nlibrary(tmap)\n\ntmap_mode(\"plot\")  \n\ntm_shape(bike_network) +\n  tm_lines(col = \"steelblue\", lwd = 1) +\n  tm_shape(school_buffer) +\n  tm_borders(col = \"firebrick\", lwd = 1) +\n  tm_shape(at_risk_schools) +\n  tm_dots(col = \"black\", size = 0.3) +\n  tm_layout(\n    title = \"At-Risk Schools: High Crime and No Nearby Bike Lanes\",\n    legend.outside = TRUE,\n    frame = FALSE\n  )\n\n\n\n\n\n\n\n\n\nschool_analysis_df &lt;- school_analysis %&gt;%\n  st_drop_geometry()\n\n# Summary\nsummary_table &lt;- school_analysis_df %&gt;%\n  summarise(\n    total_schools = n(),\n    total_at_risk = sum(at_risk, na.rm = TRUE),\n    avg_crime_near_school = round(mean(crime_count, na.rm = TRUE), 1)\n  ) %&gt;%\n  rename(\n    \"Total Schools\" = total_schools,\n    \"At-Risk Schools\" = total_at_risk,\n    \"Average Crimes Near Schools\" = avg_crime_near_school\n  )\n\nsummary_table %&gt;%\n  kbl(\n    caption = \"Table 1. Summary Statistics of School Safety and Bicycle Infrastructure in Philadelphia\",\n    align = c('c', 'c', 'c'),\n    booktabs = TRUE\n  ) %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n  )\n\n\n\nTable 1. Summary Statistics of School Safety and Bicycle Infrastructure in Philadelphia\n\n\nTotal Schools\nAt-Risk Schools\nAverage Crimes Near Schools\n\n\n\n\n495\n52\n1083.9\n\n\n\n\n\n# Drop geometry and clean data\nat_risk_table &lt;- at_risk_schools %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(school_name_label)) %&gt;%\n  select(\n    `School Name` = school_name_label,\n    `Crime Incidents` = crime_count,\n    `Nearby Bike Lane` = has_bike\n  ) %&gt;%\n  mutate(\n    `Crime Incidents` = formatC(`Crime Incidents`, format = \"d\", big.mark = \",\"),\n    `Nearby Bike Lane` = if_else(`Nearby Bike Lane` == 1, \"Yes\", \"No\")\n  ) %&gt;%\n  arrange(desc(as.numeric(`Crime Incidents`))) %&gt;%\n  slice_head(n = 10)\n\n# Output table\nat_risk_table %&gt;%\n  kbl(\n    caption = \"Table 2. Top 10 At-Risk Schools in Philadelphia: High Crime and No Nearby Bicycle Infrastructure\",\n    align = c(\"l\", \"c\", \"c\"),\n    booktabs = TRUE\n  ) %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n  ) %&gt;%\n  column_spec(1, bold = TRUE, width = \"5cm\") %&gt;%\n  column_spec(2:3, width = \"3cm\") %&gt;%\n  add_header_above(c(\" \" = 1, \"Safety Risk Indicators\" = 2))\n\n\n\nTable 2. Top 10 At-Risk Schools in Philadelphia: High Crime and No Nearby Bicycle Infrastructure\n\n\n\n\n\n\n\n\n\nSafety Risk Indicators\n\n\n\nSchool Name\nCrime Incidents\nNearby Bike Lane\n\n\n\n\nTECH FREIRE CHARTER SCHOOL\n138\nNo\n\n\nALLIANCE FOR PROGRESS CHARTER SCHOOL\n128\nNo\n\n\nALLIANCE FOR PROGRESS CHARTER SCHOOL (ANNEX)\n121\nNo\n\n\nISAAC A. SHEPPARD SCHOOL\n113\nNo\n\n\nONE BRIGHT RAY - FAIRHILL CAMPUS\n112\nNo\n\n\nYOUTHBUILD PHILADELPHIA CHARTER SCHOOL\n109\nNo\n\n\nGENERAL GEORGE G. MEADE SCHOOL\n100\nNo\n\n\nKIPP NORTH PHILADELPHIA ACADEMY\n95\nNo\n\n\nDEEP ROOTS CHARTER SCHOOL\n95\nNo\n\n\nPEOPLE FOR PEOPLE CHARTER SCHOOL\n91\nNo\n\n\n\n\n\n\nAnalysis requirements: - Clear code comments explaining each step - Appropriate CRS transformations - Summary statistics or counts - At least one map showing your findings - Brief interpretation of results (3-5 sentences)\nYour interpretation: The analysis found that 52 out of 495 schools in Philadelphia are located in areas with both high crime density and no nearby bicycle infrastructure, posing potential safety concerns for students walking or biking to school.Most at-risk schools are concentrated in North and West Philadelphia, where crime incidents are more frequent. The absence of bicycle lanes near these schools may reduce safe commuting options for students and discourage active travel.Integrating safer bike routes and crime prevention strategies around schools could significantly improve student safety and accessibility."
  },
  {
    "objectID": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nI focused on simpler, cleaner map design and improved table formatting for easier interpretation."
  },
  {
    "objectID": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#submission-requirements",
    "href": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html",
    "href": "Assignments/Assignment_1/Assignment_1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Texas Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#scenario",
    "href": "Assignments/Assignment_1/Assignment_1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Texas Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#learning-objectives",
    "href": "Assignments/Assignment_1/Assignment_1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#submission-instructions",
    "href": "Assignments/Assignment_1/Assignment_1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#data-retrieval",
    "href": "Assignments/Assignment_1/Assignment_1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\ncounty_data_2022 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_pop = \"B01003_001\",       # Total population\n    med_household_income = \"B19013_001\"   # Median household income\n    ),\n  state = \"TX\",\n  year = 2022,\n  output = \"wide\"\n)\n# Clean the county names to remove state name and \"County\" \ncounty_data_2022 &lt;- county_data_2022 %&gt;%\n  mutate(county_name = str_remove(NAME, \", Texas\"))\n\n# Display the first few rows\nhead(county_data_2022)\n\n# A tibble: 6 × 7\n  GEOID NAME   total_popE total_popM med_household_incomeE med_household_incomeM\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;                 &lt;dbl&gt;                 &lt;dbl&gt;\n1 48001 Ander…      58077         NA                 57445                  4562\n2 48003 Andre…      18362         NA                 86458                 16116\n3 48005 Angel…      86608         NA                 57055                  2484\n4 48007 Arans…      24048         NA                 58168                  6458\n5 48009 Arche…       8649         NA                 69954                  8482\n6 48011 Armst…       1912        145                 70417                 14574\n# ℹ 1 more variable: county_name &lt;chr&gt;"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#data-quality-assessment",
    "href": "Assignments/Assignment_1/Assignment_1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\nlibrary(scales)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Calculate MOE percentage and reliability categories using mutate()\nTX_county_reliability &lt;- county_data_2022 %&gt;%\n  mutate(\n    med_income_moe_pct = (med_household_incomeM / med_household_incomeE) * 100,\n    med_income_confi = case_when(\n      med_income_moe_pct &lt; 5 ~ \"High Confidence (&lt;5%)\",\n      med_income_moe_pct &gt; 5 & med_income_moe_pct &lt;10 ~ \"Moderate Confidence (5% - 10%)\",\n      med_income_moe_pct &gt; 10  ~ \"Low Confidence (&gt;10%)\"\n ),\n    unreliable_income = med_income_moe_pct &gt;= 10\n  )\n# Create a summary showing count of counties in each reliability category\nTX_reliability_summary &lt;- TX_county_reliability %&gt;%\n  count(med_income_confi) %&gt;%\n  mutate(percent = round(100 * n / sum(n), 1))\n\n# Display the summary table\nkable(\n  TX_reliability_summary,\n  caption = \"Texas county-level median household income reliability (ACS 2022)\",\n  col.names = c(\"Reliability Category\", \"Count\", \"Percentage\"),\n  align = c(\"l\", \"r\", \"r\")\n) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    full_width = FALSE,\n    position = \"center\"\n  )\n\n\n\nTexas county-level median household income reliability (ACS 2022)\n\n\nReliability Category\nCount\nPercentage\n\n\n\n\nHigh Confidence (&lt;5%)\n58\n22.8\n\n\nLow Confidence (&gt;10%)\n113\n44.5\n\n\nModerate Confidence (5% - 10%)\n82\n32.3\n\n\nNA\n1\n0.4\n\n\n\n\n\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#high-uncertainty-counties",
    "href": "Assignments/Assignment_1/Assignment_1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\nlibrary(scales)\nlibrary(kableExtra)\n\n# Create table of top 5 counties by MOE percentage\nTX_high_uncertainty &lt;- TX_county_reliability %&gt;%\n  arrange(desc(med_income_moe_pct)) %&gt;%\n  slice(1:5) %&gt;%\n  select(\n    County = county_name,\n    `Median Income ($)` = med_household_incomeE,\n    `Margin of Error ($)` = med_household_incomeM,\n    `MOE (%)` = med_income_moe_pct,\n    Reliability = med_income_confi\n  ) %&gt;%\n  # Format numbers for professional output\n  mutate(\n    `Median Income ($)` = dollar(`Median Income ($)`),\n    `Margin of Error ($)` = dollar(`Margin of Error ($)`),\n    `MOE (%)` = paste0(round(`MOE (%)`, 1), \"%\")\n  )\n\n# Format as table with kable() - include appropriate column names and caption\nkable(\n  TX_high_uncertainty,\n  caption = \"Top 5 Texas Counties by Margin of Error in Median Household Income (ACS 2022)\",\n  align = c(\"l\", \"r\", \"r\", \"r\", \"l\")  # set column alignment\n) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\")\n\n\nTop 5 Texas Counties by Margin of Error in Median Household Income (ACS 2022)\n\n\nCounty\nMedian Income ($)\nMargin of Error ($)\nMOE (%)\nReliability\n\n\n\n\nJeff Davis County\n$38,125\n$25,205\n66.1%\nLow Confidence (&gt;10%)\n\n\nCulberson County\n$35,924\n$18,455\n51.4%\nLow Confidence (&gt;10%)\n\n\nKing County\n$59,375\n$29,395\n49.5%\nLow Confidence (&gt;10%)\n\n\nKinney County\n$52,386\n$23,728\n45.3%\nLow Confidence (&gt;10%)\n\n\nDimmit County\n$27,374\n$12,374\n45.2%\nLow Confidence (&gt;10%)\n\n\n\n\n\nData Quality Commentary:\nThe five Texas counties with the highest margins of error in median household income estimates—Jeff Davis, Culberson, King, Kinney, and Dimmit—show MOE percentages ranging from 45% to 66%. Such extreme levels indicate that ACS estimates for these areas are highly unreliable. The primary causes are small populations, limited survey samples, and income variability that magnifies error. If used directly in algorithmic decision-making, these data could misclassify community needs and distort funding priorities. Policymakers should instead supplement ACS data with administrative or tax records, or require manual review, to ensure fair and accurate resource allocation."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#focus-area-selection",
    "href": "Assignments/Assignment_1/Assignment_1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\nlibrary(scales)\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\nselected_counties &lt;- TX_county_reliability %&gt;%\n  filter(med_household_incomeE %in% c(35924, 27374)) %&gt;%\n  mutate(`MOE (%)` = round(med_income_moe_pct, 1)) %&gt;%\n  select(\n    County = county_name,\n    `Median Income ($)` = med_household_incomeE,\n    `MOE (%)`,\n    Reliability = med_income_confi\n  )\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n# Display the selected counties\nkable(\n  selected_counties,\n  caption = \"Selected Texas Counties for Tract-Level Analysis\",\n  align = c(\"l\", \"r\", \"r\", \"r\", \"l\")\n) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\", bootstrap_options = c(\"striped\", \"hover\"))\n\n\nSelected Texas Counties for Tract-Level Analysis\n\n\nCounty\nMedian Income ($)\nMOE (%)\nReliability\n\n\n\n\nCulberson County\n35924\n51.4\nLow Confidence (&gt;10%)\n\n\nDimmit County\n27374\n45.2\nLow Confidence (&gt;10%)\n\n\n\n\n\nComment on the output:Culberson and Dimmit Counties were selected as examples of Low Confidence data. Their high MOE values reflect the challenges of using ACS estimates in small, rural counties."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#tract-level-demographics",
    "href": "Assignments/Assignment_1/Assignment_1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\nrace_vars &lt;- get_acs(\n  geography = \"tract\",\n  survey = \"acs5\",\n  variables = c(\n    white = \"B03002_003\", \n    black = \"B03002_004\",\n    hisp_latinx = \"B03002_012\",\n    total_pop = \"B03002_001\"\n  ),\n  year = 2022,\n  state = \"TX\",\n  county = c(\"109\", \"127\"),  # Culberson = 48109, Dimmit = 48127\n  output = \"wide\"\n)\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\nrace_vars &lt;- race_vars %&gt;%\n  mutate(\n    pct_white = 100 * whiteE / total_popE,\n    pct_black = 100 * blackE / total_popE,\n    pct_hispanic = 100 * hisp_latinxE / total_popE,\n    \n    # Split NAME on semicolon to extract tract and county\n    tract_name = sapply(strsplit(NAME, \";\"), function(x) trimws(x[1])),\n    county_name = sapply(strsplit(NAME, \";\"), function(x) trimws(x[2])) %&gt;%\n                  str_remove(\" County\")\n  )\n# Inspect first few rows\nhead(race_vars %&gt;%\n       select(tract_name, county_name, pct_white, pct_black, pct_hispanic))\n\n# A tibble: 4 × 5\n  tract_name           county_name pct_white pct_black pct_hispanic\n  &lt;chr&gt;                &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 Census Tract 9503    Culberson       11.4      0.183         81.3\n2 Census Tract 9502.01 Dimmit           7.39     0.242         91.7\n3 Census Tract 9502.02 Dimmit          17.2      1.56          77.2\n4 Census Tract 9504    Dimmit           4.20     0.156         93.8\n\n# Add readable tract and county name columns using str_extract() or similar"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#demographic-analysis",
    "href": "Assignments/Assignment_1/Assignment_1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\nlibrary(dplyr)\nlibrary(knitr)\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\ntop_hispanic_tract &lt;- race_vars %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice(1) %&gt;%\n  transmute(\n    Tract = tract_name,\n    County = county_name,\n    `Hispanic %` = percent(pct_hispanic / 100, accuracy = 0.1)\n  )\n\nkable(top_hispanic_tract,\n      caption = \"Tract with Highest Hispanic/Latino Population\",\n      align = c(\"l\", \"l\", \"r\")) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\",\n                bootstrap_options = c(\"striped\", \"hover\"))\n\n\nTract with Highest Hispanic/Latino Population\n\n\nTract\nCounty\nHispanic %\n\n\n\n\nCensus Tract 9504\nDimmit\n93.8%\n\n\n\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\ncounty_summary &lt;- race_vars %&gt;%\n  group_by(county_name) %&gt;%\n  summarise(\n    `Number of Tracts` = n(),\n    `Avg. White %` = percent(mean(pct_white, na.rm = TRUE) / 100, accuracy = 0.1),\n    `Avg. Black %` = percent(mean(pct_black, na.rm = TRUE) / 100, accuracy = 0.1),\n    `Avg. Hispanic %` = percent(mean(pct_hispanic, na.rm = TRUE) / 100, accuracy = 0.1)\n  )\n# Create a nicely formatted table of your results using kable()\nkable(\n  county_summary,\n  caption = \"Average Demographic Composition by County\",\n  align = c(\"l\", \"r\", \"r\", \"r\", \"r\")\n) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\",\n                bootstrap_options = c(\"striped\", \"hover\"))\n\n\nAverage Demographic Composition by County\n\n\ncounty_name\nNumber of Tracts\nAvg. White %\nAvg. Black %\nAvg. Hispanic %\n\n\n\n\nCulberson\n1\n11.4%\n0.2%\n81.3%\n\n\nDimmit\n3\n9.6%\n0.7%\n87.6%"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#moe-analysis-for-demographic-variables",
    "href": "Assignments/Assignment_1/Assignment_1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\ndemo_moe &lt;- race_vars %&gt;%\n  mutate(\n    white_moe_pct = (whiteM / whiteE) * 100,\n    black_moe_pct = (blackM / blackE) * 100,\n    hispanic_moe_pct = (hisp_latinxM / hisp_latinxE) * 100,\n    \n    # Flag tracts where any demographic MOE &gt; 15%\n    high_moe_flag = ifelse(\n      white_moe_pct &gt; 15 | black_moe_pct &gt; 15 | hispanic_moe_pct &gt; 15,\n      TRUE, FALSE\n    )\n  )\n# Create summary statistics showing how many tracts have data quality issues\nmoe_summary_county &lt;- demo_moe %&gt;%\n  group_by(county_name) %&gt;%\n  summarise(\n    total_tracts = n(),\n    high_moe_tracts = sum(high_moe_flag, na.rm = TRUE),\n    percent_high_moe = round(100 * mean(high_moe_flag, na.rm = TRUE), 1)\n  )\n\nkable(\n  moe_summary_county,\n  caption = \"**MOE Summary by County (ACS 2022)**\",\n  row.names = FALSE,\n  col.names = c(\"County\", \"Total Tracts\", \"High MOE Tracts\", \"Percent High MOE (%)\"),\n  align = c(\"l\", \"c\", \"c\", \"r\")\n) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\",\n                bootstrap_options = c(\"striped\", \"hover\"))\n\n\n**MOE Summary by County (ACS 2022)**\n\n\nCounty\nTotal Tracts\nHigh MOE Tracts\nPercent High MOE (%)\n\n\n\n\nCulberson\n1\n1\n100\n\n\nDimmit\n3\n3\n100"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#pattern-analysis",
    "href": "Assignments/Assignment_1/Assignment_1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n# Group tracts by whether they have high MOE issues\nmoe_patterns &lt;- demo_moe %&gt;%\n  filter(high_moe_flag == TRUE) %&gt;%   # keep only high-MOE tracts\n  group_by(county_name) %&gt;%\n  summarise(\n    `Population Average` = round(mean(total_popE, na.rm = TRUE), 0),\n    `% White Avg` = round(mean(pct_white, na.rm = TRUE), 2),\n    `% Black Avg` = round(mean(pct_black, na.rm = TRUE), 2),\n    `% LatinX Avg` = round(mean(pct_hispanic, na.rm = TRUE), 2),\n    `Tracts Quantity` = n(),\n    .groups = \"drop\"\n  )\n\nkable(\n  moe_patterns,\n  caption = \"**High-MOE Tracts by County (ACS 2022)**\",\n  align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"r\"),\n  col.names = c(\"County\", \"Population Average\", \"% White Avg\", \"% Black Avg\", \"% LatinX Avg\", \"Tracts Quantity\"),\n  digits = 2\n) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\",\n                bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n**High-MOE Tracts by County (ACS 2022)**\n\n\nCounty\nPopulation Average\n% White Avg\n% Black Avg\n% LatinX Avg\nTracts Quantity\n\n\n\n\nCulberson\n2181\n11.42\n0.18\n81.34\n1\n\n\nDimmit\n2891\n9.60\n0.65\n87.57\n3\n\n\n\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nPattern Analysis: High MOE tracts are found in small, rural counties with low populations and high Hispanic/Latino shares.These factors reduce ACS reliability, meaning minority and rural communities face greater risk of being misrepresented in algorithmic decisions."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#analysis-integration-and-professional-summary",
    "href": "Assignments/Assignment_1/Assignment_1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\nLooking at ACS 2022 data for Texas, we see that data quality isn’t the same everywhere. Larger, urban counties usually have solid numbers, while smaller rural counties often have very high margins of error. In some places, the income data is so uncertain that it’s hard to use with confidence.\nThe biggest risk shows up in rural, Hispanic-majority counties like Dimmit and Culberson. These areas often have the highest uncertainty, which means if an algorithm used this data directly, the results could shortchange the very communities that need the most support.\nThe main reason for this problem is how the ACS survey works. Small populations naturally create bigger sampling errors, and rural or minority communities may also face challenges like language barriers or low response rates. That makes their numbers less reliable.\nTo make decisions fairer, a tiered approach makes sense. Counties with strong data can go into the algorithm as-is. Counties with moderate-quality data should be monitored, and counties with weak data should get extra checks or even manual review. Over time, improving census participation in rural and minority communities would help fix the root of the problem."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#specific-recommendations",
    "href": "Assignments/Assignment_1/Assignment_1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\nlibrary(scales)\nlibrary(kableExtra)\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\nTX_recommendations &lt;- TX_county_reliability %&gt;%\n  select(county_name, med_household_incomeE, med_income_moe_pct, med_income_confi) %&gt;%\n  mutate(\n    `Median Income ($)` = dollar(med_household_incomeE),\n    `MOE (%)` = percent(med_income_moe_pct / 100, accuracy = 0.1),\n    Recommendation = case_when(\n      med_income_confi == \"High Confidence (&lt;5%)\" ~ \"✅ Safe for algorithmic decisions\",\n      med_income_confi == \"Moderate Confidence (5% - 10%)\" ~ \"⚠️ Use with caution – monitor outcomes\",\n      med_income_confi == \"Low Confidence (&gt;10%)\" ~ \"❌ Requires manual review or extra data\",\n      TRUE ~ \"Check data\"\n    )\n  ) %&gt;%\n  select(county_name, `Median Income ($)`, `MOE (%)`, med_income_confi, Recommendation)\n\n\n# Format as a professional table with kable()\nkable(\n  TX_recommendations,\n  caption = \"**County-Level Reliability and Algorithm Recommendations (ACS 2022)**\",\n  col.names = c(\"County\", \"Median Income ($)\", \"MOE (%)\", \"Reliability\", \"Recommendation\"),\n  escape = FALSE,\n  align = c(\"l\", \"r\", \"r\", \"l\", \"l\")\n) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\",\n                bootstrap_options = c(\"striped\", \"hover\", \"condensed\")) %&gt;%\n  column_spec(1, bold = TRUE)\n\n\n**County-Level Reliability and Algorithm Recommendations (ACS 2022)**\n\n\nCounty\nMedian Income ($)\nMOE (%)\nReliability\nRecommendation\n\n\n\n\nAnderson County\n$57,445\n7.9%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nAndrews County\n$86,458\n18.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nAngelina County\n$57,055\n4.4%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nAransas County\n$58,168\n11.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nArcher County\n$69,954\n12.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nArmstrong County\n$70,417\n20.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nAtascosa County\n$67,442\n6.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nAustin County\n$73,556\n6.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nBailey County\n$69,830\n18.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nBandera County\n$70,965\n8.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nBastrop County\n$80,151\n6.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nBaylor County\n$52,716\n25.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nBee County\n$50,283\n10.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nBell County\n$62,858\n2.8%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nBexar County\n$67,275\n1.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nBlanco County\n$79,717\n9.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nBorden County\n$80,625\n24.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nBosque County\n$63,868\n6.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nBowie County\n$56,628\n4.1%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nBrazoria County\n$91,972\n3.3%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nBrazos County\n$57,562\n3.5%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nBrewster County\n$47,747\n11.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nBriscoe County\n$35,446\n27.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nBrooks County\n$30,566\n33.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nBrown County\n$53,792\n4.7%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nBurleson County\n$71,745\n6.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nBurnet County\n$71,482\n8.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCaldwell County\n$66,779\n7.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCalhoun County\n$62,267\n9.9%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCallahan County\n$63,906\n3.6%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nCameron County\n$47,435\n3.4%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nCamp County\n$53,968\n7.6%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCarson County\n$83,199\n4.5%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nCass County\n$54,303\n6.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCastro County\n$59,886\n17.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nChambers County\n$106,103\n8.3%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCherokee County\n$56,971\n8.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nChildress County\n$56,063\n29.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nClay County\n$75,227\n7.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCochran County\n$41,597\n17.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nCoke County\n$40,230\n13.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nColeman County\n$51,034\n7.9%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCollin County\n$113,255\n1.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nCollingsworth County\n$52,045\n22.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nColorado County\n$63,352\n8.2%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nComal County\n$93,744\n2.8%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nComanche County\n$57,383\n13.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nConcho County\n$55,750\n27.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nCooke County\n$66,374\n8.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCoryell County\n$63,281\n3.6%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nCottle County\n$47,625\n37.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nCrane County\n$71,364\n32.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nCrockett County\n$64,103\n34.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nCrosby County\n$50,268\n10.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nCulberson County\n$35,924\n51.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nDallam County\n$71,969\n9.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nDallas County\n$70,732\n0.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nDawson County\n$45,268\n27.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nDeaf Smith County\n$51,942\n6.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nDelta County\n$68,491\n27.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nDenton County\n$104,180\n1.3%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nDeWitt County\n$61,100\n7.9%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nDickens County\n$46,638\n13.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nDimmit County\n$27,374\n45.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nDonley County\n$51,711\n12.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nDuval County\n$50,697\n20.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nEastland County\n$52,902\n12.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nEctor County\n$70,566\n4.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nEdwards County\n$40,809\n27.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nEllis County\n$93,248\n2.7%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nEl Paso County\n$55,417\n1.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nErath County\n$59,654\n6.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nFalls County\n$45,172\n15.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nFannin County\n$65,835\n6.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nFayette County\n$72,881\n5.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nFisher County\n$60,461\n8.2%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nFloyd County\n$49,321\n9.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nFoard County\n$41,944\n20.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nFort Bend County\n$109,987\n2.6%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nFranklin County\n$67,915\n4.4%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nFreestone County\n$55,902\n10.5%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nFrio County\n$56,042\n30.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nGaines County\n$73,299\n13.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nGalveston County\n$83,913\n2.8%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nGarza County\n$56,215\n35.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nGillespie County\n$70,162\n8.2%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nGlasscock County\n$112,188\n27.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nGoliad County\n$58,125\n25.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nGonzales County\n$64,255\n8.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nGray County\n$54,563\n7.2%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nGrayson County\n$66,608\n3.6%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nGregg County\n$63,811\n3.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nGrimes County\n$63,484\n9.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nGuadalupe County\n$88,111\n3.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHale County\n$50,721\n9.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nHall County\n$43,873\n11.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHamilton County\n$54,890\n17.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHansford County\n$62,350\n19.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHardeman County\n$60,455\n15.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHardin County\n$70,164\n5.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nHarris County\n$70,789\n0.7%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHarrison County\n$63,427\n4.8%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHartley County\n$78,065\n27.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHaskell County\n$52,786\n16.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHays County\n$79,990\n3.7%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHemphill County\n$67,798\n27.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHenderson County\n$59,778\n4.3%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHidalgo County\n$49,371\n2.3%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHill County\n$60,669\n6.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nHockley County\n$53,283\n7.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nHood County\n$80,013\n4.7%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHopkins County\n$63,766\n5.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nHouston County\n$51,043\n10.5%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHoward County\n$67,243\n6.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nHudspeth County\n$35,163\n23.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHunt County\n$66,885\n4.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHutchinson County\n$62,211\n7.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nIrion County\n$54,708\n17.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nJack County\n$58,861\n13.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nJackson County\n$67,176\n17.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nJasper County\n$48,818\n9.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nJeff Davis County\n$38,125\n66.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nJefferson County\n$57,294\n2.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nJim Hogg County\n$42,292\n13.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nJim Wells County\n$46,626\n12.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nJohnson County\n$77,058\n3.1%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nJones County\n$59,361\n10.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nKarnes County\n$57,798\n14.5%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nKaufman County\n$84,075\n4.1%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nKendall County\n$104,196\n8.3%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nKenedy County\n$45,455\n25.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nKent County\n$68,553\n15.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nKerr County\n$66,713\n6.2%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nKimble County\n$62,386\n22.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nKing County\n$59,375\n49.5%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nKinney County\n$52,386\n45.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nKleberg County\n$52,487\n9.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nKnox County\n$48,750\n9.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nLamar County\n$58,246\n4.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nLamb County\n$54,519\n8.6%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nLampasas County\n$73,269\n7.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nLa Salle County\n$62,798\n26.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nLavaca County\n$58,530\n7.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nLee County\n$66,448\n10.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nLeon County\n$57,363\n12.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nLiberty County\n$59,605\n6.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nLimestone County\n$53,102\n7.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nLipscomb County\n$71,625\n12.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nLive Oak County\n$55,949\n18.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nLlano County\n$64,241\n8.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nLoving County\nNA\nNA\nNA\nCheck data\n\n\nLubbock County\n$61,911\n3.5%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nLynn County\n$52,996\n7.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMcCulloch County\n$53,214\n16.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nMcLennan County\n$59,781\n3.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nMcMullen County\n$60,313\n41.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nMadison County\n$65,768\n9.6%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMarion County\n$48,040\n5.0%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nMartin County\n$70,217\n27.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nMason County\n$77,583\n15.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nMatagorda County\n$56,412\n6.3%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMaverick County\n$48,497\n10.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nMedina County\n$73,060\n4.0%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nMenard County\n$40,945\n17.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nMidland County\n$90,123\n5.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMilam County\n$56,985\n5.9%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMills County\n$59,315\n9.3%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMitchell County\n$49,869\n12.5%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nMontague County\n$63,336\n8.6%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMontgomery County\n$95,946\n3.4%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nMoore County\n$59,041\n6.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMorris County\n$51,532\n6.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMotley County\n$66,528\n8.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nNacogdoches County\n$51,153\n4.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nNavarro County\n$56,261\n7.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nNewton County\n$38,871\n16.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nNolan County\n$47,437\n7.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nNueces County\n$64,027\n2.3%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nOchiltree County\n$62,240\n17.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nOldham County\n$71,103\n11.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nOrange County\n$71,910\n7.9%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nPalo Pinto County\n$65,242\n4.4%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nPanola County\n$58,205\n18.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nParker County\n$95,721\n3.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nParmer County\n$65,575\n13.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nPecos County\n$59,325\n17.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nPolk County\n$57,315\n5.2%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nPotter County\n$47,974\n4.0%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nPresidio County\n$29,012\n24.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nRains County\n$60,291\n10.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nRandall County\n$78,038\n3.5%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nReagan County\n$70,319\n12.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nReal County\n$46,842\n33.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nRed River County\n$44,583\n9.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nReeves County\n$57,487\n22.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nRefugio County\n$54,304\n4.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nRoberts County\n$62,667\n14.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nRobertson County\n$59,410\n17.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nRockwall County\n$121,303\n3.8%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nRunnels County\n$55,424\n6.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nRusk County\n$61,661\n9.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nSabine County\n$47,061\n16.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nSan Augustine County\n$45,888\n9.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nSan Jacinto County\n$54,839\n13.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nSan Patricio County\n$63,842\n6.9%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nSan Saba County\n$54,087\n16.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nSchleicher County\n$53,774\n15.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nScurry County\n$58,932\n21.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nShackelford County\n$60,924\n14.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nShelby County\n$49,231\n10.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nSherman County\n$66,169\n27.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nSmith County\n$69,053\n3.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nSomervell County\n$87,899\n33.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nStarr County\n$35,979\n8.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nStephens County\n$44,712\n18.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nSterling County\n$63,558\n22.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nStonewall County\n$66,591\n32.5%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nSutton County\n$56,778\n22.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nSwisher County\n$40,290\n13.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nTarrant County\n$78,872\n1.0%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nTaylor County\n$61,806\n3.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nTerrell County\n$52,813\n21.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nTerry County\n$42,694\n10.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nThrockmorton County\n$55,221\n21.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nTitus County\n$57,634\n8.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nTom Green County\n$67,215\n4.7%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nTravis County\n$92,731\n1.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nTrinity County\n$51,165\n11.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nTyler County\n$50,898\n10.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nUpshur County\n$60,456\n7.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nUpton County\n$55,284\n21.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nUvalde County\n$55,000\n15.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nVal Verde County\n$57,250\n8.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nVan Zandt County\n$62,334\n8.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nVictoria County\n$66,308\n3.6%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nWalker County\n$47,193\n6.3%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nWaller County\n$71,643\n6.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nWard County\n$70,771\n12.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nWashington County\n$70,043\n9.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nWebb County\n$59,984\n3.1%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nWharton County\n$59,712\n6.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nWheeler County\n$58,158\n14.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nWichita County\n$58,862\n3.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nWilbarger County\n$50,769\n18.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nWillacy County\n$42,839\n13.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nWilliamson County\n$102,851\n1.4%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nWilson County\n$89,708\n4.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nWinkler County\n$89,155\n16.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nWise County\n$85,385\n6.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nWood County\n$61,748\n6.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nYoakum County\n$80,317\n8.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nYoung County\n$65,565\n16.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nZapata County\n$35,061\n10.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nZavala County\n$49,243\n28.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: High-confidence counties such as Bexar, Dallas, Travis, Williamson, and Collin have strong data quality (MOE &lt;5%). These counties are large, urban, and well-sampled, making them reliable for algorithm-driven funding and planning decisions.\nCounties requiring additional oversight: Moderate-confidence counties like Anderson, Bastrop, Caldwell, and Wise fall in the 5–10% MOE range. Algorithms may be used here, but outcomes should be monitored regularly with human oversight to check for misallocations.\nCounties needing alternative approaches: Low-confidence counties such as Dimmit, Culberson, Jeff Davis, King, and Kinney have very high MOEs (&gt;10%, sometimes above 40–60%). These areas need manual review, supplemental surveys, or local administrative data to ensure fair resource distribution."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#questions-for-further-investigation",
    "href": "Assignments/Assignment_1/Assignment_1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nAre high-MOE counties geographically clustered (e.g., along the border or in rural west Texas), and does this spatial pattern affect equity?\nHow do data quality issues change over time — are counties improving or declining in reliability across ACS cycles?\nDo certain demographic groups (Hispanic, Black, rural populations) consistently face higher MOEs, and what targeted outreach could improve data collection?"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#submission-checklist",
    "href": "Assignments/Assignment_1/Assignment_1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\n[☑ ] All code chunks run without errors\n[☑ ] All “[Fill this in]” prompts have been completed\n[☑ ] Tables are properly formatted and readable\n[☑ ] Executive summary addresses all four required components\n[☑ ] Portfolio navigation includes this assignment\n[☑ ] Census API key is properly set\n[☑ ] Document renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "labs/lab2/assignment2_template.html",
    "href": "labs/lab2/assignment2_template.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/lab2/assignment2_template.html#assignment-overview",
    "href": "labs/lab2/assignment2_template.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/lab2/assignment2_template.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "labs/lab2/assignment2_template.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data: - Pennsylvania county boundaries - Pennsylvania hospitals (from lecture data) - Pennsylvania census tracts\nYour Task:\n\n# Load required packages\n\n\n# Load spatial data\n\n\n\n\n# Check that all data loaded correctly\n\nQuestions to answer: - How many hospitals are in your dataset? - How many census tracts? - What coordinate reference system is each dataset in?\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables: - Total population - Median household income - Population 65 years and over (you may need to sum multiple age categories)\nYour Task:\n\n# Get demographic data from ACS\n\n\n\n\n# Join to tract boundaries\n\nQuestions to answer: - What year of ACS data are you using? - How many tracts have missing income data? - What is the median income across all PA census tracts?\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria: 1. Low median household income (choose an appropriate threshold) 2. Significant elderly population (choose an appropriate threshold)\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\n\nQuestions to answer: - What income threshold did you choose and why? - What elderly population threshold did you choose and why? - How many tracts meet your vulnerability criteria? - What percentage of PA census tracts are considered vulnerable by your definition?\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n# Transform to appropriate projected CRS\n\n\n# Calculate distance from each tract centroid to nearest hospital\n\nRequirements: - Use an appropriate projected coordinate system for Pennsylvania - Calculate distances in miles - Explain why you chose your projection\nQuestions to answer: - What is the average distance to the nearest hospital for vulnerable tracts? - What is the maximum distance? - How many vulnerable tracts are more than 15 miles from the nearest hospital?\n\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n# Create underserved variable\n\nQuestions to answer: - How many tracts are underserved? - What percentage of vulnerable tracts are underserved? - Does this surprise you? Why or why not?\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\n# Spatial join tracts to counties\n\n\n# Aggregate statistics by county\n\nRequired county-level statistics: - Number of vulnerable tracts - Number of underserved tracts\n- Percentage of vulnerable tracts that are underserved - Average distance to nearest hospital for vulnerable tracts - Total vulnerable population\nQuestions to answer: - Which 5 counties have the highest percentage of underserved vulnerable tracts? - Which counties have the most vulnerable people living far from hospitals? - Are there any patterns in where underserved counties are located?\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n# Create and format priority counties table\n\nRequirements: - Use knitr::kable() or similar for formatting - Include descriptive column names - Format numbers appropriately (commas for population, percentages, etc.) - Add an informative caption - Sort by priority (you decide the metric)"
  },
  {
    "objectID": "labs/lab2/assignment2_template.html#part-2-comprehensive-visualization",
    "href": "labs/lab2/assignment2_template.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\n# Create county-level access map\n\nRequirements: - Fill counties by percentage of vulnerable tracts that are underserved - Include hospital locations as points - Use an appropriate color scheme - Include clear title, subtitle, and caption - Use theme_void() or similar clean theme - Add a legend with formatted labels\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\n# Create detailed tract-level map\n\nRequirements: - Show underserved vulnerable tracts in a contrasting color - Include county boundaries for context - Show hospital locations - Use appropriate visual hierarchy (what should stand out?) - Include informative title and subtitle\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create distribution visualization\n\nSuggested chart types: - Histogram or density plot of distances - Box plot comparing distances across regions - Bar chart of underserved tracts by county - Scatter plot of distance vs. vulnerable population size\nRequirements: - Clear axes labels with units - Appropriate title - Professional formatting - Brief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "labs/lab2/assignment2_template.html#part-3-bring-your-own-data-analysis",
    "href": "labs/lab2/assignment2_template.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\nNote these are just loose suggestions to spark ideas - follow or make your own as the data permits and as your ideas evolve. This analysis should include bringing in your own dataset, ensuring the projection/CRS of your layers align and are appropriate for the analysis (not lat/long or geodetic coordinate systems). The analysis portion should include some combination of spatial and attribute operations to answer a relatively straightforward question\n\n\nEducation & Youth Services\nOption A: Educational Desert Analysis - Data: Schools, Libraries, Recreation Centers, Census tracts (child population) - Question: “Which neighborhoods lack adequate educational infrastructure for children?” - Operations: Buffer schools/libraries (0.5 mile walking distance), identify coverage gaps, overlay with child population density - Policy relevance: School district planning, library placement, after-school program siting\nOption B: School Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: “Are school zones safe for walking/biking, or are they crime hotspots?” - Operations: Buffer schools (1000ft safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\n\n\n\nEnvironmental Justice\nOption C: Green Space Equity - Data: Parks, Street Trees, Census tracts (race/income demographics) - Question: “Do low-income and minority neighborhoods have equitable access to green space?” - Operations: Buffer parks (10-minute walk = 0.5 mile), calculate tree canopy or park acreage per capita, compare by demographics - Policy relevance: Climate resilience, environmental justice, urban forestry investment —\n\n\nPublic Safety & Justice\nOption D: Crime & Community Resources - Data: Crime Incidents, Recreation Centers, Libraries, Street Lights - Question: “Are high-crime areas underserved by community resources?” - Operations: Aggregate crime counts to census tracts or neighborhoods, count community resources per area, spatial correlation analysis - Policy relevance: Community investment, violence prevention strategies —\n\n\nInfrastructure & Services\nOption E: Polling Place Accessibility - Data: Polling Places, SEPTA stops, Census tracts (elderly population, disability rates) - Question: “Are polling places accessible for elderly and disabled voters?” - Operations: Buffer polling places and transit stops, identify vulnerable populations, find areas lacking access - Policy relevance: Voting rights, election infrastructure, ADA compliance\n\n\n\nHealth & Wellness\nOption F: Recreation & Population Health - Data: Recreation Centers, Playgrounds, Parks, Census tracts (demographics) - Question: “Is lack of recreation access associated with vulnerable populations?” - Operations: Calculate recreation facilities per capita by neighborhood, buffer facilities for walking access, overlay with demographic indicators - Policy relevance: Public health investment, recreation programming, obesity prevention\n\n\n\nEmergency Services\nOption G: EMS Response Coverage - Data: Fire Stations, EMS stations, Population density, High-rise buildings - Question: “Are population-dense areas adequately covered by emergency services?” - Operations: Create service area buffers (5-minute drive = ~2 miles), assess population coverage, identify gaps in high-density areas - Policy relevance: Emergency preparedness, station siting decisions\n\n\n\nArts & Culture\nOption H: Cultural Asset Distribution - Data: Public Art, Museums, Historic sites/markers, Neighborhoods - Question: “Do all neighborhoods have equitable access to cultural amenities?” - Operations: Count cultural assets per neighborhood, normalize by population, compare distribution across demographic groups - Policy relevance: Cultural equity, tourism, quality of life, neighborhood identity\n\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/ - Most datasets available as GeoJSON, Shapefile, or CSV with coordinates - Always check the Metadata for a data dictionary of the fields.\nAdditional Sources: - Pennsylvania Open Data: https://data.pa.gov/ - Census Bureau (via tidycensus): Demographics, economic indicators, commute patterns - TIGER/Line (via tigris): Geographic boundaries\n\n\nRecommended Starting Points\nIf you’re feeling confident: Choose an advanced challenge with multiple data layers. If you are a beginner, choose something more manageable that helps you understand the basics\nIf you have a different idea: Propose your own question! Just make sure: - You can access the spatial data - You can perform at least 2 spatial operations\n\n\nYour Analysis\nYour Task:\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load your additional dataset\n\nQuestions to answer: - What dataset did you choose and why? - What is the data source and date? - How many features does it contain? - What CRS is it in? Did you need to transform it?\n\n\nPose a research question\n\nWrite a clear research statement that your analysis will answer.\nExamples: - “Do vulnerable tracts have adequate public transit access to hospitals?” - “Are EMS stations appropriately located near vulnerable populations?” - “Do areas with low vehicle access have worse hospital access?”\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+): - Buffers - Spatial joins - Spatial filtering with predicates - Distance calculations - Intersections or unions - Point-in-polygon aggregation\nYour Task:\n\n# Your spatial analysis\n\nAnalysis requirements: - Clear code comments explaining each step - Appropriate CRS transformations - Summary statistics or counts - At least one map showing your findings - Brief interpretation of results (3-5 sentences)\nYour interpretation:\n[Write your findings here]"
  },
  {
    "objectID": "labs/lab2/assignment2_template.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "labs/lab2/assignment2_template.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nTake a few moments to clean up your markdown document and then write a line or two or three about how you may have incorporated feedback that you recieved after your first assignment."
  },
  {
    "objectID": "labs/lab2/assignment2_template.html#submission-requirements",
    "href": "labs/lab2/assignment2_template.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\nSubmit the correct and working links of your assignment on Canvas"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html",
    "href": "labs/lab0/script/lab0_template.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you’ll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#data-structure-exploration",
    "href": "labs/lab0/script/lab0_template.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\nglimpse(car_data)\n\nRows: 50,000\nColumns: 7\n$ Manufacturer          &lt;chr&gt; \"Ford\", \"Porsche\", \"Ford\", \"Toyota\", \"VW\", \"Ford…\n$ Model                 &lt;chr&gt; \"Fiesta\", \"718 Cayman\", \"Mondeo\", \"RAV4\", \"Polo\"…\n$ `Engine size`         &lt;dbl&gt; 1.0, 4.0, 1.6, 1.8, 1.0, 1.4, 1.8, 1.4, 1.2, 2.0…\n$ `Fuel type`           &lt;chr&gt; \"Petrol\", \"Petrol\", \"Diesel\", \"Hybrid\", \"Petrol\"…\n$ `Year of manufacture` &lt;dbl&gt; 2002, 2016, 2014, 1988, 2006, 2018, 2010, 2015, …\n$ Mileage               &lt;dbl&gt; 127300, 57850, 39190, 210814, 127869, 33603, 866…\n$ Price                 &lt;dbl&gt; 3074, 49704, 24072, 1705, 4101, 29204, 14350, 30…\n\n# Check the column names\nnames(car_data)\n\n[1] \"Manufacturer\"        \"Model\"               \"Engine size\"        \n[4] \"Fuel type\"           \"Year of manufacture\" \"Mileage\"            \n[7] \"Price\"              \n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 × 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym…           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer: - How many rows and columns does the dataset have? - What types of variables do you see (numeric, character, etc.)? - Are there any column names that might cause problems? Why?\nYour answers: - Rows: 50,000 - Columns:7 - Variable types: character - Problematic names:"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#tibble-vs-data-frame",
    "href": "labs/lab0/script/lab0_template.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 49,990 more rows\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\ncar_df &lt;- as.data.frame(car_data)\n#car_df\n\nQuestion: What differences do you notice in how they print?\nYour answer: [YOUR ANSWER]"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#selecting-columns",
    "href": "labs/lab0/script/lab0_template.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\n# Select just Model and Mileage columns\n\n\n# Select Manufacturer, Price, and Fuel type\n\n\n# Challenge: Select all columns EXCEPT Engine Size"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#renaming-columns",
    "href": "labs/lab0/script/lab0_template.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet’s fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\n\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\"        \"Model\"               \"Engine size\"        \n[4] \"Fuel type\"           \"Year of manufacture\" \"Mileage\"            \n[7] \"Price\"              \n\n\nQuestion: Why did we need backticks around Year of manufacture but not around year?\nYour answer: [YOUR ANSWER]"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#calculate-car-age",
    "href": "labs/lab0/script/lab0_template.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create an 'age' column (2025 minus year of manufacture)\n\n\n# Create a mileage_per_year column  \n\n\n# Look at your new columns\n#select(car_data, Model, year, age, Mileage, mileage_per_year)"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#categorize-cars",
    "href": "labs/lab0/script/lab0_template.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, its is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is mid-range (use case_when)\n\n\n# Check your categories select the new column and show it"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#basic-filtering",
    "href": "labs/lab0/script/lab0_template.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\n\n\n# Find cars with mileage less than 30,000\n\n\n# Find luxury cars (from price category) with low mileage"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#multiple-conditions",
    "href": "labs/lab0/script/lab0_template.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Honda OR Nissan\n\n\n# Find cars with price between $20,000 and $35,000\n\n\n# Find diesel cars less than 10 years old\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: [YOUR ANSWER]"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#basic-summaries",
    "href": "labs/lab0/script/lab0_template.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\n\n\n# Count cars by manufacturer"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#categorical-summaries",
    "href": "labs/lab0/script/lab0_template.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#what-well-cover",
    "href": "labs/lab1/week-03/lecture/week3.html#what-well-cover",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "What We’ll Cover",
    "text": "What We’ll Cover\nPart 1: Why Visualization Matters\n\nAnscombe’s Quartet and the limits of summary statistics\nVisualization in policy context\nConnection to algorithmic bias and data ethics\n\nPart 2: Grammar of Graphics\n\nggplot2 fundamentals\nAesthetic mappings and geoms\nLive demonstration\n\nPart 3: Exploratory Data Analysis\n\nEDA workflow and principles\nUnderstanding distributions and relationships\nCritical focus: Data quality and uncertainty\n\nPart 4: Data Joins & Integration\n\nCombining datasets with dplyr joins\n\nPart 5: Hands-On Lab\n\nGuided practice with census data\nCreate publication-ready visualizations\nPractice ethical data communication"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#opening-question",
    "href": "labs/lab1/week-03/lecture/week3.html#opening-question",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Opening Question",
    "text": "Opening Question\nThink about Assignment 1:\nYou created tables showing income reliability patterns across counties. But what if you needed to present these findings to:\n\nThe state legislature (2-minute briefing)\nCommunity advocacy groups\nLocal news reporters\n\nDiscussion: How might visual presentation change the impact of your analysis?"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#anscombes-quartet-the-famous-example",
    "href": "labs/lab1/week-03/lecture/week3.html#anscombes-quartet-the-famous-example",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Anscombe’s Quartet: The Famous Example",
    "text": "Anscombe’s Quartet: The Famous Example\nFour datasets with identical summary statistics:\n\nSame means (x̄ = 9, ȳ = 7.5)\nSame variances\nSame correlation (r = 0.816)\nSame regression line\n\nBut completely different patterns when visualized"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#the-policy-implications",
    "href": "labs/lab1/week-03/lecture/week3.html#the-policy-implications",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The Policy Implications",
    "text": "The Policy Implications\nWhy this matters for your work:\n\nSummary statistics can hide critical patterns\nOutliers may represent important communities\nRelationships aren’t always linear\nVisual inspection reveals data quality issues\n\nExample: A county with “average” income might have extreme inequality that algorithms would miss without visualization."
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#connecting-week-2-ethical-data-communication",
    "href": "labs/lab1/week-03/lecture/week3.html#connecting-week-2-ethical-data-communication",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Connecting Week 2: Ethical Data Communication",
    "text": "Connecting Week 2: Ethical Data Communication\nFrom last week’s algorithmic bias discussion:\nResearch finding: Only 27% of planners warn users about unreliable ACS data - Most planners don’t report margins of error - Many lack training on statistical uncertainty - This violates AICP Code of Ethics\nYour responsibility:\n\nCreate honest, transparent visualizations\nAlways assess and communicate data quality\nConsider who might be harmed by uncertain data"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#bad-visualizations-have-real-consequences",
    "href": "labs/lab1/week-03/lecture/week3.html#bad-visualizations-have-real-consequences",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Bad Visualizations Have Real Consequences",
    "text": "Bad Visualizations Have Real Consequences\nCommon problems in government data presentation:\n\nMisleading scales or axes\nCherry-picked time periods\n\nHidden or ignored uncertainty\nMissing context about data reliability\n\nReal impact: The Jurjevich et al. study found that 72% of Portland census tracts had unreliable child poverty estimates, yet planners rarely communicated this uncertainty.\nResult: Poor policy decisions based on misunderstood data"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#the-ggplot2-philosophy",
    "href": "labs/lab1/week-03/lecture/week3.html#the-ggplot2-philosophy",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The ggplot2 Philosophy",
    "text": "The ggplot2 Philosophy\nGrammar of Graphics principles:\nData → Aesthetics → Geometries → Visual\n\nData: Your dataset (census data, survey responses, etc.)\nAesthetics: What variables map to visual properties (x, y, color, size)\nGeometries: How to display the data (points, bars, lines)\nAdditional layers: Scales, themes, facets, annotations"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#basic-ggplot2-structure",
    "href": "labs/lab1/week-03/lecture/week3.html#basic-ggplot2-structure",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Basic ggplot2 Structure",
    "text": "Basic ggplot2 Structure\nEvery ggplot has this pattern:\nggplot(data = your_data) +   aes(x = variable1, y = variable2) +   geom_something() +   additional_layers()\nYou build plots by adding layers with +"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#live-demo-basic-scatter-plot",
    "href": "labs/lab1/week-03/lecture/week3.html#live-demo-basic-scatter-plot",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Live Demo: Basic Scatter Plot",
    "text": "Live Demo: Basic Scatter Plot"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#aesthetic-mappings-the-key-to-ggplot2",
    "href": "labs/lab1/week-03/lecture/week3.html#aesthetic-mappings-the-key-to-ggplot2",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Aesthetic Mappings: The Key to ggplot2",
    "text": "Aesthetic Mappings: The Key to ggplot2\nAesthetics map data to visual properties:\n\nx, y - position\ncolor - point/line color\nfill - area fill color\n\nsize - point/line size\nshape - point shape\nalpha - transparency\n\nImportant: Aesthetics go inside aes(), constants go outside"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#improving-plots-with-labels-and-themes",
    "href": "labs/lab1/week-03/lecture/week3.html#improving-plots-with-labels-and-themes",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Improving Plots with Labels and Themes",
    "text": "Improving Plots with Labels and Themes"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#the-eda-mindset",
    "href": "labs/lab1/week-03/lecture/week3.html#the-eda-mindset",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The EDA Mindset",
    "text": "The EDA Mindset\nExploratory Data Analysis is detective work:\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\n\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?\n\nGoal: Understand your data before making decisions or building models"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#eda-workflow-with-data-quality-focus",
    "href": "labs/lab1/week-03/lecture/week3.html#eda-workflow-with-data-quality-focus",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "EDA Workflow with Data Quality Focus",
    "text": "EDA Workflow with Data Quality Focus\nEnhanced process for policy analysis:\n\nLoad and inspect - dimensions, variable types, missing data\nAssess reliability - examine margins of error, calculate coefficients of variation\nVisualize distributions - histograms, boxplots for each variable\nExplore relationships - scatter plots, correlations\nIdentify patterns - grouping, clustering, geographical patterns\nQuestion anomalies - investigate outliers and unusual patterns\nDocument limitations - prepare honest communication about data quality"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#understanding-distributions",
    "href": "labs/lab1/week-03/lecture/week3.html#understanding-distributions",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Understanding Distributions",
    "text": "Understanding Distributions\nWhy distribution shape matters:\n\nWhat to look for: Skewness, outliers, multiple peaks, gaps"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#boxplots",
    "href": "labs/lab1/week-03/lecture/week3.html#boxplots",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Boxplots!",
    "text": "Boxplots!"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#critical-data-quality-through-visualization",
    "href": "labs/lab1/week-03/lecture/week3.html#critical-data-quality-through-visualization",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Critical: Data Quality Through Visualization",
    "text": "Critical: Data Quality Through Visualization\nResearch insight: Most planners don’t visualize or communicate uncertainty\n\nPattern: Smaller populations have higher uncertainty Ethical implication: These communities might be systematically undercounted"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#research-based-recommendations-for-planners",
    "href": "labs/lab1/week-03/lecture/week3.html#research-based-recommendations-for-planners",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Research-Based Recommendations for Planners",
    "text": "Research-Based Recommendations for Planners\nJurjevich et al. (2018): 5 Essential Guidelines for Using ACS Data\n\nReport the corresponding MOEs of ACS estimates - Always include margin of error values\nInclude a footnote when not reporting MOEs - Explicitly acknowledge omission\n\nProvide context for (un)reliability - Use coefficient of variation (CV):\n\nCV &lt; 12% = reliable (green coding)\nCV 12-40% = somewhat reliable (yellow)\nCV &gt; 40% = unreliable (red coding)\n\nReduce statistical uncertainty - Collapse data detail, aggregate geographies, use multi-year estimates\nAlways conduct statistical significance tests when comparing ACS estimates over time\n\nKey insight: These practices are not just technical best practices—they are ethical requirements under the AICP Code of Ethics"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#eda-for-policy-analysis",
    "href": "labs/lab1/week-03/lecture/week3.html#eda-for-policy-analysis",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "EDA for Policy Analysis",
    "text": "EDA for Policy Analysis\nKey questions for census data:\n\nGeographic patterns: Are problems concentrated in certain areas?\nPopulation relationships: How does size affect data quality?\nDemographic patterns: Are certain communities systematically different?\nTemporal trends: How do patterns change over time?\nData integrity: Where might survey bias affect results?\nReliability assessment: Which estimates should we trust?"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#why-join-data",
    "href": "labs/lab1/week-03/lecture/week3.html#why-join-data",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Why Join Data?",
    "text": "Why Join Data?\nTo combining datasets of course:\n\nCensus demographics + Economic indicators\nSurvey responses + Geographic boundaries\n\nCurrent data + Historical trends\nAdministrative records + Survey data"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#types-of-joins-tabular",
    "href": "labs/lab1/week-03/lecture/week3.html#types-of-joins-tabular",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Types of Joins (tabular)",
    "text": "Types of Joins (tabular)\nFour main types in dplyr:\n\nleft_join() - Keep all rows from left dataset\nright_join() - Keep all rows from right dataset\n\ninner_join() - Keep only rows that match in both\nfull_join() - Keep all rows from both datasets\n\nMost common: left_join() to add columns to your main dataset"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#live-demo-joining-census-tables",
    "href": "labs/lab1/week-03/lecture/week3.html#live-demo-joining-census-tables",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Live Demo: Joining Census Tables",
    "text": "Live Demo: Joining Census Tables\n\n\n# A tibble: 6 × 6\n  GEOID NAME                    median_income income_moe college_pop college_moe\n  &lt;chr&gt; &lt;chr&gt;                           &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams County, Pennsylv…         78975       3334       10195         761\n2 42003 Allegheny County, Penn…         72537        869      229538        3311\n3 42005 Armstrong County, Penn…         61011       2202        6171         438\n4 42007 Beaver County, Pennsyl…         67194       1531       22588        1012\n5 42009 Bedford County, Pennsy…         58337       2606        3396         307\n6 42011 Berks County, Pennsylv…         74617       1191       50120        1654"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#checking-join-results-and-data-quality",
    "href": "labs/lab1/week-03/lecture/week3.html#checking-join-results-and-data-quality",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Checking Join Results and Data Quality",
    "text": "Checking Join Results and Data Quality\nAlways verify joins AND assess combined reliability:\n\n\nIncome data rows: 67 \n\n\nEducation data rows: 67 \n\n\nCombined data rows: 67 \n\n\n# A tibble: 1 × 2\n  missing_income missing_education\n           &lt;int&gt;             &lt;int&gt;\n1              0                 0\n\n\n# A tibble: 6 × 3\n  NAME                           income_cv college_cv\n  &lt;chr&gt;                              &lt;dbl&gt;      &lt;dbl&gt;\n1 Adams County, Pennsylvania          4.22       7.46\n2 Allegheny County, Pennsylvania      1.20       1.44\n3 Armstrong County, Pennsylvania      3.61       7.10\n4 Beaver County, Pennsylvania         2.28       4.48\n5 Bedford County, Pennsylvania        4.47       9.04\n6 Berks County, Pennsylvania          1.60       3.30"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#lab-structure-for-today",
    "href": "labs/lab1/week-03/lecture/week3.html#lab-structure-for-today",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Lab Structure for Today",
    "text": "Lab Structure for Today\nYou’ll work through six exercises:\n\nFinding Census Variables - Learn to search for the data you need\nSingle Variable EDA - Explore distributions and identify outliers\nTwo Variable Relationships - Create meaningful scatter plots\nData Quality Visualization - Practice ethical uncertainty communication\nMultiple Variables - Color, faceting, and complex relationships\nData Integration - Join datasets and create publication-ready visualizations"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#skills-youll-practice",
    "href": "labs/lab1/week-03/lecture/week3.html#skills-youll-practice",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Skills You’ll Practice",
    "text": "Skills You’ll Practice\nggplot2 fundamentals:\n\nScatter plots, histograms, boxplots\nAesthetic mappings and customization\nProfessional themes and labels\n\nEDA workflow:\n\nDistribution analysis\nOutlier detection\n\nPattern identification\n\nEthical data practice:\n\nVisualizing and reporting margins of error\nUsing coefficient of variation to assess reliability"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#connection-to-professional-ethics",
    "href": "labs/lab1/week-03/lecture/week3.html#connection-to-professional-ethics",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Connection to Professional Ethics",
    "text": "Connection to Professional Ethics\nBy the end of today, you’ll be able to:\n\nVisually assess data quality issues\nCreate compelling presentations of demographic patterns\nCommunicate statistical uncertainty ethically and clearly\nIntegrate multiple data sources"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#questions-before-we-begin",
    "href": "labs/lab1/week-03/lecture/week3.html#questions-before-we-begin",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Questions Before We Begin?",
    "text": "Questions Before We Begin?\nReady for hands-on practice?\nRemember: Today’s skills build directly on Week 1-2 foundations:\n\nSame dplyr functions, now with visualization\nSame census data concepts, now with multiple tables\n\nLet’s create some beautiful graphs"
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n# Set your Census API key if you haven't already\ncensus_api_key(\"ec702835845a134b4376c60759aa72ce62f6df59\")\n\n# We'll use Pennsylvania data for consistency with previous weeks\nstate_choice &lt;- \"PA\""
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#setup-and-data-loading",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#setup-and-data-loading",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n# Set your Census API key if you haven't already\ncensus_api_key(\"ec702835845a134b4376c60759aa72ce62f6df59\")\n\n# We'll use Pennsylvania data for consistency with previous weeks\nstate_choice &lt;- \"PA\""
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-0-finding-census-variable-codes",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-0-finding-census-variable-codes",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 0: Finding Census Variable Codes",
    "text": "Exercise 0: Finding Census Variable Codes\nThe Challenge: You know you want data on total population, median income, and median age, but you don’t know the specific Census variable codes. How do you find them?\n\n0.1 Load the Variable Dictionary\n\n# Load all available variables for ACS 5-year 2022\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\n\n# Look at the structure\nglimpse(acs_vars_2022)\n\nRows: 28,152\nColumns: 4\n$ name      &lt;chr&gt; \"B01001A_001\", \"B01001A_002\", \"B01001A_003\", \"B01001A_004\", …\n$ label     &lt;chr&gt; \"Estimate!!Total:\", \"Estimate!!Total:!!Male:\", \"Estimate!!To…\n$ concept   &lt;chr&gt; \"Sex by Age (White Alone)\", \"Sex by Age (White Alone)\", \"Sex…\n$ geography &lt;chr&gt; \"tract\", \"tract\", \"tract\", \"tract\", \"tract\", \"tract\", \"tract…\n\nhead(acs_vars_2022)\n\n# A tibble: 6 × 4\n  name        label                                   concept          geography\n  &lt;chr&gt;       &lt;chr&gt;                                   &lt;chr&gt;            &lt;chr&gt;    \n1 B01001A_001 Estimate!!Total:                        Sex by Age (Whi… tract    \n2 B01001A_002 Estimate!!Total:!!Male:                 Sex by Age (Whi… tract    \n3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years  Sex by Age (Whi… tract    \n4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years   Sex by Age (Whi… tract    \n5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years Sex by Age (Whi… tract    \n6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years Sex by Age (Whi… tract    \n\n\nWhat you see:\n\nname: The variable code (e.g., “B01003_001”)\nlabel: Human-readable description\nconcept: The broader table this variable belongs to\n\n\n\n0.2 Search for Population Variables\nYour Task: Find the variable code for total population.\n\n# Search for population-related variables\npopulation_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"Total.*population\"))\n\n# Look at the results\nhead(population_vars, 10)\n\n# A tibble: 10 × 4\n   name       label                                            concept geography\n   &lt;chr&gt;      &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n 1 B16008_002 \"Estimate!!Total:!!Native population:\"           Citize… tract    \n 2 B16008_003 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 3 B16008_004 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 4 B16008_005 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 5 B16008_006 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 6 B16008_007 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 7 B16008_008 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 8 B16008_009 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 9 B16008_010 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n10 B16008_011 \"Estimate!!Total:!!Native population:!!18 years… Citize… tract    \n\n# Or search in the concept field\npop_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Total Population\"))\n\nhead(pop_concept)\n\n# A tibble: 6 × 4\n  name        label                             concept                geography\n  &lt;chr&gt;       &lt;chr&gt;                             &lt;chr&gt;                  &lt;chr&gt;    \n1 B01003_001  Estimate!!Total                   Total Population       block gr…\n2 B25008A_001 Estimate!!Total:                  Total Population in O… block gr…\n3 B25008A_002 Estimate!!Total:!!Owner occupied  Total Population in O… block gr…\n4 B25008A_003 Estimate!!Total:!!Renter occupied Total Population in O… block gr…\n5 B25008B_001 Estimate!!Total:                  Total Population in O… block gr…\n6 B25008B_002 Estimate!!Total:!!Owner occupied  Total Population in O… block gr…\n\n\nTip: Look for “Total” followed by “population” - usually B01003_001\n\n\n0.3 Search for Income Variables\nYour Task: Find median household income variables.\n\n# Search for median income\nincome_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*income\"))\n\n# Look specifically for household income\nhousehold_income &lt;- income_vars %&gt;%\n  filter(str_detect(label, \"household\"))\n\nprint(\"Household income variables:\")\n\n[1] \"Household income variables:\"\n\nhead(household_income)\n\n# A tibble: 6 × 4\n  name        label                                            concept geography\n  &lt;chr&gt;       &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n1 B10010_002  Estimate!!Median family income in the past 12 m… Median… tract    \n2 B10010_003  Estimate!!Median family income in the past 12 m… Median… tract    \n3 B19013A_001 Estimate!!Median household income in the past 1… Median… tract    \n4 B19013B_001 Estimate!!Median household income in the past 1… Median… tract    \n5 B19013C_001 Estimate!!Median household income in the past 1… Median… tract    \n6 B19013D_001 Estimate!!Median household income in the past 1… Median… tract    \n\n# Alternative: search by concept\nincome_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Median Household Income\"))\n\nhead(income_concept)\n\n# A tibble: 6 × 4\n  name        label                                            concept geography\n  &lt;chr&gt;       &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n1 B19013A_001 Estimate!!Median household income in the past 1… Median… tract    \n2 B19013B_001 Estimate!!Median household income in the past 1… Median… tract    \n3 B19013C_001 Estimate!!Median household income in the past 1… Median… tract    \n4 B19013D_001 Estimate!!Median household income in the past 1… Median… tract    \n5 B19013E_001 Estimate!!Median household income in the past 1… Median… county   \n6 B19013F_001 Estimate!!Median household income in the past 1… Median… tract    \n\n\nPattern Recognition: Median household income is typically B19013_001\n\n\n0.4 Search for Age Variables\nYour Task: Find median age variables."
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-1-single-variable-eda",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-1-single-variable-eda",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 1: Single Variable EDA",
    "text": "Exercise 1: Single Variable EDA\n\n1.1 Load and Inspect Data\n\n# Get county-level data for your state\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_pop = \"B01003_001\",       # Total population\n    median_income = \"B19013_001\",   # Median household income\n    median_age = \"B01002_001\"       # Median age\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(county_name = str_remove(NAME, paste0(\", Pennsylvania\")))\n\n# Basic inspection\nglimpse(county_data)\n\nRows: 67\nColumns: 9\n$ GEOID          &lt;chr&gt; \"42001\", \"42003\", \"42005\", \"42007\", \"42009\", \"42011\", \"…\n$ NAME           &lt;chr&gt; \"Adams County, Pennsylvania\", \"Allegheny County, Pennsy…\n$ total_popE     &lt;dbl&gt; 104604, 1245310, 65538, 167629, 47613, 428483, 122640, …\n$ total_popM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ median_incomeE &lt;dbl&gt; 78975, 72537, 61011, 67194, 58337, 74617, 59386, 60650,…\n$ median_incomeM &lt;dbl&gt; 3334, 869, 2202, 1531, 2606, 1191, 2058, 2167, 1516, 21…\n$ median_ageE    &lt;dbl&gt; 43.8, 40.6, 47.0, 44.9, 47.3, 39.9, 42.9, 43.9, 44.0, 4…\n$ median_ageM    &lt;dbl&gt; 0.2, 0.1, 0.2, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, …\n$ county_name    &lt;chr&gt; \"Adams County\", \"Allegheny County\", \"Armstrong County\",…\n\n\n\n\n1.2 Explore Income Distribution\nYour Task: Create a histogram of median household income and describe what you see.\n\n# Create histogram of median income\nggplot(county_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"lightpink\", alpha = 0.5) +\n  labs(\n    title = \"Distribution of Median Household Income\",\n    x = \"Median Household Income ($)\",\n    y = \"Number of Counties\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n1.3 Box Plot for Outlier Detection\nYour Task: Create a boxplot to identify specific outlier counties.\n\n# Box plot to see outliers clearly\nggplot(county_data) +\n  aes(y = median_incomeE) +\n  geom_boxplot(fill = \"lightpink\", width = 0.5) +\n  labs(\n    title = \"Median Income Distribution with Outliers\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Identify the outlier counties\nincome_outliers &lt;- county_data %&gt;%\n  mutate(\n    Q1 = quantile(median_incomeE, 0.25, na.rm = TRUE),\n    Q3 = quantile(median_incomeE, 0.75, na.rm = TRUE),\n    IQR = Q3 - Q1,\n    outlier = median_incomeE &lt; (Q1 - 1.5 * IQR) | median_incomeE &gt; (Q3 + 1.5 * IQR)\n  ) %&gt;%\n  filter(outlier) %&gt;%\n  select(county_name, median_incomeE)\n\nprint(\"Outlier counties:\")\n\n[1] \"Outlier counties:\"\n\nincome_outliers\n\n# A tibble: 3 × 2\n  county_name       median_incomeE\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 Bucks County              107826\n2 Chester County            118574\n3 Montgomery County         107441\n\n\n\n\n1.4 Challenge Exercise: Population Distribution\nYour Task: Create your own visualization of population distribution and identify outliers.\nRequirements:\n\nCreate a histogram of total population (total_popE)\nUse a different color than the income example (try “darkgreen” or “purple”)\nAdd appropriate labels and title\nCreate a boxplot to identify population outliers\nFind and list the 3 most populous and 3 least populous counties\n\n\n# Create histogram of total population\nggplot(county_data) +\n  aes(x = total_popE) +\n  geom_histogram(bins = 15, fill = \"lightpink\", alpha = 0.7) +\n  labs(\n    title = \"Total Population Distribution\",\n    x = \"Population\",\n    y = \"Number of Counties\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels=comma)\n\n\n\n\n\n\n\n\n\n# Box plot to see outliers clearly\nggplot(county_data) +\n  aes(y = total_popE) +\n  geom_boxplot(fill = \"lightpink\", width = 0.5) +\n  labs(\n    title = \"Total population distribution with Outliers\",\n    y = \"Population\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels=comma)\n\n\n\n\n\n\n\n# Identify the outlier counties\npopulation_outliers &lt;- county_data %&gt;%\n  mutate(\n    Q1 = quantile(total_popE, 0.25, na.rm = TRUE),\n    Q3 = quantile(total_popE, 0.75, na.rm = TRUE),\n    IQR = Q3 - Q1,\n    outlier = total_popE &lt; (Q1 - 1.5 * IQR) | total_popE &gt; (Q3 + 1.5 * IQR)\n  ) %&gt;%\n  filter(outlier) %&gt;%\n  select(county_name, total_popE)\n\nprint(\"Outlier counties:\")\n\n[1] \"Outlier counties:\"\n\npopulation_outliers\n\n# A tibble: 7 × 2\n  county_name         total_popE\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Allegheny County       1245310\n2 Bucks County            645163\n3 Chester County          536474\n4 Delaware County         575312\n5 Lancaster County        553202\n6 Montgomery County       856399\n7 Philadelphia County    1593208"
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-2-two-variable-relationships",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-2-two-variable-relationships",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 2: Two Variable Relationships",
    "text": "Exercise 2: Two Variable Relationships\n\n2.1 Population vs Income Scatter Plot\nYour Task: Explore the relationship between population size and median income.\n\n# Basic scatter plot\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point() +\n  labs(\n    title = \"Population vs Median Income\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n2.2 Add Trend Line and Labels\nYour Task: Improve the plot by adding a trend line and labeling interesting points.\n\n# Enhanced scatter plot with trend line\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"lightpink\") +\n  labs(\n    title = \"Population vs Median Income in Pennsylvania Counties\",\n    subtitle = \"2018-2022 ACS 5-Year Estimates\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\",\n    caption = \"Source: U.S. Census Bureau\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Calculate correlation\ncorrelation &lt;- cor(county_data$total_popE, county_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Correlation coefficient:\", round(correlation, 3)))\n\n[1] \"Correlation coefficient: 0.457\"\n\n\n\n\n2.3 Deal with Skewed Data\nYour Task: The population data is highly skewed. Try a log transformation.\n\n# Log-transformed scatter plot\nggplot(county_data) +\n  aes(x = log(total_popE), y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Log(Population) vs Median Income\",\n    x = \"Log(Total Population)\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\nQuestion: Does the log transformation reveal a clearer relationship? Yes. It’s a positive slope, it suggests that more populous counties tend to have higher median household incomes.\n\n\n2.4 Challenge Exercise: Age vs Income Relationship\nYour Task: Explore the relationship between median age and median income using different visualization techniques.\nRequirements:\n\nCreate a scatter plot with median age on x-axis and median income on y-axis\nUse red points (color = \"red\") with 50% transparency (alpha = 0.5)\nAdd a smooth trend line using method = \"loess\" instead of “lm”\nUse the “dark” theme (theme_dark())\nFormat the y-axis with dollar signs\nAdd a title that mentions both variables\n\n\n# Enhanced scatter plot with trend line\nggplot(county_data) +\n  aes(x = median_ageE, y = median_incomeE) +\n  geom_point(alpha = 0.5, color = \"red\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Median Age vs Median Income in Pennsylvania Counties\",\n    subtitle = \"2018-2022 ACS 5-Year Estimates\",\n    x = \"Median Age\",\n    y = \"Median Household Income ($)\",\n    caption = \"Source: U.S. Census Bureau\"\n  ) +\n  theme_dark() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Calculate correlation\ncorrelation &lt;- cor(county_data$total_popE, county_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Correlation coefficient:\", round(correlation, 3)))\n\n[1] \"Correlation coefficient: 0.457\""
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-3-data-quality-visualization",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-3-data-quality-visualization",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 3: Data Quality Visualization",
    "text": "Exercise 3: Data Quality Visualization\n\n3.1 Visualize Margins of Error\nYour Task: Create a visualization showing how data reliability varies across counties.\n\n# Calculate MOE percentages\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    income_moe_pct = (median_incomeM / median_incomeE) * 100,\n    pop_category = case_when(\n      total_popE &lt; 50000 ~ \"Small (&lt;50K)\",\n      total_popE &lt; 200000 ~ \"Medium (50K-200K)\",\n      TRUE ~ \"Large (200K+)\"\n    )\n  )\n\n# MOE by population size\nggplot(county_reliability) +\n  aes(x = total_popE, y = income_moe_pct) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Data Reliability Decreases with Population Size\",\n    x = \"Total Population\",\n    y = \"Margin of Error (%)\",\n    caption = \"Red line = 10% reliability threshold\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\n\n\n3.2 Compare Reliability by County Size\nYour Task: Use box plots to compare MOE across county size categories.\n\n# Box plots by population category\nggplot(county_reliability) +\n  aes(x = pop_category, y = income_moe_pct, fill = pop_category) +\n  geom_boxplot() +\n  labs(\n    title = \"Data Reliability by County Size Category\",\n    x = \"Population Category\",\n    y = \"Margin of Error (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")  # Remove legend since x-axis is clear\n\n\n\n\n\n\n\n\n\n\n3.3 Challenge Exercise: Age Data Reliability\nYour Task: Analyze the reliability of median age data across counties.\nRequirements:\n\nCalculate MOE percentage for median age (median_ageM / median_ageE * 100)\nCreate a scatter plot showing population vs age MOE percentage\nUse purple points (color = \"purple\") with size = 2\nAdd a horizontal line at 5% MOE using geom_hline() with a blue dashed line\nUse theme_classic()instead of theme_minimal()\nCreate a boxplot comparing age MOE across the three population categories\n\n\n# Calculate MOE percentages\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    age_moe_pct = (median_ageM / median_ageE) * 100,\n    pop_category = case_when(\n      median_ageE &lt; 40 ~ \"Small (&lt;40)\",\n      median_ageE &lt; 50 ~ \"Medium (40-50)\",\n      TRUE ~ \"Large (&gt;50)\"\n    )\n  )\n\n# MOE by population size\nggplot(county_reliability) +\n  aes(x = median_ageE, y = age_moe_pct) +\n  geom_point(alpha = 0.7, color = \"purple\",size = 2) +\n  geom_hline(yintercept = 5, color = \"blue\", linetype = \"dashed\",size = 0.5 ) +\n  labs(\n    title = \"Data Reliability of median age data across\ncounties.\",\n    x = \"Median Age\",\n    y = \"Margin of Error (%)\",\n    caption = \"Red line = 5% reliability threshold\"\n  ) +\n  theme_classic() +\n  scale_x_continuous(labels = comma)\n\n\n\n\n\n\n\n\n\n# Box plots by population category\nggplot(county_reliability) +\n  aes(x = pop_category, y = age_moe_pct, fill = pop_category) +\n  geom_boxplot() +\n  labs(\n    title = \"age MOE across the three population categories\",\n    x = \"Population Category\",\n    y = \"Margin of Error (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")  # Remove legend since x-axis is clear"
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-4-multiple-variables-with-color-and-faceting",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-4-multiple-variables-with-color-and-faceting",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 4: Multiple Variables with Color and Faceting",
    "text": "Exercise 4: Multiple Variables with Color and Faceting\n\n4.1 Three-Variable Scatter Plot\nYour Task: Add median age as a color dimension to the population-income relationship.\n\n# Three-variable scatter plot\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE, color = median_ageE) +\n  geom_point(size = 2, alpha = 0.7) +\n  scale_color_viridis_c(name = \"Median\\nAge\") +\n  labs(\n    title = \"Population, Income, and Age Patterns\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n4.2 Create Categories for Faceting\nYour Task: Create age categories and use faceting to compare patterns.\n\n# Create age categories and faceted plot\ncounty_faceted &lt;- county_data %&gt;%\n  mutate(\n    age_category = case_when(\n      median_ageE &lt; 40 ~ \"Young (&lt; 40)\",\n      median_ageE &lt; 45 ~ \"Middle-aged (40-45)\",\n      TRUE ~ \"Older (45+)\"\n    )\n  )\n\nggplot(county_faceted) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~age_category) +\n  labs(\n    title = \"Population-Income Relationship by Age Profile\",\n    x = \"Total Population\",\n    y = \"Median Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\nQuestion: Do the relationships between population and income differ by age profile? Yes\nYour Task: Create a visualization using income categories and multiple aesthetic mappings.\nRequirements:\n\nCreate income categories: “Low” (&lt;$50k), “Middle” ($50k-$80k), “High” (&gt;$80k)\nMake a scatter plot with population (x) vs median age (y) - Color points by income category\nSize points by the margin of error for income (median_incomeM)\nUse the “Set2” color palette: scale_color_brewer(palette = \"Set2\") **note: you’ll need to load the RColorBrewer package for this`\nFacet by income category using facet_wrap()\nUse theme_bw() theme\n\n\n# Create income categories\ncounty_income_plot &lt;- county_data %&gt;%\n  mutate(\n    income_cat = case_when(\n      median_incomeE &lt; 50000 ~ \"Low (&lt;$50K)\",\n      median_incomeE &lt; 80000 ~ \"Middle ($50K–$80K)\",\n      TRUE ~ \"High (&gt;$80K)\"\n    )\n  )\n\n# Scatter plot\nggplot(county_income_plot) +\n  aes(\n    x = total_popE,\n    y = median_ageE,\n    color = income_cat,\n    size = median_incomeM\n  ) +\n  geom_point(alpha = 0.7) +\n  scale_color_brewer(palette = \"Set2\", name = \"Income Category\") +\n  scale_size_continuous(name = \"Income MOE\") +\n  labs(\n    title = \"Population vs Median Age by Income Category\",\n    x = \"Total Population\",\n    y = \"Median Age\"\n  ) +\n  facet_wrap(~ income_cat) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n  scale_x_continuous(labels = scales::comma)\n\n&lt;ScaleContinuousPosition&gt;\n Range:  \n Limits:    0 --    1"
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-5-data-joins-and-integration",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-5-data-joins-and-integration",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 5: Data Joins and Integration",
    "text": "Exercise 5: Data Joins and Integration\n\n5.1 Get Additional Census Data\nYour Task: Load educational attainment data and join it with our existing data.\n\n# Get educational attainment data\neducation_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_25plus = \"B15003_001\",    # Total population 25 years and over\n    bachelor_plus = \"B15003_022\"    # Bachelor's degree or higher\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(\n    pct_college = (bachelor_plusE / total_25plusE) * 100,\n    county_name = str_remove(NAME, paste0(\", \", state_choice))\n  ) %&gt;%\n  select(GEOID, county_name, pct_college)\n\n# Check the data\nhead(education_data)\n\n# A tibble: 6 × 3\n  GEOID county_name                    pct_college\n  &lt;chr&gt; &lt;chr&gt;                                &lt;dbl&gt;\n1 42001 Adams County, Pennsylvania           13.9 \n2 42003 Allegheny County, Pennsylvania       25.4 \n3 42005 Armstrong County, Pennsylvania       12.7 \n4 42007 Beaver County, Pennsylvania          18.3 \n5 42009 Bedford County, Pennsylvania          9.73\n6 42011 Berks County, Pennsylvania           17.2 \n\n\n\n\n5.2 Join the Datasets\nYour Task: Join the education data with our main county dataset.\n\n# Perform the join\ncombined_data &lt;- county_data %&gt;%\n  left_join(education_data, by = \"GEOID\")\n\n# Check the join worked\ncat(\"Original data rows:\", nrow(county_data), \"\\n\")\n\nOriginal data rows: 67 \n\ncat(\"Combined data rows:\", nrow(combined_data), \"\\n\")\n\nCombined data rows: 67 \n\ncat(\"Missing education data:\", sum(is.na(combined_data$pct_college)), \"\\n\")\n\nMissing education data: 0 \n\n# View the combined data\nhead(combined_data)\n\n# A tibble: 6 × 11\n  GEOID NAME     total_popE total_popM median_incomeE median_incomeM median_ageE\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams C…     104604         NA          78975           3334        43.8\n2 42003 Alleghe…    1245310         NA          72537            869        40.6\n3 42005 Armstro…      65538         NA          61011           2202        47  \n4 42007 Beaver …     167629         NA          67194           1531        44.9\n5 42009 Bedford…      47613         NA          58337           2606        47.3\n6 42011 Berks C…     428483         NA          74617           1191        39.9\n# ℹ 4 more variables: median_ageM &lt;dbl&gt;, county_name.x &lt;chr&gt;,\n#   county_name.y &lt;chr&gt;, pct_college &lt;dbl&gt;\n\n\n\n\n5.3 Analyze the New Relationship\nYour Task: Explore the relationship between education and income.\n\n# Education vs Income scatter plot\nggplot(combined_data) +\n  aes(x = pct_college, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Education vs Income Across Counties\",\n    x = \"Percent with Bachelor's Degree or Higher\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Calculate correlation\nedu_income_cor &lt;- cor(combined_data$pct_college, combined_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Education-Income Correlation:\", round(edu_income_cor, 3)))\n\n[1] \"Education-Income Correlation: 0.811\"\n\n\n\n\n5.4 Get Housing Data and Triple Join\nYour Task: Add housing cost data to create a three-way analysis.\n\n# Get housing cost data\nhousing_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_rent = \"B25058_001\",     # Median contract rent\n    median_home_value = \"B25077_001\" # Median value of owner-occupied units\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  select(GEOID, median_rent = median_rentE, median_home_value = median_home_valueE)\n\n# Join all three datasets\nfull_data &lt;- combined_data %&gt;%\n  left_join(housing_data, by = \"GEOID\")\n\n# Create a housing affordability measure\nfull_data &lt;- full_data %&gt;%\n  mutate(\n    rent_to_income = (median_rent * 12) / median_incomeE * 100,\n    income_category = case_when(\n      median_incomeE &lt; 50000 ~ \"Low Income\",\n      median_incomeE &lt; 80000 ~ \"Middle Income\",\n      TRUE ~ \"High Income\"\n    )\n  )\nhead(full_data)\n\n# A tibble: 6 × 15\n  GEOID NAME     total_popE total_popM median_incomeE median_incomeM median_ageE\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams C…     104604         NA          78975           3334        43.8\n2 42003 Alleghe…    1245310         NA          72537            869        40.6\n3 42005 Armstro…      65538         NA          61011           2202        47  \n4 42007 Beaver …     167629         NA          67194           1531        44.9\n5 42009 Bedford…      47613         NA          58337           2606        47.3\n6 42011 Berks C…     428483         NA          74617           1191        39.9\n# ℹ 8 more variables: median_ageM &lt;dbl&gt;, county_name.x &lt;chr&gt;,\n#   county_name.y &lt;chr&gt;, pct_college &lt;dbl&gt;, median_rent &lt;dbl&gt;,\n#   median_home_value &lt;dbl&gt;, rent_to_income &lt;dbl&gt;, income_category &lt;chr&gt;\n\n\n\n\n5.5 Advanced Multi-Variable Analysis\nYour Task: Create a comprehensive visualization showing multiple relationships.\n\n# Complex multi-variable plot\nggplot(full_data) +\n  aes(x = pct_college, y = rent_to_income, \n      color = income_category, size = total_popE) +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Education, Housing Affordability, and Income Patterns\",\n    subtitle = \"Larger points = larger population\",\n    x = \"Percent with Bachelor's Degree or Higher\",\n    y = \"Annual Rent as % of Median Income\",\n    color = \"Income Category\",\n    size = \"Population\"\n  ) +\n  theme_minimal() +\n  guides(size = guide_legend(override.aes = list(alpha = 1)))"
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-6-publication-ready-visualization",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-6-publication-ready-visualization",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 6: Publication-Ready Visualization",
    "text": "Exercise 6: Publication-Ready Visualization\n\n6.1 Create a Policy-Focused Visualization\nYour Task: Combine multiple visualizations to tell a more complete story about county characteristics.\n\n# Create a multi-panel figure\nlibrary(patchwork)  # For combining plots\n\n# Plot 1: Income distribution\np1 &lt;- ggplot(full_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"A) Income Distribution\", \n       x = \"Median Income ($)\", y = \"Counties\") +\n  scale_x_continuous(labels = dollar) +\n  theme_minimal()\n\n# Plot 2: Education vs Income\np2 &lt;- ggplot(full_data) +\n  aes(x = pct_college, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"B) Education vs Income\",\n       x = \"% College Educated\", y = \"Median Income ($)\") +\n  scale_y_continuous(labels = dollar) +\n  theme_minimal()\n\n# Plot 3: Housing affordability by income category\np3 &lt;- ggplot(full_data) +\n  aes(x = income_category, y = rent_to_income, fill = income_category) +\n  geom_boxplot() +\n  labs(title = \"C) Housing Affordability by Income\",\n       x = \"Income Category\", y = \"Rent as % of Income\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Plot 4: Data reliability by population\np4 &lt;- ggplot(\n  county_data %&gt;%\n    mutate(income_moe_pct = (median_incomeM / median_incomeE) * 100)\n) +\n  aes(x = total_popE, y = income_moe_pct) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"D) Data Reliability\",\n       x = \"Population\", y = \"MOE (%)\") +\n  scale_x_continuous(labels = comma) +\n  theme_minimal()\n\n\n# Combine all plots\ncombined_plot &lt;- (p1 | p2) / (p3 | p4)\ncombined_plot + plot_annotation(\n  title = \"Pennsylvania County Analysis: Income, Education, and Housing Patterns\",\n  caption = \"Source: American Community Survey 2018-2022\"\n)"
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-7-ethical-data-communication---implementing-research-recommendations",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-7-ethical-data-communication---implementing-research-recommendations",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 7: Ethical Data Communication - Implementing Research Recommendations",
    "text": "Exercise 7: Ethical Data Communication - Implementing Research Recommendations\nBackground: Research by Jurjevich et al. (2018) found that only 27% of planners warn users about unreliable ACS data, violating AICP ethical standards. In this exercise, you’ll practice the five research-based guidelines for ethical ACS data communication.\n\n7.1 Create Professional Data Tables with Uncertainty\nYour Task: Follow the Jurjevich et al. guidelines to create an ethical data presentation.\n\n# Get comprehensive data for ethical analysis\nethical_demo_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",   # Median household income\n    total_25plus = \"B15003_001\",    # Total population 25 years and over\n    bachelor_plus = \"B15003_022\",   # Bachelor's degree or higher\n    total_pop = \"B01003_001\"        # Total population\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(\n    # Calculate derived statistics\n    pct_college = (bachelor_plusE / total_25plusE) * 100,\n    \n    # Calculate MOE for percentage using error propagation\n    pct_college_moe = pct_college * sqrt((bachelor_plusM/bachelor_plusE)^2 + (total_25plusM/total_25plusE)^2),\n    \n    # Calculate coefficient of variation for all key variables\n    income_cv = (median_incomeM / median_incomeE) * 100,\n    education_cv = (pct_college_moe / pct_college) * 100,\n    \n    # Create reliability categories based on CV\n    income_reliability = case_when(\n      income_cv &lt; 12 ~ \"High\",\n      income_cv &lt;= 40 ~ \"Moderate\", \n      TRUE ~ \"Low\"\n    ),\n    \n    education_reliability = case_when(\n      education_cv &lt; 12 ~ \"High\",\n      education_cv &lt;= 40 ~ \"Moderate\",\n      TRUE ~ \"Low\"\n    ),\n    \n    # Create color coding for reliability\n    income_color = case_when(\n      income_reliability == \"High\" ~ \"🟢\",\n      income_reliability == \"Moderate\" ~ \"🟡\",\n      TRUE ~ \"🔴\"\n    ),\n    \n    education_color = case_when(\n      education_reliability == \"High\" ~ \"🟢\",\n      education_reliability == \"Moderate\" ~ \"🟡\", \n      TRUE ~ \"🔴\"\n    ),\n    \n    # Clean county names\n    county_name = str_remove(NAME, paste0(\", \", state_choice))\n  )\n\n# Create ethical data table focusing on least reliable estimates\nethical_data_table &lt;- ethical_demo_data %&gt;%\n  select(county_name, median_incomeE, median_incomeM, income_cv, income_color,\n         pct_college, pct_college_moe, education_cv, education_color) %&gt;%\n  arrange(desc(income_cv)) %&gt;%  # Show least reliable first\n  slice_head(n = 10)\n\n# Create professional table following guidelines\nlibrary(knitr)\nlibrary(kableExtra)\n\nethical_data_table %&gt;%\n  select(county_name, median_incomeE, median_incomeM, income_cv, income_color) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \n                  \"CV (%)\", \"Reliability\"),\n    caption = \"Pennsylvania Counties: Median Household Income with Statistical Uncertainty\",\n    format.args = list(big.mark = \",\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  footnote(\n    general = c(\"Coefficient of Variation (CV) indicates reliability:\",\n                \"🟢 High reliability (CV &lt; 12%)\",\n                \"🟡 Moderate reliability (CV 12-40%)\", \n                \"🔴 Low reliability (CV &gt; 40%)\",\n                \"Following Jurjevich et al. (2018) research recommendations\",\n                \"Source: American Community Survey 2018-2022 5-Year Estimates\"),\n    general_title = \"Notes:\"\n  )\n\n\nPennsylvania Counties: Median Household Income with Statistical Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nCV (%)\nReliability\n\n\n\n\nForest County, Pennsylvania\n46,188\n4,612\n9.985278\n🟢 |\n\n\nSullivan County, Pennsylvania\n62,910\n5,821\n9.252901\n🟢 |\n\n\nUnion County, Pennsylvania\n64,914\n4,753\n7.321995\n🟢 |\n\n\nMontour County, Pennsylvania\n72,626\n5,146\n7.085617\n🟢 |\n\n\nElk County, Pennsylvania\n61,672\n4,091\n6.633480\n🟢 |\n\n\nGreene County, Pennsylvania\n66,283\n4,247\n6.407374\n🟢 |\n\n\nCameron County, Pennsylvania\n46,186\n2,605\n5.640237\n🟢 |\n\n\nSnyder County, Pennsylvania\n65,914\n3,666\n5.561793\n🟢 |\n\n\nCarbon County, Pennsylvania\n64,538\n3,424\n5.305402\n🟢 |\n\n\nWarren County, Pennsylvania\n57,925\n3,005\n5.187743\n🟢 |\n\n\n\nNotes:\n\n\n\n\n\n\n Coefficient of Variation (CV) indicates reliability:\n\n\n\n\n\n\n 🟢 High reliability (CV &lt; 12%)\n\n\n\n\n\n\n 🟡 Moderate reliability (CV 12-40%)\n\n\n\n\n\n\n 🔴 Low reliability (CV &gt; 40%)\n\n\n\n\n\n\n Following Jurjevich et al. (2018) research recommendations\n\n\n\n\n\n\n Source: American Community Survey 2018-2022 5-Year Estimates\n\n\n\n\n\n\n\n\n\n\n\n\n7.3 Now try Census Tracts\n\n# Get census tract poverty data for Philadelphia\nphilly_poverty &lt;- get_acs(\n    geography = \"tract\",\n    variables = c(\n      poverty_pop = \"B17001_001\",     \n      poverty_below = \"B17001_002\"    \n    ),\n    state = \"PA\",\n    county = \"101\",\n    year = 2022,\n    output = \"wide\"\n  ) %&gt;%\n  filter(poverty_popE &gt; 0) %&gt;%  # Remove tracts with no poverty data\n  mutate(\n    # Calculate poverty rate and its MOE\n    poverty_rate = (poverty_belowE / poverty_popE) * 100,\n    \n    # MOE for derived percentage using error propagation\n    poverty_rate_moe = poverty_rate * sqrt((poverty_belowM/poverty_belowE)^2 + (poverty_popM/poverty_popE)^2),\n    \n    # Coefficient of variation\n    poverty_cv = (poverty_rate_moe / poverty_rate) * 100,\n    \n    # Reliability assessment\n    reliability = case_when(\n      poverty_cv &lt; 12 ~ \"High\",\n      poverty_cv &lt;= 40 ~ \"Moderate\",\n      poverty_cv &lt;= 75 ~ \"Low\",\n      TRUE ~ \"Very Low\"\n    ),\n    \n    # Color coding\n    reliability_color = case_when(\n      reliability == \"High\" ~ \"🟢\",\n      reliability == \"Moderate\" ~ \"🟡\",\n      reliability == \"Low\" ~ \"🟠\",\n      TRUE ~ \"🔴\"\n    ),\n    \n    # Population size categories\n    pop_category = case_when(\n      poverty_popE &lt; 500 ~ \"Very Small (&lt;500)\",\n      poverty_popE &lt; 1000 ~ \"Small (500-1000)\",\n      poverty_popE &lt; 1500 ~ \"Medium (1000-1500)\",\n      TRUE ~ \"Large (1500+)\"\n    )\n  )\n\n# Check the data quality crisis at tracts\nreliability_summary &lt;- philly_poverty %&gt;%\n  count(reliability) %&gt;%\n  mutate(\n    percentage = round(n / sum(n) * 100, 1),\n    total_bg = sum(n)\n  )\n\nprint(\"Philadelphia Census Tract Poverty Data Reliability:\")\n\n[1] \"Philadelphia Census Tract Poverty Data Reliability:\"\n\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"Number of Tracts\", \"Percentage\", \"Total\"),\n    caption = \"The Data Quality Crisis: Philadelphia Census Tract Poverty Estimates\"\n  ) %&gt;%\n  kable_styling()\n\n\nThe Data Quality Crisis: Philadelphia Census Tract Poverty Estimates\n\n\nData Quality\nNumber of Tracts\nPercentage\nTotal\n\n\n\n\nLow\n295\n75.8\n389\n\n\nModerate\n53\n13.6\n389\n\n\nVery Low\n41\n10.5\n389\n\n\n\n\n\n\n# Show the most problematic estimates (following Guideline 3: provide context)\nworst_estimates &lt;- philly_poverty %&gt;%\n  filter(reliability %in% c(\"Low\", \"Very Low\")) %&gt;%\n  arrange(desc(poverty_cv)) %&gt;%\n  slice_head(n = 10)\n\nworst_estimates %&gt;%\n  select(GEOID, poverty_rate, poverty_rate_moe, poverty_cv, reliability_color, poverty_popE) %&gt;%\n  kable(\n    col.names = c(\"Tract\", \"Poverty Rate (%)\", \"MOE\", \"CV (%)\", \"Quality\", \"Pop Size\"),\n    caption = \"Guideline 3: Tracts with Least Reliable Poverty Estimates\",\n    digits = c(0, 1, 1, 1, 0, 0)\n  ) %&gt;%\n  kable_styling() %&gt;%\n  footnote(\n    general = c(\"These estimates should NOT be used for policy decisions\",\n                \"CV &gt; 75% indicates very low reliability\",\n                \"Recommend aggregation or alternative data sources\")\n  )\n\n\nGuideline 3: Tracts with Least Reliable Poverty Estimates\n\n\nTract\nPoverty Rate (%)\nMOE\nCV (%)\nQuality\nPop Size\n\n\n\n\n42101989100\n15.8\n45.2\n286.1\n🔴 |\n38|\n\n\n42101000101\n0.7\n1.1\n157.9\n🔴 |\n1947|\n\n\n42101980200\n37.9\n45.2\n119.4\n🔴 |\n66|\n\n\n42101023100\n3.8\n4.5\n119.4\n🔴 |\n1573|\n\n\n42101025600\n1.7\n2.0\n114.2\n🔴 |\n2642|\n\n\n42101014202\n1.7\n1.8\n107.0\n🔴 |\n2273|\n\n\n42101000403\n6.6\n6.7\n101.8\n🔴 |\n1047|\n\n\n42101026100\n4.7\n4.4\n95.0\n🔴 |\n2842|\n\n\n42101036502\n4.9\n4.7\n94.9\n🔴 |\n4284|\n\n\n42101032000\n21.8\n20.6\n94.8\n🔴 |\n7873|\n\n\n\nNote: \n\n\n\n\n\n\n\n These estimates should NOT be used for policy decisions\n\n\n\n\n\n\n\n CV &gt; 75% indicates very low reliability\n\n\n\n\n\n\n\n Recommend aggregation or alternative data sources"
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#key-references-and-acknowledgments",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#key-references-and-acknowledgments",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Key References and Acknowledgments",
    "text": "Key References and Acknowledgments\nJurjevich, J. R., Griffin, A. L., Spielman, S. E., Folch, D. C., Merrick, M., & Nagle, N. N. (2018). Navigating statistical uncertainty: How urban and regional planners understand and work with American community survey (ACS) data for guiding policy. Journal of the American Planning Association, 84(2), 112-126.\nWalker, K. (2023). Analyzing US Census Data: Methods, Maps, and Models in R. Available at: https://walker-data.com/census-r/\nAI Acknowledgments: This lab was developed with coding assistance from Claude AI. I have run, reviewed, and edited the final version. Any remaining errors are my own."
  },
  {
    "objectID": "labs/lab5/Lab_5_Instructions.html",
    "href": "labs/lab5/Lab_5_Instructions.html",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "For Homework 5, you’ll work either all byyyy yourself or in teams of 2 to:\n\n\n\nDownload data for Q2, Q3, or Q4 2024 from: https://www.rideindego.com/about/data/\nAdapt this code to work with your quarter:\n\nUpdate date ranges for weather data\nCheck for any data structure changes\nCreate the same 5 models\nCalculate MAE for each model\n\nCompare results to Q1 2025:\n\nHow do MAE values compare? Why might they differ?\nAre temporal patterns different (e.g., summer vs. winter)?\nWhich features are most important in your quarter?\n\n\n\n\n\nAnalyze your model’s errors in detail:\n\nSpatial patterns:\n\nCreate error maps\nIdentify neighborhoods with high errors\nHypothesize why (missing features? different demand patterns?)\n\nTemporal patterns:\n\nWhen are errors highest?\nDo certain hours/days have systematic under/over-prediction?\nAre there seasonal patterns?\n\nDemographic patterns:\n\nRelate errors to census characteristics\nAre certain communities systematically harder to predict?\nWhat are the equity implications?\n\n\n\n\n\nBased on your error analysis, add 2-3 NEW features to improve the model:\nPotential features to consider:\nTemporal features:\n\nHoliday indicators (Memorial Day, July 4th, Labor Day)\nSchool calendar (Penn, Drexel, Temple in session?)\nSpecial events (concerts, sports games, conventions)\nDay of month (payday effects?)\n\nWeather features:\n\nFeels-like temperature (wind chill/heat index)\n“Perfect biking weather” indicator (60-75°F, no rain)\nPrecipitation forecast (not just current)\nWeekend + nice weather interaction\n\nSpatial features:\n\nDistance to Center City\nDistance to nearest university\nDistance to nearest park\nPoints of interest nearby (restaurants, offices, bars)\nStation capacity\nBike lane connectivity\n\nTrip history features:\n\nRolling 7-day average demand\nSame hour last week\nStation “type” clustering (residential, commercial, tourist)\n\nImplementation:\n\nAdd your features to the best model\nCompare MAE before and after\nExplain why you chose these features\nDid they improve predictions? Where?\n\nTry a poisson model for count data\n\nDoes this improve model fit?\n\n\n\n\nWrite 1-2 paragraphs addressing:\n\nOperational implications:\n\nIs your final MAE “good enough” for Indego to use?\nWhen do prediction errors cause problems for rebalancing?\nWould you recommend deploying this system? Under what conditions?\n\nEquity considerations:\n\nDo prediction errors disproportionately affect certain neighborhoods?\nCould this system worsen existing disparities in bike access?\nWhat safeguards would you recommend?\n\nModel limitations:\n\nWhat patterns is your model missing?\nWhat assumptions might not hold in real deployment?\nHow would you improve this with more time/data?"
  },
  {
    "objectID": "labs/lab5/Lab_5_Instructions.html#part-1-replicate-with-different-quarter-alternately-you-could-do-a-longer-time-span-by-merging-multiple-quarters-together.-im-not-picky-about-that",
    "href": "labs/lab5/Lab_5_Instructions.html#part-1-replicate-with-different-quarter-alternately-you-could-do-a-longer-time-span-by-merging-multiple-quarters-together.-im-not-picky-about-that",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Download data for Q2, Q3, or Q4 2024 from: https://www.rideindego.com/about/data/\nAdapt this code to work with your quarter:\n\nUpdate date ranges for weather data\nCheck for any data structure changes\nCreate the same 5 models\nCalculate MAE for each model\n\nCompare results to Q1 2025:\n\nHow do MAE values compare? Why might they differ?\nAre temporal patterns different (e.g., summer vs. winter)?\nWhich features are most important in your quarter?"
  },
  {
    "objectID": "labs/lab5/Lab_5_Instructions.html#part-2-error-analysis",
    "href": "labs/lab5/Lab_5_Instructions.html#part-2-error-analysis",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Analyze your model’s errors in detail:\n\nSpatial patterns:\n\nCreate error maps\nIdentify neighborhoods with high errors\nHypothesize why (missing features? different demand patterns?)\n\nTemporal patterns:\n\nWhen are errors highest?\nDo certain hours/days have systematic under/over-prediction?\nAre there seasonal patterns?\n\nDemographic patterns:\n\nRelate errors to census characteristics\nAre certain communities systematically harder to predict?\nWhat are the equity implications?"
  },
  {
    "objectID": "labs/lab5/Lab_5_Instructions.html#part-3-feature-engineering-model-improvement",
    "href": "labs/lab5/Lab_5_Instructions.html#part-3-feature-engineering-model-improvement",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Based on your error analysis, add 2-3 NEW features to improve the model:\nPotential features to consider:\nTemporal features:\n\nHoliday indicators (Memorial Day, July 4th, Labor Day)\nSchool calendar (Penn, Drexel, Temple in session?)\nSpecial events (concerts, sports games, conventions)\nDay of month (payday effects?)\n\nWeather features:\n\nFeels-like temperature (wind chill/heat index)\n“Perfect biking weather” indicator (60-75°F, no rain)\nPrecipitation forecast (not just current)\nWeekend + nice weather interaction\n\nSpatial features:\n\nDistance to Center City\nDistance to nearest university\nDistance to nearest park\nPoints of interest nearby (restaurants, offices, bars)\nStation capacity\nBike lane connectivity\n\nTrip history features:\n\nRolling 7-day average demand\nSame hour last week\nStation “type” clustering (residential, commercial, tourist)\n\nImplementation:\n\nAdd your features to the best model\nCompare MAE before and after\nExplain why you chose these features\nDid they improve predictions? Where?\n\nTry a poisson model for count data\n\nDoes this improve model fit?"
  },
  {
    "objectID": "labs/lab5/Lab_5_Instructions.html#part-4-critical-reflection",
    "href": "labs/lab5/Lab_5_Instructions.html#part-4-critical-reflection",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Write 1-2 paragraphs addressing:\n\nOperational implications:\n\nIs your final MAE “good enough” for Indego to use?\nWhen do prediction errors cause problems for rebalancing?\nWould you recommend deploying this system? Under what conditions?\n\nEquity considerations:\n\nDo prediction errors disproportionately affect certain neighborhoods?\nCould this system worsen existing disparities in bike access?\nWhat safeguards would you recommend?\n\nModel limitations:\n\nWhat patterns is your model missing?\nWhat assumptions might not hold in real deployment?\nHow would you improve this with more time/data?"
  },
  {
    "objectID": "labs/lab5/Lab_5_Instructions.html#what-to-submit-per-team",
    "href": "labs/lab5/Lab_5_Instructions.html#what-to-submit-per-team",
    "title": "HW5: Your Turn!",
    "section": "What to Submit (per team)",
    "text": "What to Submit (per team)\n\nqmd file with all your code (commented!)\nHTML output with results and visualizations\nBrief report summarizing (with supporting data & visualization):\n\nYour quarter and why you chose it\nModel comparison results\nError analysis insights\nNew features you added and why\nCritical reflection on deployment\n\nYou only need to submitted once as a team (one submit the link to your portfolio on Canvas)"
  },
  {
    "objectID": "labs/lab5/Lab_5_Instructions.html#tips-for-success",
    "href": "labs/lab5/Lab_5_Instructions.html#tips-for-success",
    "title": "HW5: Your Turn!",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nStart early - data download and processing takes time\nWork together - pair programming is your friend\nTest incrementally - don’t wait until the end to run code\nDocument everything - explain your choices\nBe creative - the best features come from understanding Philly!\nThink critically - technical sophistication isn’t enough"
  },
  {
    "objectID": "labs/lab4/Assignment4_Instructions.html",
    "href": "labs/lab4/Assignment4_Instructions.html",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "In this lab, you will apply the spatial predictive modeling techniques demonstrated in the class exercise using a different 311 service request type of your choice as the predictor variable. You will build a complete spatial predictive model, document your process, and interpret your results.\n\n\nDue Date: November 17, 2025, 10:00AM\nDeliverable: One rendered document, posted to your portfolio website.\n\n\n\nBy completing this assignment, you will demonstrate your ability to:\n\nAdapt example code to analyze a new dataset\nBuild spatial features for predictive modeling\nApply count regression techniques to spatial data\nImplement spatial cross-validation\nInterpret and communicate model results\nCritically evaluate model performance\n\n\n\n\n\n\n\n\nVisit the Chicago 311 Service Requests dataset:\nhttps://data.cityofchicago.org/stories/s/311-Dataset-Changes-12-11-2018/d7nq-5g7t\nBrowse the available service request types (e.g., Graffiti Removal, Pothole Repair, Street Light Out, Sanitation Code Violations, etc.) and choose one violation type that interests you.\n\n\n\n\n\n\n\n\n\n\nUsing the class exercise as your template, adapt the code to analyze your chosen 311 violation type.\n\n\nWork through all major sections of the analysis:\n\n\n\nLoad your 311 data and Chicago spatial boundaries\nCreate visualizations showing the spatial distribution of your violation type\nDescribe patterns you observe\n\n\n\n\n\nCreate a 500m x 500m fishnet grid\nAggregate your violations to grid cells\nVisualize the count distribution\n\n\n\n\n\nCalculate k-nearest neighbor features\nPerform Local Moran’s I analysis\nIdentify hot spots and cold spots\nCreate distance-to-hotspot measures\nJoin any additional contextual data if you are looking for more to do and really get into this (e.g., demographics, land use)\n\n\n\n\n\nFit Poisson regression\nFit Negative Binomial regression\nCompare model fit (AIC)\n\n\n\n\n\nImplement Leave-One-Group-Out cross-validation on 2017 data\nCalculate and report error metrics (MAE, RMSE)\n\n\n\n\n\nCompare to KDE baseline\n\n\n\n\n\nDownload 2018 crimes https://data.cityofchicago.org/Public-Safety/Crimes-2018/3i3m-jwuy/about_data\nBe sure to filter for BURGLARIES (FORCED ENTRY only to match the 2017 data)\nAggregate 2018 violations to the same fishnet grid\nUse your 2017 model to predict 2018 counts\nCalculate temporal validation metrics\nCompare spatial vs. temporal validation performance\n\n\n\n\n\n\n\n\n\nFor each major section, you must explain in your own words:\n\nWhat you are doing in that step\nWhy this step is important for the analysis\nWhat you found or learned from the results\n\nDo not simply copy text from the example or from your AI friend. Think about the purpose of each technique and articulate it in your own words.\n\n\n\n\n\n\n\nYour knitted HTML document should be professional and easy to read:\n\n\n\nRemove unnecessary code, comments, or debugging lines\nKeep only essential code chunks\nUse code-fold: show in your YAML header\n\n\n\n\n\nUse headers to organize sections\nInclude a table of contents\nAdd your name and date\n\n\n\n\n\nEnsure all text renders properly (no broken markdown)\nCheck that headers appear as headers (not plain text)\nUse proper markdown formatting\n\n\n\n\n\nAll plots should have titles and labels\nMaps should be readable\nUse consistent color schemes\n\n\n\n\n\nProofread for typos\nRemove any “Your answer here” placeholders\nMake sure all code runs without errors\nMake IT NEAT. WE GOTTA LOOK GOOD HERE.\n\n\n\n\n\n\n\nBefore you submit, verify that your document includes:\n\n\n\nIntroduction: Brief description of your chosen 311 violation type and why you selected it\nData Exploration: Maps and summary statistics of your violation data\nFishnet Grid: Successfully created and visualized grid with aggregated counts\nSpatial Features: At least 2-3 spatial features calculated (k-NN, distance measures, etc.)\nLocal Moran’s I: Spatial autocorrelation analysis with interpretation\nCount Regression: Both Poisson and Negative Binomial models with comparison\nSpatial Cross-Validation: LOGO CV implemented on 2017 data with reported metrics\nTemporal Validation: 2018 data aggregated and tested with 2017 model(OPTIONAL.)\nValidation Comparison: Comparison of spatial vs. temporal validation performance\nModel Comparison: Your model vs. KDE baseline for 2018 predictions\nError Analysis: Maps and discussion of where model performs well/poorly\nWritten Explanations: Your own words explaining each step\n\n\n\n\n\nDocument knits to HTML without errors\nCode is clean and well-organized\nAll visualizations display properly\nHeaders and formatting render correctly\nFile is named successfully posted to your website"
  },
  {
    "objectID": "labs/lab4/Assignment4_Instructions.html#assignment-overview",
    "href": "labs/lab4/Assignment4_Instructions.html#assignment-overview",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "In this lab, you will apply the spatial predictive modeling techniques demonstrated in the class exercise using a different 311 service request type of your choice as the predictor variable. You will build a complete spatial predictive model, document your process, and interpret your results.\n\n\nDue Date: November 17, 2025, 10:00AM\nDeliverable: One rendered document, posted to your portfolio website.\n\n\n\nBy completing this assignment, you will demonstrate your ability to:\n\nAdapt example code to analyze a new dataset\nBuild spatial features for predictive modeling\nApply count regression techniques to spatial data\nImplement spatial cross-validation\nInterpret and communicate model results\nCritically evaluate model performance"
  },
  {
    "objectID": "labs/lab4/Assignment4_Instructions.html#step-1-choose-your-311-violation-type",
    "href": "labs/lab4/Assignment4_Instructions.html#step-1-choose-your-311-violation-type",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "Visit the Chicago 311 Service Requests dataset:\nhttps://data.cityofchicago.org/stories/s/311-Dataset-Changes-12-11-2018/d7nq-5g7t\nBrowse the available service request types (e.g., Graffiti Removal, Pothole Repair, Street Light Out, Sanitation Code Violations, etc.) and choose one violation type that interests you."
  },
  {
    "objectID": "labs/lab4/Assignment4_Instructions.html#step-2-complete-the-analysis",
    "href": "labs/lab4/Assignment4_Instructions.html#step-2-complete-the-analysis",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "Using the class exercise as your template, adapt the code to analyze your chosen 311 violation type.\n\n\nWork through all major sections of the analysis:\n\n\n\nLoad your 311 data and Chicago spatial boundaries\nCreate visualizations showing the spatial distribution of your violation type\nDescribe patterns you observe\n\n\n\n\n\nCreate a 500m x 500m fishnet grid\nAggregate your violations to grid cells\nVisualize the count distribution\n\n\n\n\n\nCalculate k-nearest neighbor features\nPerform Local Moran’s I analysis\nIdentify hot spots and cold spots\nCreate distance-to-hotspot measures\nJoin any additional contextual data if you are looking for more to do and really get into this (e.g., demographics, land use)\n\n\n\n\n\nFit Poisson regression\nFit Negative Binomial regression\nCompare model fit (AIC)\n\n\n\n\n\nImplement Leave-One-Group-Out cross-validation on 2017 data\nCalculate and report error metrics (MAE, RMSE)\n\n\n\n\n\nCompare to KDE baseline\n\n\n\n\n\nDownload 2018 crimes https://data.cityofchicago.org/Public-Safety/Crimes-2018/3i3m-jwuy/about_data\nBe sure to filter for BURGLARIES (FORCED ENTRY only to match the 2017 data)\nAggregate 2018 violations to the same fishnet grid\nUse your 2017 model to predict 2018 counts\nCalculate temporal validation metrics\nCompare spatial vs. temporal validation performance"
  },
  {
    "objectID": "labs/lab4/Assignment4_Instructions.html#step-3-write-your-analysis",
    "href": "labs/lab4/Assignment4_Instructions.html#step-3-write-your-analysis",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "For each major section, you must explain in your own words:\n\nWhat you are doing in that step\nWhy this step is important for the analysis\nWhat you found or learned from the results\n\nDo not simply copy text from the example or from your AI friend. Think about the purpose of each technique and articulate it in your own words."
  },
  {
    "objectID": "labs/lab4/Assignment4_Instructions.html#step-4-format-your-document",
    "href": "labs/lab4/Assignment4_Instructions.html#step-4-format-your-document",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "Your knitted HTML document should be professional and easy to read:\n\n\n\nRemove unnecessary code, comments, or debugging lines\nKeep only essential code chunks\nUse code-fold: show in your YAML header\n\n\n\n\n\nUse headers to organize sections\nInclude a table of contents\nAdd your name and date\n\n\n\n\n\nEnsure all text renders properly (no broken markdown)\nCheck that headers appear as headers (not plain text)\nUse proper markdown formatting\n\n\n\n\n\nAll plots should have titles and labels\nMaps should be readable\nUse consistent color schemes\n\n\n\n\n\nProofread for typos\nRemove any “Your answer here” placeholders\nMake sure all code runs without errors\nMake IT NEAT. WE GOTTA LOOK GOOD HERE."
  },
  {
    "objectID": "labs/lab4/Assignment4_Instructions.html#submission-checklist",
    "href": "labs/lab4/Assignment4_Instructions.html#submission-checklist",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "Before you submit, verify that your document includes:\n\n\n\nIntroduction: Brief description of your chosen 311 violation type and why you selected it\nData Exploration: Maps and summary statistics of your violation data\nFishnet Grid: Successfully created and visualized grid with aggregated counts\nSpatial Features: At least 2-3 spatial features calculated (k-NN, distance measures, etc.)\nLocal Moran’s I: Spatial autocorrelation analysis with interpretation\nCount Regression: Both Poisson and Negative Binomial models with comparison\nSpatial Cross-Validation: LOGO CV implemented on 2017 data with reported metrics\nTemporal Validation: 2018 data aggregated and tested with 2017 model(OPTIONAL.)\nValidation Comparison: Comparison of spatial vs. temporal validation performance\nModel Comparison: Your model vs. KDE baseline for 2018 predictions\nError Analysis: Maps and discussion of where model performs well/poorly\nWritten Explanations: Your own words explaining each step\n\n\n\n\n\nDocument knits to HTML without errors\nCode is clean and well-organized\nAll visualizations display properly\nHeaders and formatting render correctly\nFile is named successfully posted to your website"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "",
    "text": "Download a different quarter of Indego data (Q4 2024)\nApply the same workflow\nAnalyze error patterns in your quarter\nAdd 2-3 new features to improve the model\nWrite a brief report on what you learned"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#assignment-structure",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#assignment-structure",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "",
    "text": "Download a different quarter of Indego data (Q4 2024)\nApply the same workflow\nAnalyze error patterns in your quarter\nAdd 2-3 new features to improve the model\nWrite a brief report on what you learned"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#load-libraries",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#load-libraries",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Load Libraries",
    "text": "Load Libraries\n\n\nCode\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#define-themes",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#define-themes",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Define Themes",
    "text": "Define Themes\n\n\nCode\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 &lt;- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#set-census-api-key",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#set-census-api-key",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Set Census API Key",
    "text": "Set Census API Key\n\n\nCode\ncensus_api_key(\"ec702835845a134b4376c60759aa72ce62f6df59\", overwrite = TRUE, install = TRUE)"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#load-indego-trip-data-q4-2024",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#load-indego-trip-data-q4-2024",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Load Indego Trip Data (Q4 2024)",
    "text": "Load Indego Trip Data (Q4 2024)\n\n\nCode\nindego &lt;- read_csv(\"/Users/cathy/stats a1/Untitled/musa-5080-fall-2025-portfolio-setup-student-portfolio-kc2/Assignments/Assignment_last/indego-trips-2024-q4.csv\")\n\ngetwd()\n\n\n[1] \"/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Assignment_5\"\n\n\nCode\n# Quick look at the data\nglimpse(indego)\n\n\nRows: 299,121\nColumns: 15\n$ trip_id             &lt;dbl&gt; 1050296434, 1050293063, 1050296229, 1050276817, 10…\n$ duration            &lt;dbl&gt; 22, 8, 12, 1, 5, 9, 5, 6, 16, 16, 16, 39, 6, 40, 1…\n$ start_time          &lt;chr&gt; \"10/1/2024 0:00\", \"10/1/2024 0:06\", \"10/1/2024 0:0…\n$ end_time            &lt;chr&gt; \"10/1/2024 0:22\", \"10/1/2024 0:14\", \"10/1/2024 0:1…\n$ start_station       &lt;dbl&gt; 3322, 3166, 3007, 3166, 3075, 3166, 3030, 3010, 33…\n$ start_lat           &lt;dbl&gt; 39.93638, 39.97195, 39.94517, 39.97195, 39.96718, …\n$ start_lon           &lt;dbl&gt; -75.15526, -75.13445, -75.15993, -75.13445, -75.16…\n$ end_station         &lt;dbl&gt; 3375, 3017, 3244, 3166, 3102, 3008, 3203, 3163, 33…\n$ end_lat             &lt;dbl&gt; 39.96036, 39.98003, 39.93865, 39.97195, 39.96759, …\n$ end_lon             &lt;dbl&gt; -75.14020, -75.14371, -75.16674, -75.13445, -75.17…\n$ bike_id             &lt;chr&gt; \"03580\", \"05386\", \"18082\", \"02729\", \"18519\", \"0272…\n$ plan_duration       &lt;dbl&gt; 365, 365, 365, 1, 30, 1, 365, 365, 30, 30, 30, 30,…\n$ trip_route_category &lt;chr&gt; \"One Way\", \"One Way\", \"One Way\", \"Round Trip\", \"On…\n$ passholder_type     &lt;chr&gt; \"Indego365\", \"Indego365\", \"Indego365\", \"Walk-up\", …\n$ bike_type           &lt;chr&gt; \"standard\", \"standard\", \"electric\", \"standard\", \"e…"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#examine-the-data-structure",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#examine-the-data-structure",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Examine the Data Structure",
    "text": "Examine the Data Structure\n\n\nCode\n# How many trips?\ncat(\"Total trips in Q4 2024:\", nrow(indego), \"\\n\")\n\n\nTotal trips in Q4 2024: 299121 \n\n\nCode\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego$start_time)), \"to\", \n    max(mdy_hm(indego$start_time)), \"\\n\")\n\n\nDate range: 1727740800 to 1735689360 \n\n\nCode\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n\n\nUnique start stations: 256 \n\n\nCode\n# Trip types\ntable(indego$trip_route_category)\n\n\n\n   One Way Round Trip \n    282675      16446 \n\n\nCode\n# Passholder types\ntable(indego$passholder_type)\n\n\n\n  Day Pass   Indego30  Indego365 IndegoFlex       NULL    Walk-up \n     10711     159895     112690          1       1165      14659 \n\n\nCode\n# Bike types\ntable(indego$bike_type)\n\n\n\nelectric standard \n  175503   123618"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#create-time-bins",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#create-time-bins",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Create Time Bins",
    "text": "Create Time Bins\nWe need to aggregate trips into hourly intervals for our panel data structure.\n\n\nCode\nindego &lt;- indego %&gt;%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego %&gt;% select(start_datetime, interval60, week, dotw, hour, weekend))\n\n\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;ord&gt; &lt;int&gt;   &lt;dbl&gt;\n1 2024-10-01 00:00:00 2024-10-01 00:00:00    40 Tue       0       0\n2 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n3 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n4 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n5 2024-10-01 00:07:00 2024-10-01 00:00:00    40 Tue       0       0\n6 2024-10-01 00:08:00 2024-10-01 00:00:00    40 Tue       0       0"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#trips-over-time",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#trips-over-time",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Trips Over Time",
    "text": "Trips Over Time\n\n\nCode\n# Daily trip counts\ndaily_trips &lt;- indego %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q4 2024\",\n    subtitle = \"Winter demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#hourly-patterns",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#hourly-patterns",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Hourly Patterns",
    "text": "Hourly Patterns\n\n\nCode\n# Average trips by hour and day type\nhourly_patterns &lt;- indego %&gt;%\n  group_by(hour, weekend) %&gt;%\n  summarize(avg_trips = n() / n_distinct(date)) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#top-stations",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#top-stations",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Top Stations",
    "text": "Top Stations\n\n\nCode\n# Most popular origin stations\ntop_stations &lt;- indego %&gt;%\n  count(start_station, start_lat, start_lon, name = \"trips\") %&gt;%\n  arrange(desc(trips)) %&gt;%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 20 Indego Stations by Trip Origins\n\n\nstart_station\nstart_lat\nstart_lon\ntrips\n\n\n\n\n3,010\n39.94711\n-75.16618\n5,943\n\n\n3,032\n39.94527\n-75.17971\n4,471\n\n\n3,359\n39.94888\n-75.16978\n3,923\n\n\n3,244\n39.93865\n-75.16674\n3,492\n\n\n3,295\n39.95028\n-75.16027\n3,411\n\n\n3,020\n39.94855\n-75.19007\n3,369\n\n\n3,208\n39.95048\n-75.19324\n3,343\n\n\n3,066\n39.94561\n-75.17348\n3,342\n\n\n3,054\n39.96250\n-75.17420\n3,297\n\n\n3,101\n39.94295\n-75.15955\n3,214\n\n\n3,022\n39.95472\n-75.18323\n3,152\n\n\n3,028\n39.94061\n-75.14958\n3,101\n\n\n3,362\n39.94816\n-75.16226\n3,072\n\n\n3,063\n39.94633\n-75.16980\n2,972\n\n\n3,185\n39.95169\n-75.15888\n2,952\n\n\n3,059\n39.96244\n-75.16121\n2,926\n\n\n3,012\n39.94218\n-75.17747\n2,907\n\n\n3,061\n39.95425\n-75.17761\n2,847\n\n\n3,161\n39.95486\n-75.18091\n2,837\n\n\n3,256\n39.95269\n-75.17779\n2,816"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#load-philadelphia-census-data",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#load-philadelphia-census-data",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Load Philadelphia Census Data",
    "text": "Load Philadelphia Census Data\n\n\nCode\nphilly_census\n\n\nSimple feature collection with 408 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -75.28026 ymin: 39.86705 xmax: -74.95578 ymax: 40.13799\nGeodetic CRS:  WGS 84\nFirst 10 features:\n         GEOID                                                  NAME Total_Pop\n1  42101001500    Census Tract 15; Philadelphia County; Pennsylvania      3251\n2  42101001800    Census Tract 18; Philadelphia County; Pennsylvania      3300\n3  42101002802 Census Tract 28.02; Philadelphia County; Pennsylvania      5720\n4  42101004001 Census Tract 40.01; Philadelphia County; Pennsylvania      4029\n5  42101006300    Census Tract 63; Philadelphia County; Pennsylvania      4415\n6  42101007700    Census Tract 77; Philadelphia County; Pennsylvania      1815\n7  42101008701 Census Tract 87.01; Philadelphia County; Pennsylvania      3374\n8  42101011000   Census Tract 110; Philadelphia County; Pennsylvania      2729\n9  42101011500   Census Tract 115; Philadelphia County; Pennsylvania      4881\n10 42101014000   Census Tract 140; Philadelphia County; Pennsylvania      5753\n   B01003_001M Med_Inc B19013_001M Total_Commuters B08301_001M\n1          677  110859       24975            2073         387\n2          369  114063       30714            2255         308\n3          796   78871       20396            3032         478\n4          437   61583       22293            2326         383\n5          853   32347        4840            1980         456\n6          210   48581       13812             969         189\n7          480   59722        6278            2427         380\n8          734   14983        6647             708         281\n9          763   81154        9400            2593         456\n10        1162   59375       17602            3082         682\n   Transit_Commuters B08301_010M White_Pop B02001_002M Med_Home_Value\n1                429         188      2185         268         568300\n2                123          66      2494         381         605000\n3                685         219      3691         592         350600\n4                506         144      3223         380         296400\n5                534         285       182          88          76600\n6                192          71       984         190         289700\n7                658         278      2111         463         579500\n8                218         184       231         112         107200\n9                438         176       357         238         157700\n10               518         235      2311         778         296400\n   B25077_001M                       geometry Percent_Taking_Transit\n1        58894 MULTIPOLYGON (((-75.16557 3...              20.694645\n2        34876 MULTIPOLYGON (((-75.1662 39...               5.454545\n3        12572 MULTIPOLYGON (((-75.16735 3...              22.592348\n4        22333 MULTIPOLYGON (((-75.17002 3...              21.754084\n5        10843 MULTIPOLYGON (((-75.24685 3...              26.969697\n6       118720 MULTIPOLYGON (((-75.21292 3...              19.814241\n7       150694 MULTIPOLYGON (((-75.21271 3...              27.111660\n8        12137 MULTIPOLYGON (((-75.21347 3...              30.790960\n9        13571 MULTIPOLYGON (((-75.25592 3...              16.891631\n10      215482 MULTIPOLYGON (((-75.16722 3...              16.807268\n   Percent_White\n1      67.210089\n2      75.575758\n3      64.527972\n4      79.995036\n5       4.122310\n6      54.214876\n7      62.566686\n8       8.464639\n9       7.314075\n10     40.170346"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#map-philadelphia-context",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#map-philadelphia-context",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Map Philadelphia Context",
    "text": "Map Philadelphia Context\n\n\nCode\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#join-census-data-to-stations",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#join-census-data-to-stations",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Join Census Data to Stations",
    "text": "Join Census Data to Stations\n\n\nCode\n# Create sf object for stations\nstations_sf &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census &lt;- st_join(stations_sf, philly_census, left = TRUE) %&gt;%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  left_join(\n    stations_census %&gt;% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census &lt;- indego %&gt;%\n  left_join(\n    stations_census %&gt;% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  left_join(\n    stations_census %&gt;% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %&gt;% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %&gt;% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#visualize-weather-patterns",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#visualize-weather-patterns",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Visualize Weather Patterns",
    "text": "Visualize Weather Patterns\n\n\nCode\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q4 2024\",\n    subtitle = \"Winter to early spring transition\",\n    x = \"Date\",\n    y = \"Temperature (°F)\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#aggregate-trips-to-station-hour-level",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#aggregate-trips-to-station-hour-level",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Aggregate Trips to Station-Hour Level",
    "text": "Aggregate Trips to Station-Hour Level\n\n\nCode\n# Count trips by station-hour\ntrips_panel &lt;- indego_census %&gt;%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %&gt;%\n  summarize(Trip_Count = n()) %&gt;%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n\n\n[1] 150972\n\n\nCode\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n\n\n[1] 237\n\n\nCode\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n\n\n[1] 2208"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#create-complete-panel-structure",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#create-complete-panel-structure",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Create Complete Panel Structure",
    "text": "Create Complete Panel Structure\nNot every station has trips every hour. We need a complete panel where every station-hour combination exists (even if Trip_Count = 0).\n\n\nCode\n# Calculate expected panel size\nn_stations &lt;- length(unique(trips_panel$start_station))\nn_hours &lt;- length(unique(trips_panel$interval60))\nexpected_rows &lt;- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n\n\nExpected panel rows: 523,296 \n\n\nCode\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\n\nCurrent rows: 150,972 \n\n\nCode\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\n\nMissing rows: 372,324 \n\n\nCode\n# Create complete panel\nstudy_panel &lt;- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %&gt;%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %&gt;%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes &lt;- trips_panel %&gt;%\n  group_by(start_station) %&gt;%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n\n\nComplete panel rows: 523,296"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#add-time-features",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#add-time-features",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Add Time Features",
    "text": "Add Time Features\n\n\nCode\nstudy_panel &lt;- study_panel %&gt;%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE, locale = \"C\"),\n    dotw = wday(interval60, label = TRUE, locale = \"C\"),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#join-weather-data",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#join-weather-data",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Join Weather Data",
    "text": "Join Weather Data\n\n\nCode\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %&gt;% select(Trip_Count, Temperature, Precipitation))\n\n\n   Trip_Count       Temperature   Precipitation   \n Min.   : 0.0000   Min.   :12.0   Min.   :0.0000  \n 1st Qu.: 0.0000   1st Qu.:41.0   1st Qu.:0.0000  \n Median : 0.0000   Median :51.0   Median :0.0000  \n Mean   : 0.5178   Mean   :50.8   Mean   :0.0042  \n 3rd Qu.: 1.0000   3rd Qu.:61.0   3rd Qu.:0.0000  \n Max.   :28.0000   Max.   :83.0   Max.   :0.5200  \n                   NA's   :5688   NA's   :5688"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#why-lags",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#why-lags",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Why Lags?",
    "text": "Why Lags?\nIf there were 15 bike trips from Station A at 8:00 AM, there will probably be ~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.\n\n\nCode\n# Sort by station and time\nstudy_panel &lt;- study_panel %&gt;%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel &lt;- study_panel %&gt;%\n  group_by(start_station) %&gt;%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %&gt;%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete &lt;- study_panel %&gt;%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n\n\nRows after removing NA lags: 605,298"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#visualize-lag-correlations",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#visualize-lag-correlations",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Visualize Lag Correlations",
    "text": "Visualize Lag Correlations\n\n\nCode\n# Sample one station to visualize\nexample_station &lt;- study_panel_complete %&gt;%\n  filter(start_station == first(start_station)) %&gt;%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#why-temporal-validation-matters",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#why-temporal-validation-matters",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Why Temporal Validation Matters",
    "text": "Why Temporal Validation Matters\nIn real operations, at 6:00 AM on March 15, we need to predict demand for March 15-31. We have data from Jan 1 - March 14, but NOT from March 15-31 (it hasn’t happened yet!).\nWrong approach: Train on weeks 10-13, test on weeks 1-9 (predicting past from future!)\nCorrect approach: Train on weeks 1-9, test on weeks 10-13 (predicting future from past)\n\n\nCode\n# Split by week\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 48) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\nlate_stations &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 48) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations &lt;- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  filter(start_station %in% common_stations)\n\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  mutate(\n    month = lubridate::month(date, label = TRUE),\n    month = factor(month),\n    start_station = factor(start_station),\n    dotw_simple = factor(\n      dotw,\n      levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n    )\n  )\n\n\n\n\nCode\nlibrary(slider)\n\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  arrange(start_station, date, hour) %&gt;%\n  group_by(start_station) %&gt;%\n\n  mutate(\n    # Perfect biking weather\n    perfect_weather = ifelse(Temperature &gt;= 60 & Temperature &lt;= 75 & Precipitation == 0, 1, 0),\n\n    # Rolling 7-day average demand (168 hours)\n    rolling7 = slide_dbl(Trip_Count, mean, .before = 24*7, .complete = FALSE),\n\n    # Same hour last week\n    same_hour_last_week = lag(Trip_Count, 24*7)\n  ) %&gt;%\n  ungroup()\n\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  mutate(\n    rolling7 = ifelse(is.na(rolling7), mean(Trip_Count, na.rm = TRUE), rolling7),\n    same_hour_last_week = ifelse(is.na(same_hour_last_week),\n                                 mean(Trip_Count, na.rm = TRUE),\n                                 same_hour_last_week)\n  )\n\n\n\n\nCode\ntrain &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 48)\n\ntest &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 48)\n\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n\n\nTraining date range: 19997 to 20051 \n\n\nCode\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n\n\nTesting date range: 20052 to 20088 \n\n\nCode\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n\n\nTraining observations: 327,012 \n\n\nCode\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n\n\nTesting observations: 257,854 \n\n\n\n\nCode\ntrain &lt;- train %&gt;%\n  mutate(\n    month         = factor(month),\n    start_station = factor(start_station),\n    dotw_simple   = factor(dotw_simple)\n  )\n\ntest &lt;- test %&gt;%\n  mutate(\n    month         = factor(month,         levels = levels(train$month)),\n    start_station = factor(start_station, levels = levels(train$start_station)),\n    dotw_simple   = factor(dotw_simple,   levels = levels(train$dotw_simple))\n  )"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#model-1-baseline-time-weather",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#model-1-baseline-time-weather",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Model 1: Baseline (Time + Weather)",
    "text": "Model 1: Baseline (Time + Weather)\n\n\nCode\ncontrasts(train$dotw_simple) &lt;- contr.treatment(7)\n\n# Now run the model\nmodel1 &lt;- lm(\n  Trip_Count ~ hour + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\nsummary(model1)\n\n\n\nCall:\nlm(formula = Trip_Count ~ hour + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2242 -0.7032 -0.4402  0.2969 27.5312 \n\nCoefficients:\n                Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)   -0.0957783  0.0144440  -6.631    0.000000000033392 ***\nhour           0.0285824  0.0003465  82.488 &lt; 0.0000000000000002 ***\ndotw_simple2   0.0891253  0.0085847  10.382 &lt; 0.0000000000000002 ***\ndotw_simple3   0.0555993  0.0084422   6.586    0.000000000045295 ***\ndotw_simple4   0.0289168  0.0081856   3.533             0.000411 ***\ndotw_simple5  -0.0175401  0.0084144  -2.085             0.037112 *  \ndotw_simple6  -0.0302831  0.0083595  -3.623             0.000292 ***\ndotw_simple7  -0.0618905  0.0085699  -7.222    0.000000000000514 ***\nTemperature    0.0078551  0.0002315  33.925 &lt; 0.0000000000000002 ***\nPrecipitation -1.4626291  0.1042579 -14.029 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.267 on 327002 degrees of freedom\nMultiple R-squared:  0.03606,   Adjusted R-squared:  0.03603 \nF-statistic:  1359 on 9 and 327002 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#model-2-add-temporal-lags",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#model-2-add-temporal-lags",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Model 2: Add Temporal Lags",
    "text": "Model 2: Add Temporal Lags\n\n\nCode\nmodel2 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.8668  -0.5084  -0.1262   0.1534  25.5232 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       -0.1569860  0.0166190  -9.446 &lt; 0.0000000000000002 ***\nas.factor(hour)1  -0.0216319  0.0133468  -1.621             0.105071    \nas.factor(hour)2  -0.0143378  0.0130803  -1.096             0.273019    \nas.factor(hour)3  -0.0450630  0.0130461  -3.454             0.000552 ***\nas.factor(hour)4  -0.0259598  0.0130687  -1.986             0.046989 *  \nas.factor(hour)5   0.0510260  0.0131790   3.872             0.000108 ***\nas.factor(hour)6   0.2685796  0.0131234  20.466 &lt; 0.0000000000000002 ***\nas.factor(hour)7   0.4354250  0.0133659  32.577 &lt; 0.0000000000000002 ***\nas.factor(hour)8   0.7254165  0.0131763  55.055 &lt; 0.0000000000000002 ***\nas.factor(hour)9   0.2931534  0.0132998  22.042 &lt; 0.0000000000000002 ***\nas.factor(hour)10  0.2225753  0.0129511  17.186 &lt; 0.0000000000000002 ***\nas.factor(hour)11  0.2751405  0.0131013  21.001 &lt; 0.0000000000000002 ***\nas.factor(hour)12  0.3859183  0.0129109  29.891 &lt; 0.0000000000000002 ***\nas.factor(hour)13  0.3418110  0.0128506  26.599 &lt; 0.0000000000000002 ***\nas.factor(hour)14  0.3907836  0.0129756  30.117 &lt; 0.0000000000000002 ***\nas.factor(hour)15  0.4873104  0.0134338  36.275 &lt; 0.0000000000000002 ***\nas.factor(hour)16  0.5694374  0.0131324  43.361 &lt; 0.0000000000000002 ***\nas.factor(hour)17  0.7308318  0.0132964  54.965 &lt; 0.0000000000000002 ***\nas.factor(hour)18  0.3636165  0.0133790  27.178 &lt; 0.0000000000000002 ***\nas.factor(hour)19  0.1823048  0.0133231  13.683 &lt; 0.0000000000000002 ***\nas.factor(hour)20  0.0603081  0.0133896   4.504    0.000006667385762 ***\nas.factor(hour)21  0.0596190  0.0133618   4.462    0.000008126737395 ***\nas.factor(hour)22  0.0630014  0.0133074   4.734    0.000002199004306 ***\nas.factor(hour)23  0.0201131  0.0133487   1.507             0.131875    \ndotw_simple2       0.0070622  0.0071915   0.982             0.326095    \ndotw_simple3      -0.0075946  0.0070778  -1.073             0.283271    \ndotw_simple4      -0.0162277  0.0068563  -2.367             0.017942 *  \ndotw_simple5      -0.0471739  0.0070685  -6.674    0.000000000024953 ***\ndotw_simple6      -0.0382126  0.0070008  -5.458    0.000000048093673 ***\ndotw_simple7      -0.0643858  0.0071935  -8.951 &lt; 0.0000000000000002 ***\nTemperature        0.0031340  0.0002103  14.905 &lt; 0.0000000000000002 ***\nPrecipitation     -0.6473491  0.0889092  -7.281    0.000000000000332 ***\nlag1Hour           0.3209734  0.0016611 193.233 &lt; 0.0000000000000002 ***\nlag3Hours          0.1158005  0.0016226  71.368 &lt; 0.0000000000000002 ***\nlag1day            0.2069410  0.0015668 132.079 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.059 on 326977 degrees of freedom\nMultiple R-squared:  0.3266,    Adjusted R-squared:  0.3265 \nF-statistic:  4664 on 34 and 326977 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#model-3-add-demographics",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#model-3-add-demographics",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Model 3: Add Demographics",
    "text": "Model 3: Add Demographics\n\n\nCode\nmodel3 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.3562  -0.7533  -0.2867   0.4645  24.6227 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.7597649649  0.0466846513  16.274\nas.factor(hour)1         -0.0515669832  0.0490154669  -1.052\nas.factor(hour)2         -0.0867740413  0.0523375323  -1.658\nas.factor(hour)3         -0.1994943606  0.0627243786  -3.180\nas.factor(hour)4         -0.1870355163  0.0588232534  -3.180\nas.factor(hour)5         -0.1009816563  0.0440919999  -2.290\nas.factor(hour)6          0.1652923989  0.0378989985   4.361\nas.factor(hour)7          0.3573906340  0.0365910065   9.767\nas.factor(hour)8          0.6527104467  0.0352097312  18.538\nas.factor(hour)9          0.0535208603  0.0354954032   1.508\nas.factor(hour)10         0.0259482203  0.0354888074   0.731\nas.factor(hour)11         0.0790705712  0.0354788110   2.229\nas.factor(hour)12         0.1650284441  0.0349014673   4.728\nas.factor(hour)13         0.1360741117  0.0349489699   3.894\nas.factor(hour)14         0.1556598805  0.0348533438   4.466\nas.factor(hour)15         0.2930950543  0.0351244362   8.344\nas.factor(hour)16         0.3946091252  0.0347154998  11.367\nas.factor(hour)17         0.6372143520  0.0347959385  18.313\nas.factor(hour)18         0.1928539490  0.0351710581   5.483\nas.factor(hour)19         0.0000185049  0.0357066015   0.001\nas.factor(hour)20        -0.0946541145  0.0365418549  -2.590\nas.factor(hour)21        -0.0839665744  0.0373764235  -2.247\nas.factor(hour)22        -0.0493759989  0.0382665554  -1.290\nas.factor(hour)23        -0.0867600638  0.0401608303  -2.160\ndotw_simple2              0.0064803572  0.0149253069   0.434\ndotw_simple3             -0.0053754518  0.0148779317  -0.361\ndotw_simple4             -0.0584719470  0.0145098445  -4.030\ndotw_simple5             -0.1014781644  0.0149669594  -6.780\ndotw_simple6             -0.0351848322  0.0149613667  -2.352\ndotw_simple7             -0.0461454793  0.0156168132  -2.955\nTemperature               0.0049563357  0.0004460874  11.111\nPrecipitation            -2.8551621967  0.4218360402  -6.768\nlag1Hour                  0.2436210171  0.0025869509  94.173\nlag3Hours                 0.0713649746  0.0026933121  26.497\nlag1day                   0.1601251478  0.0025028340  63.978\nMed_Inc.x                -0.0000001978  0.0000001388  -1.425\nPercent_Taking_Transit.y -0.0038654165  0.0005089668  -7.595\nPercent_White.y           0.0040303496  0.0002570674  15.678\n                                     Pr(&gt;|t|)    \n(Intercept)              &lt; 0.0000000000000002 ***\nas.factor(hour)1                      0.29278    \nas.factor(hour)2                      0.09733 .  \nas.factor(hour)3                      0.00147 ** \nas.factor(hour)4                      0.00148 ** \nas.factor(hour)5                      0.02201 *  \nas.factor(hour)6           0.0000129349979040 ***\nas.factor(hour)7         &lt; 0.0000000000000002 ***\nas.factor(hour)8         &lt; 0.0000000000000002 ***\nas.factor(hour)9                      0.13160    \nas.factor(hour)10                     0.46468    \nas.factor(hour)11                     0.02584 *  \nas.factor(hour)12          0.0000022655351488 ***\nas.factor(hour)13          0.0000988606650017 ***\nas.factor(hour)14          0.0000079719791901 ***\nas.factor(hour)15        &lt; 0.0000000000000002 ***\nas.factor(hour)16        &lt; 0.0000000000000002 ***\nas.factor(hour)17        &lt; 0.0000000000000002 ***\nas.factor(hour)18          0.0000000418309382 ***\nas.factor(hour)19                     0.99959    \nas.factor(hour)20                     0.00959 ** \nas.factor(hour)21                     0.02467 *  \nas.factor(hour)22                     0.19694    \nas.factor(hour)23                     0.03075 *  \ndotw_simple2                          0.66415    \ndotw_simple3                          0.71787    \ndotw_simple4               0.0000558574431824 ***\ndotw_simple5               0.0000000000120633 ***\ndotw_simple6                          0.01869 *  \ndotw_simple7                          0.00313 ** \nTemperature              &lt; 0.0000000000000002 ***\nPrecipitation              0.0000000000130822 ***\nlag1Hour                 &lt; 0.0000000000000002 ***\nlag3Hours                &lt; 0.0000000000000002 ***\nlag1day                  &lt; 0.0000000000000002 ***\nMed_Inc.x                             0.15411    \nPercent_Taking_Transit.y   0.0000000000000311 ***\nPercent_White.y          &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.324 on 114624 degrees of freedom\n  (212350 observations deleted due to missingness)\nMultiple R-squared:  0.2167,    Adjusted R-squared:  0.2164 \nF-statistic: 856.9 on 37 and 114624 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#model-4-add-station-fixed-effects",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#model-4-add-station-fixed-effects",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Model 4: Add Station Fixed Effects",
    "text": "Model 4: Add Station Fixed Effects\n\n\nCode\nmodel4 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    start_station,\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n\n\nModel 4 R-squared: 0.2465125 \n\n\nCode\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n\n\nModel 4 Adj R-squared: 0.2447868"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#model-5-add-rush-hour-interaction",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#model-5-add-rush-hour-interaction",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Model 5: Add Rush Hour Interaction",
    "text": "Model 5: Add Rush Hour Interaction\n\n\nCode\nmodel5 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + month +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    start_station +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n\n\nModel 5 R-squared: 0.2524079 \n\n\nCode\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n\n\nModel 5 Adj R-squared: 0.2506826"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#calculate-predictions-and-mae",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#calculate-predictions-and-mae",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Calculate Predictions and MAE",
    "text": "Calculate Predictions and MAE\n\n\nCode\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest &lt;- test %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) &lt;- contr.treatment(7)\n\ntest &lt;- test %&gt;%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results &lt;- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nMean Absolute Error by Model (Test Set)\n\n\nModel\nMAE (trips)\n\n\n\n\n1. Time + Weather\n0.62\n\n\n2. + Temporal Lags\n0.44\n\n\n3. + Demographics\n0.67\n\n\n4. + Station FE\n0.69\n\n\n5. + Rush Hour Interaction\n0.72"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#visualize-model-comparison",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#visualize-model-comparison",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Visualize Model Comparison",
    "text": "Visualize Model Comparison\n\n\nCode\nggplot(mae_results, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#observed-vs.-predicted",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#observed-vs.-predicted",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Observed vs. Predicted",
    "text": "Observed vs. Predicted\n\n\nCode\ntest &lt;- test %&gt;%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour &lt; 7 ~ \"Overnight\",\n      hour &gt;= 7 & hour &lt; 10 ~ \"AM Rush\",\n      hour &gt;= 10 & hour &lt; 15 ~ \"Mid-Day\",\n      hour &gt;= 15 & hour &lt;= 18 ~ \"PM Rush\",\n      hour &gt; 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#spatial-error-patterns",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#spatial-error-patterns",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Spatial Error Patterns",
    "text": "Spatial Error Patterns\n\n\nCode\n# Calculate MAE by station\nstation_errors &lt;- test %&gt;%\n  group_by(start_station, start_lat.x, start_lon.y) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Calculate station errors\nstation_errors &lt;- test %&gt;%\n  filter(!is.na(pred2)) %&gt;%\n  group_by(start_station, start_lat.x, start_lon.y) %&gt;%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE\\n(trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Higher in Center City\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand\np2 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg\\nDemand\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\",\n       subtitle = \"Trips per station-hour\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine with better layout\nlibrary(gridExtra)\nlibrary(grid)\ngrid.arrange(\n  p1, p2, \n  ncol = 2,\n  top = textGrob(\n    \"Model 2 Performance: Errors vs. Demand Patterns\",\n    gp = gpar(fontsize = 16, fontface = \"bold\")\n  )\n)\n\n\n\n\n\n\n\n\n\nCode\n# Map 1: Prediction Errors\np1 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n\n\n\n\n\n\n\n\n\nCode\np1"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#temporal-error-patterns",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#temporal-error-patterns",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Temporal Error Patterns",
    "text": "Temporal Error Patterns\n\n\nCode\n# MAE by time of day and day type\ntemporal_errors &lt;- test %&gt;%\n  group_by(time_of_day, weekend) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#errors-and-demographics",
    "href": "Assignments/Assignment_5/Yu_Zheng_Assignment5.html#errors-and-demographics",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Errors and Demographics",
    "text": "Errors and Demographics\n\n\nCode\nstation_errors_demo &lt;- station_errors %&gt;%\n  mutate(start_station = as.character(start_station)) %&gt;%\n  left_join(\n    station_attributes %&gt;%\n      mutate(start_station = as.character(start_station)) %&gt;%\n      select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %&gt;%\n  filter(!is.na(Med_Inc))\n\n\n# Create plots\np1 &lt;- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 &lt;- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 &lt;- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)"
  },
  {
    "objectID": "Assignments/Midterm/Yu_Xiao_Appendix.html",
    "href": "Assignments/Midterm/Yu_Xiao_Appendix.html",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "",
    "text": "Phase 1: Data Preparation (Technical Appendix)\n\nProperty Sales Data Cleaning\nWe filtered our property sales data to only show sales for the past two years to ensure up-to-date insights. We restricted the dataset to \"SINGLE FAMILY\" and \"MULTI FAMILY\" homes, excluding apartments and non-residential sales to ensure comparability, relevancy, and accuracy in our prediction models. Various apartments contained within the same building were found to be listed with a comprehensive sales price of the entire building which posed the threat of distorting our analysis. Observation values that were regarded as data entry errors or outliers were also removed. Properties with zero bathrooms, bedrooms, or livable area were removed as well as properties with a sales price below $10,000 and above $1,000,000.\n\n\nVariable Selection and Variable Data Cleaning\nWe retained select structural attributes from our original dataset that we considered theoretically relevant to predicting sale price: number of bedrooms, number of bathrooms, total livable area, year the home was built, exterior condition, availability of garage spaces, and finally, the dependent variable, sale price.\nCode for Property Sales Data Cleaning and Variable Selection\n\n\nCode\n#load necessary libraries \nlibrary(sf)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(caret)\nlibrary(knitr)\nlibrary(scales)\n\n#load census key for later use\ncensus_api_key(\"42bf8a20a3df1def380f330cf7edad0dd5842ce6\")\n\n#save data in url \nurl &lt;- \"https://phl.carto.com/api/v2/sql?filename=opa_properties_public&format=geojson&skipfields=cartodb_id&q=SELECT+*+FROM+opa_properties_public\"\n\n#suppress warnings for clarity and read data as spatial object\nsuppressWarnings({\n property_data &lt;- st_read(url)\n})\n\n#clean data\nparcel_data &lt;- property_data%&gt;%\n  select(location, #load columns that could potentially be used as predictors \n         category_code_description, #maybe garage_spaces and central air too?\n         number_of_bedrooms,\n         number_of_bathrooms, \n         total_livable_area,\n         year_built,\n         exterior_condition,\n         garage_spaces,\n         sale_price,\n         sale_date)%&gt;%\n  filter(category_code_description %in% \n  c(\"SINGLE FAMILY\",\"MULTI FAMILY\"))%&gt;% #no apartments, sales price of building\n  drop_na(number_of_bedrooms, #remove anomalies like houses with no rooms\n          number_of_bathrooms,\n          total_livable_area,\n          sale_price,\n          year_built) %&gt;%\n  filter(number_of_bedrooms&gt;0, \n         number_of_bathrooms&gt;0,\n         total_livable_area&gt;0, \n         sale_price&gt;=10000, sale_price &lt;=1000000)%&gt;% #remove very low/high prices\n  mutate(sale_year = str_remove(sale_date, \"-.*\"))%&gt;%   \n  filter(sale_year %in% c(\"2023\",\"2024\"))%&gt;% #limit to only 2023 and 2024\n  mutate(year_built = as.numeric(year_built))%&gt;%\n  mutate(Age = 2025 - year_built)%&gt;% filter(Age &lt;2000)%&gt;% #create a age column\n  filter(exterior_condition != 0) %&gt;% #create exterior condition binary  \n  mutate(\n    exterior_good = case_when(\n      exterior_condition &gt;= 1 & exterior_condition &lt;= 5 ~ 1,\n      exterior_condition &gt;= 6 & exterior_condition &lt;= 9 ~ 0,\n      TRUE ~ NA_real_ \n    )\n  )\n\n\n\n\n\nTable 1. Property Dataset Dimensions Before and After Cleaning and Selecting Varaiables\n\n\nDataset\nRows\nColumns\n\n\n\n\nRaw Property Data\n583812\n79\n\n\nCleaned Parcel Data\n25486\n14\n\n\n\n\n\nWe derived our socioeconomic predictors from tract-level census data provided by the 2022 American Community Survey: median income, number with at least a bachelor’s degree, total number of those with at least a bachelor’s degree, number of those living in poverty, and total of those living in poverty. Census tracts with missing median income or zero reported as the median income were removed as it often indicated missing or zero values for other key predictors. We also mutated our census dataset to include two more columns, the percentage of people with bachelors and percentage of people in poverty, in order to standardize the observations across varying tract population sizes. Spatial data of the census tracts was also loaded in order to join our census variables to our parcel-level property data.\nCode for Census Data Cleaning and Variable Selection\n\n\nCode\n#load data about poverty(counts and total), bachelors(counts and total), and income \ncensus_data &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  county = \"Philadelphia\",\n  variables = c(\n    median_income = \"B19013_001\",\n    num_with_bach = \"B15003_022\",\n    bachelors_total =\"B15003_001\",\n    num_in_poverty = \"B17001_002\",\n    poverty_total =\"B17001_001\"\n  ),\n  year = 2022,\n  output = \"wide\"\n)\n\n#create percentage columns for bachelors and poverty \nphilly_census &lt;- census_data%&gt;%\n  mutate(\n    percentage_bach = num_with_bachE / bachelors_totalE,\n    percentage_pov =  num_in_povertyE / poverty_totalE\n  )\n\n#remove data errors or incomplete fields \nphilly_census &lt;- philly_census%&gt;%\n  drop_na(median_incomeE)%&gt;%\n  filter(median_incomeE&gt;0)\n\n\n#spatial census data \nphiladelphia_tracts &lt;- tracts(\n  state = \"PA\",\n  county = \"Philadelphia\",\n  cb = TRUE,\n  year = 2022\n)\n\n#join census data and tract geometry to PARCEL data\nparcel_data &lt;- parcel_data %&gt;%\n  st_transform(st_crs(philadelphia_tracts))%&gt;%\n  st_join(philadelphia_tracts, join = st_within)%&gt;%\n  left_join(philly_census, by = \"GEOID\")\n\n\n\n\n\nTable 2. Census Dataset Dimensions Before and After Cleaning and Selecting Varaiables\n\n\nDataset\nRows\nColumns\n\n\n\n\nCensus Data\n408\n12\n\n\nCleaned Census Data\n383\n14\n\n\n\n\n\nTo further contextualize house prices, we loaded spatial datasets including the locations of colleges and universities, 2024 crime incidents, and Philadelphia neighborhood boundaries. These datasets were loaded with the intention of engineering spatial features such as proximity measurements and neighborhood stratification in order to account for spatial patterns and potential interactive spatial effects within our predictive model. Only the 2024 crime incidents needed to be cleaned due to the missing geometries that would prevent spatial analysis.\nCode for Spatial Data Cleaning and Variable Selection\n\n\nCode\n#load university data\nuniversity_data &lt;- st_read(\"/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Midterm/data/Universities_Colleges.geojson\")\n\n#load 2024 crime incident data\norg_crime_data &lt;-st_read(\"/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Midterm/data/incidents_part1_part2.shp\")\n\n#removed crime incidents with no geometry \ncrime_data &lt;- org_crime_data %&gt;%\n  filter(!st_is_empty(geometry))\n\n#load neighborhood data \nneighborhoods &lt;- st_read(\"/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Midterm/data/philadelphia-neighborhoods.shp\")\n\n\n\n\n\nTable 3. Spatial Datasets Dimensions Before and After Cleaning\n\n\nDataset\nRows\nColumns\n\n\n\n\nCrime Data\n160388\n14\n\n\nCleaned Crime Data\n153644\n14\n\n\n\n\n\n\n\n\nPhase 2: Exploratory Data Analysis\n\nDistribution of Sales Prices\nWe created a histogram to visualize the distribution of home sale prices in Philadelphia. The resulting graph revealed a positively skewed distribution, indicating that most properties were sold at lower price points, primarily between $150,000 and $350,000, while a small number of high-value homes sold for up to $1,000,000. As it pertains to our predictive model, this skewness suggests that a log-transformation of sale prices may be necessary to normalize the distribution of values and potentially improve model performance. It also indicates the need for diagnostic checks to determine the impact of outliers in order to ensure that the few high-priced homes do not distort our model estimates.\n\n\n\n\n\n\n\n\n\n\n\nGeographic Distribution of Sales Price\nWe also explored the geographic distribution of sales prices across the neighborhoods in Philadelphia. We calculated the median sales price by each Philadelphia neighborhood. The map showed distinct spatial patterns with higher prices above $400,000, represented by lighter colors, being concentrated in neighborhoods such as Center City, University City, and parts of Northwest Philadelphia.\n\n\n\n\n\n\n\n\n\n\n\n\nPhase 3: Feature Engineering (Technical Appendix)\nSeveral geographic features were created. First, a crime buffer analysis was conducted by generating 600-foot buffers around each parcel. A k-nearest neighbor (kNN) distance measure was calculated between parcels and nearby universities. And a wealthy neighborhood classification was introduced by spatially joining parcels with neighborhood boundaries and identifying areas with a median sale price above $275,500 as “Wealthy.” This categorical factor was then used to construct an interaction term with living area.\n\n\nCode\n#set crs for distance calculations\ncrime_proj &lt;- st_transform(crime_data, 3365)\nparcel_proj &lt;- st_transform(parcel_data, 3365)\nuniversity_proj &lt;- st_transform(university_data, 3365)\n\n#proximity (within 600 feet) to violent crime\n\nparcel_buffers&lt;- st_buffer(parcel_proj, dist=600)\n\nviolent_crimes &lt;- c(\"Homicide - Criminal\",\"Aggravated Assault No Firearm\",\"Theft from Vehicle\",\"Robbery Firearm\",\"Aggravated Assault Firearm\")\n\nviolent_proj &lt;- crime_proj %&gt;%\n  filter(text_gener %in% violent_crimes)\n    \nviolent_crime_counts &lt;- st_intersects(parcel_buffers, violent_proj)\nviolent_crime_counts &lt;- lengths(violent_crime_counts)\nparcel_data$violent_crime_600ft &lt;- violent_crime_counts\n\n\n\n\nCode\n#knn to university \n\nuniversity_proj &lt;- st_transform(university_data, 3365) #projection changed in order to calculate distance \n\n\ndist_matrix &lt;- st_distance(parcel_proj, university_proj)\n\n\nget_knn_distance &lt;- function(dist_matrix, k) {\n  apply(dist_matrix, 1, function(distances) {\n    mean(as.numeric(sort(distances)[1:k]))\n  })\n}\n\nparcel_data$college_nn1 &lt;- get_knn_distance(dist_matrix, k = 1)\nparcel_data$college_nn3 &lt;- get_knn_distance(dist_matrix, k = 3)\nparcel_data$college_nn5 &lt;- get_knn_distance(dist_matrix, k = 5)\n\n#determine which nearest neighbor is correlated the most with sales price \n\nparcel_data %&gt;% \n  st_drop_geometry() %&gt;%\n  select(sale_price, college_nn1, college_nn3, college_nn5) %&gt;%\n  cor(use = \"complete.obs\") %&gt;%\n  as.data.frame() %&gt;%\n  select(sale_price)\n\n\n\n\nCode\n# wealthy neighborhood interaction effects\nparcel_proj &lt;- st_transform(parcel_data, 3365)\nneighborhood_proj &lt;-st_transform(neighborhoods, 3365)\n\n\nwealthy_neighborhoods &lt;- parcel_with_neighborhood%&gt;%\n  filter(median_price &gt;= 275500)%&gt;%\n  pull(NAME)\n\n\nparcel_data &lt;- parcel_proj %&gt;%\n  st_join(neighborhood_proj, join = st_within) %&gt;%\n  mutate(\n    wealthy_neighborhood = ifelse(NAME %in% wealthy_neighborhoods, \"Wealthy\", \"Not Wealthy\"),\n    wealthy_neighborhood = as.factor(wealthy_neighborhood)\n  )\n\n\n\n\nPhase 4: Model Building\n\n\nCode\nparcel_data &lt;- parcel_data %&gt;% filter(!if_any(everything(), is.na))\n\n#create log sale price and log livable area for modeling\nparcel_data &lt;- parcel_data%&gt;%\n  mutate(log_sale_price = log(sale_price))\n\nparcel_data &lt;- parcel_data%&gt;%\n  mutate(log_livable_area = log(total_livable_area))\n\n#structural features only \nmodel1 &lt;- lm(log_sale_price ~ number_of_bathrooms + number_of_bedrooms + log_livable_area + garage_spaces + Age + I(Age^2) + exterior_good, data = parcel_data)\n\nsummary(model1)\n\n#structural + census \nmodel2 &lt;- lm(log_sale_price ~ number_of_bathrooms + number_of_bedrooms + log_livable_area  + garage_spaces + Age + I(Age^2) + exterior_good + median_incomeE + percentage_bach + percentage_pov, data = parcel_data)\n\nsummary(model2)\n\n#structural + census + spatial \nmodel3 &lt;- lm(log_sale_price ~ number_of_bathrooms+ garage_spaces + Age + I(Age^2) + exterior_good + log_livable_area + median_incomeE +percentage_bach + percentage_pov + college_nn1 + violent_crime_600ft, data = parcel_data)\n\nsummary(model3)\n\n#structural + census + spatial + interactions \nmodel4 &lt;- lm(log_sale_price ~ number_of_bathrooms+ garage_spaces + Age + I(Age^2) + exterior_good + log_livable_area * wealthy_neighborhood + median_incomeE +percentage_bach + percentage_pov + college_nn1 + violent_crime_600ft, data = parcel_data)\n\nsummary(model4)\n\n\nVariables Intepretation:\nModel1. Structural features only: number of bathrooms, livable area (logged), garage_spaces, House_age (Quadratic Effect), exterior condition\nModel2. + Census variables: + median_income, percentage of bachelor, percentage of poverty\nModel3. + Spatial features: + nearest college, number of nearby crime\nModel4. + Interactions and fixed effects: + neighborhood wealthy (interact with livable area)\n\n\nPhase 5: Model Validation\n\n\nCode\nparcel_data &lt;- parcel_data %&gt;% filter(!if_any(everything(), is.na))\n\nctrl &lt;- trainControl(\n  method = \"cv\",\n  number = 10  # 10-fold CV\n)\n\nmodel_cv1 &lt;-  train(log_sale_price ~ \n                      number_of_bathrooms +\n                      number_of_bedrooms +\n                      log_livable_area +\n                      garage_spaces + \n                      Age + I(Age^2) + \n                      exterior_good,\n                     data = parcel_data,\n                       method = \"lm\",\n                       trControl = ctrl)\n\nmodel_cv1\n\n\nLinear Regression \n\n24612 samples\n    7 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 22151, 22151, 22152, 22151, 22151, 22152, ... \nResampling results:\n\n  RMSE       Rsquared  MAE      \n  0.6065904  0.349676  0.4524835\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nCode\nmodel_cv2 &lt;- train(log_sale_price ~ \n                     number_of_bathrooms +\n                     number_of_bedrooms + \n                     log_livable_area  +\n                     garage_spaces +\n                     Age + I(Age^2) + \n                     exterior_good + \n                     median_incomeE + \n                     percentage_bach + \n                     percentage_pov,\n                     data = parcel_data,\n                      method = \"lm\",\n                      trControl = ctrl\n)\n\nmodel_cv2\n\n\nLinear Regression \n\n24612 samples\n   10 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 22151, 22150, 22150, 22151, 22150, 22152, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.4940543  0.5688232  0.3526702\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nCode\nmodel_cv3 &lt;- train(log_sale_price ~ \n                     number_of_bathrooms +\n                     garage_spaces +\n                     Age + I(Age^2) + \n                     exterior_good + \n                     log_livable_area +\n                     median_incomeE + \n                     percentage_bach +\n                     percentage_pov + \n                     college_nn1 + \n                     violent_crime_600ft,\n                     data = parcel_data,\n                    method = \"lm\",\n                    trControl = ctrl\n)\n\nmodel_cv3\n\n\nLinear Regression \n\n24612 samples\n   11 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 22151, 22150, 22152, 22151, 22150, 22150, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.4884946  0.5783064  0.3469928\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nCode\nmodel_cv4 &lt;- train(log_sale_price ~ \n                     number_of_bathrooms+\n                     garage_spaces + \n                     Age + I(Age^2) +\n                     exterior_good + \n                     log_livable_area * wealthy_neighborhood + \n                     median_incomeE +\n                     percentage_bach + \n                     percentage_pov +\n                     college_nn1 + \n                     violent_crime_600ft,\n                     data = parcel_data,\n                      method = \"lm\",\n                    trControl = ctrl\n)\n\nmodel_cv4\n\n\nLinear Regression \n\n24612 samples\n   12 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 22151, 22151, 22150, 22151, 22152, 22152, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.4814922  0.5903087  0.3400258\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\ncomparison table\n## Model Performance Improves with Each Layer\n\n| Model | CV RMSE (log) | R² |\n|-------|---------------|-----|\n| Structural Only | 0.61 | 0.35 |\n| + Census | 0.50 | 0.57 |\n| + Spatial | 0.49 | 0.58 |\n| + Interactions/FE | 0.48 | 0.59 |\n\n\nCode\nmodel_data_used &lt;- model.frame(model4)\n\nmodel_data_used$Predicted &lt;- model4$fitted.values\n\nggplot(model_data_used, aes(x = log_sale_price, y = Predicted)) +\n  geom_point(alpha = 0.4, color = \"steelblue\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Predicted vs Actual (Model 4)\",\n    x = \"Actual log(sale_price)\",\n    y = \"Predicted log(sale_price)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPhase 6: Model Diagnostics (Technical Appendix Only)\n\n\n\n\n\n\n\n\n\nIntepretation:The residuals are spatially clustered: blue areas indicate underestimation (actual prices higher than predicted), while red areas indicate overestimation.This pattern suggests spatial dependence remains in the model, meaning location-specific effects are not fully captured.\n\n\n\n\n\n\n\n\n\nInterpretation:Most points follow the 45° line, but deviations in the tails show that extreme residuals depart from normality. This indicates the model fits the majority of data well but may underfit high- and low-end price extremes.\n\n\n\n\n\n\n\n\n\nInterpretation:Most observations have Cook’s distance near zero, meaning no influential outliers dominate the model.\n\n\nPhase 7: Conclusions & Recommendations\nModel Performance:The final model achieves an adjusted R² of 0.591, explaining nearly 59% of the variation in housing prices across Philadelphia. This represents a major improvement over the baseline Model 1 (R² = 0.35), demonstrating the importance of adding neighborhood and spatial characteristics to structural property features.\nMost Significant Variables:Among all predictors, living area is the strongest driver of price, followed by the number of bathrooms and garage spaces, which add substantial functional value. Exterior condition, median income, and educational attainment also contribute positively, reflecting the combined effects of property quality and neighborhood socioeconomic context. In contrast, poverty rate and crime density show negative associations, while the interaction between living area and wealthy neighborhoods suggests that larger homes yield diminishing marginal returns in already high-value areas.\nEquity Concern:The model’s predictive accuracy varies by location. It performs best in mid-range housing markets, while areas such as Nicetown, Fairhill, and Upper Kensington show higher residual errors. These disparities indicate that valuation patterns are still influenced by historical and spatial inequalities, raising potential equity concerns if such models were used in policy or taxation contexts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\n\nMy name is Xiao Yu, and you may call me Cathy. I am currently a Master of Urban Spatial Analytics student at the University of Pennsylvania. I completed my undergraduate degree in Urban Studies and Planning at the University of Sheffield, where I developed a strong interest in spatial analysis and urban design.\n\n\n\n\n\nEmail: [uxiaoo22@upenn.edu]\nGitHub: [@uxiaoo22]"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "My name is Xiao Yu, and you may call me Cathy. I am currently a Master of Urban Spatial Analytics student at the University of Pennsylvania. I completed my undergraduate degree in Urban Studies and Planning at the University of Sheffield, where I developed a strong interest in spatial analysis and urban design."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: [uxiaoo22@upenn.edu]\nGitHub: [@uxiaoo22]"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We observe data such as counties, income, population, and education.\nWe believe there’s some relationship between these variables.\nStatistical learning refers to the set of methods for estimating that relationship."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#the-general-problem",
    "href": "weekly-notes/week-05-notes.html#the-general-problem",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We observe data such as counties, income, population, and education.\nWe believe there’s some relationship between these variables.\nStatistical learning refers to the set of methods for estimating that relationship."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#formalizing-the-relationship",
    "href": "weekly-notes/week-05-notes.html#formalizing-the-relationship",
    "title": "Week 5 Notes",
    "section": "Formalizing the Relationship",
    "text": "Formalizing the Relationship\nFor any quantitative response Y and predictors X₁, X₂, … Xₚ:\n\\[\nY = f(X) + \\epsilon\n\\]\nWhere: - f = the systematic information X provides about Y\n- ε = random error (irreducible)"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#what-is-f",
    "href": "weekly-notes/week-05-notes.html#what-is-f",
    "title": "Week 5 Notes",
    "section": "What is f?",
    "text": "What is f?\nf represents the true relationship between predictors and the outcome.\nIt’s fixed but unknown — the core object we aim to estimate.\nExample: - Y = median income\n- X = population, education, poverty rate\n- f = how these factors systematically relate to income"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#why-estimate-f",
    "href": "weekly-notes/week-05-notes.html#why-estimate-f",
    "title": "Week 5 Notes",
    "section": "Why Estimate f?",
    "text": "Why Estimate f?\nTwo key purposes of statistical learning:\n\nPrediction\n\nEstimate Y for new or missing observations\n\nFocus on accuracy rather than explanation\n\nInference\n\nUnderstand how X affects Y\n\nIdentify which predictors are most important\n\nFocus on interpreting relationships"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#how-do-we-estimate-f",
    "href": "weekly-notes/week-05-notes.html#how-do-we-estimate-f",
    "title": "Week 5 Notes",
    "section": "How Do We Estimate f?",
    "text": "How Do We Estimate f?\n\nParametric Methods\n\nAssume a specific form for f (e.g., linear)\nReduce the problem to estimating parameters\nEasier to interpret but limited by assumptions\n\n\n\nNon-Parametric Methods\n\nMake fewer assumptions about f\nMore flexible but harder to interpret and require more data\n\nKey difference:\nParametric = structure first, then estimate\nNon-parametric = data first, let the shape emerge"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#parametric-approach-linear-regression",
    "href": "weekly-notes/week-05-notes.html#parametric-approach-linear-regression",
    "title": "Week 5 Notes",
    "section": "Parametric Approach: Linear Regression",
    "text": "Parametric Approach: Linear Regression\nWe assume a linear relationship between X and Y:\n\\[\nY ≈ β₀ + β₁X₁ + β₂X₂ + … + βₚXₚ\n\\]\nOur goal is to estimate the coefficients (β’s) using Ordinary Least Squares (OLS) — the method that minimizes prediction error."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#why-linear-regression",
    "href": "weekly-notes/week-05-notes.html#why-linear-regression",
    "title": "Week 5 Notes",
    "section": "Why Linear Regression?",
    "text": "Why Linear Regression?\nAdvantages - Simple and interpretable\n- Performs surprisingly well for many real-world problems\n- Foundation for more advanced methods\nLimitations - Assumes linearity and independence\n- Sensitive to outliers\n- Requires diagnostic checks"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#prediction-vs-inference",
    "href": "weekly-notes/week-05-notes.html#prediction-vs-inference",
    "title": "Week 5 Notes",
    "section": "Prediction vs Inference",
    "text": "Prediction vs Inference\nInference asks:\n&gt; Does education significantly affect income?\nFocus: understanding mechanisms, testing hypotheses.\nPrediction asks:\n&gt; What will the income be for this county?\nFocus: accuracy and reliability of forecasts.\nThis session emphasizes prediction, while recognizing inference as a complementary goal."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#example-applications",
    "href": "weekly-notes/week-05-notes.html#example-applications",
    "title": "Week 5 Notes",
    "section": "Example Applications",
    "text": "Example Applications\n\nPrediction\nUsed by governments to: - Estimate income for areas with incomplete census data\n- Forecast population growth or public service needs\nAccurate predictions improve decision-making even without full causal understanding.\n\n\nInference\nUsed by researchers to: - Understand gentrification and inequality\n- Identify which neighborhood characteristics explain income changes\n- Evaluate the impact of education or policy interventions"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#connection-to-week-2-algorithmic-bias",
    "href": "weekly-notes/week-05-notes.html#connection-to-week-2-algorithmic-bias",
    "title": "Week 5 Notes",
    "section": "Connection to Week 2: Algorithmic Bias",
    "text": "Connection to Week 2: Algorithmic Bias\nStatistical accuracy does not guarantee ethical fairness.\nFor example, a healthcare model once predicted medical needs using costs as a proxy — resulting in racial bias.\nThis highlights that fit metrics (like R²) cannot replace ethical evaluation."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#interpreting-regression-results",
    "href": "weekly-notes/week-05-notes.html#interpreting-regression-results",
    "title": "Week 5 Notes",
    "section": "Interpreting Regression Results",
    "text": "Interpreting Regression Results\nWhen fitting a simple model of median income vs. population: - The intercept (~$62,855) represents baseline income when population = 0 (not practically meaningful).\n- The slope (~$0.02 per person) means that, on average, income rises $20 for every additional 1,000 people.\n- The relationship is statistically significant (p &lt; 0.001).\n- R² = 0.21 → about 21% of variation in income is explained by population.\nThis shows population helps explain income differences, though other variables matter too."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#model-evaluation-key-ideas",
    "href": "weekly-notes/week-05-notes.html#model-evaluation-key-ideas",
    "title": "Week 5 Notes",
    "section": "Model Evaluation: Key Ideas",
    "text": "Model Evaluation: Key Ideas\n\nThe “True” vs. Estimated Relationship\nThe real relationship (f) is unobservable.\nOur model provides an approximation based on available data.\nEach dataset produces slightly different estimates — uncertainty is measured by standard errors.\n\n\nStatistical Significance\n\nNull hypothesis (H₀): β₁ = 0 (no relationship)\n\nA small p-value means the observed effect is unlikely to be random → the relationship is real.\n\n\n\nR² and Model Fit\n\nR² measures the share of variance in Y explained by X.\n\nIt indicates strength, not correctness, of the model.\n\nA model with low R² may still provide useful predictions."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#overfitting-and-model-validation",
    "href": "weekly-notes/week-05-notes.html#overfitting-and-model-validation",
    "title": "Week 5 Notes",
    "section": "Overfitting and Model Validation",
    "text": "Overfitting and Model Validation\nA model can perform well on training data but fail on new data.\nTo avoid this, data are split into training and testing sets.\n\nTraining error measures fit on known data.\n\nTesting error measures predictive performance on unseen data.\n\nGood models balance bias and variance, avoiding both underfitting and overfitting."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#model-assumptions-and-diagnostics",
    "href": "weekly-notes/week-05-notes.html#model-assumptions-and-diagnostics",
    "title": "Week 5 Notes",
    "section": "Model Assumptions and Diagnostics",
    "text": "Model Assumptions and Diagnostics\n\nLinearity — relationship between variables should be roughly linear.\n\nCheck with residual plots.\n\nCurvature suggests model misspecification.\n\nConstant Variance (Homoscedasticity) — residual spread should be even.\n\nViolations make p-values unreliable.\n\nSolutions: transform Y, add predictors, or use robust errors.\n\nNormality of Residuals — important for hypothesis testing.\n\nUse Q–Q plots to assess normal distribution.\n\nNo Multicollinearity — predictors should not be highly correlated.\n\nHigh correlation makes coefficients unstable.\n\nNo Influential Outliers — avoid points that disproportionately shape the regression line.\n\nDetect using Cook’s Distance."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#improving-the-model",
    "href": "weekly-notes/week-05-notes.html#improving-the-model",
    "title": "Week 5 Notes",
    "section": "Improving the Model",
    "text": "Improving the Model\n\nAdding Predictors\nIncluding education and poverty rates typically improves explanatory power.\nExample: Adjusted R² increases to around 0.57, suggesting stronger fit.\n\n\nTransformations\nIf relationships are curved, use log transformations.\nFor instance, a log–population model may capture diminishing returns to scale.\n\n\nCategorical Variables\nInclude binary variables such as metro vs. non-metro areas.\nIn one model, metro counties earned about $30,000 more on average."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#summary-the-regression-workflow",
    "href": "weekly-notes/week-05-notes.html#summary-the-regression-workflow",
    "title": "Week 5 Notes",
    "section": "Summary: The Regression Workflow",
    "text": "Summary: The Regression Workflow\n\nUnderstand the conceptual model (f(X)).\n\nVisualize relationships before modeling.\n\nFit the regression model carefully.\n\nEvaluate performance using validation methods.\n\nCheck key assumptions.\n\nRefine with new variables or transformations.\n\nReflect on the ethical and social implications of modeling."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#key-takeaways",
    "href": "weekly-notes/week-05-notes.html#key-takeaways",
    "title": "Week 5 Notes",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nStatistical learning is about estimating f(X), the systematic link between predictors and outcomes.\n\nTwo central goals:\n\nInference – understand relationships\n\nPrediction – forecast new outcomes\n\n\nGood fit ≠ good model: always assess assumptions and fairness.\n\nDiagnostics matter: plots and reasoning reveal what statistics can hide.\n\nEthical awareness is essential in all modeling decisions."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 Notes",
    "section": "",
    "text": "[List main concepts from lecture]\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "title": "Week 4 Notes",
    "section": "",
    "text": "[List main concepts from lecture]\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coding-techniques",
    "href": "weekly-notes/week-04-notes.html#coding-techniques",
    "title": "Week 4 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches]\n[Quarto features learned]"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04-notes.html#questions-challenges",
    "title": "Week 4 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n[What I didn’t fully understand]\n[Areas needing more practice]"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04-notes.html#connections-to-policy",
    "title": "Week 4 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week’s content applies to real policy work]"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04-notes.html#reflection",
    "title": "Week 4 Notes",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting]\n[How I’ll apply this knowledge]"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html",
    "href": "weekly-notes/week-07-notes.html",
    "title": "Xiao Yu - Week 7 Notes: Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "Location variation (North End vs. Roxbury vs. Back Bay)\nProximity to:\n\nDowntown\nWaterfronts\nParks\n\nNeighborhood conditions:\n\nCrime rates\nSchool quality\nSocioeconomic characteristics\n\n\n\n\n\n\n\n\nImportant\n\n\n\nLocation matters!\nA house’s price is determined not only by its physical attributes but by where it is located.\n\n\n\n\n\n\n1,000 sq ft in Back Bay ≠ Roxbury\nSame attributes, very different prices\nReal estate adheres to the principle: “Location, location, location!”\n\n\n\n\n\nAdd spatial features\nUse neighborhood fixed effects\nInclude interaction terms to reflect how variables behave differently across places\n\n\n💡 Key idea: Spatial features allow regression models to capture real-world geography."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#what-does-a-basic-linear-model-ignore",
    "href": "weekly-notes/week-07-notes.html#what-does-a-basic-linear-model-ignore",
    "title": "Xiao Yu - Week 7 Notes: Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "Location variation (North End vs. Roxbury vs. Back Bay)\nProximity to:\n\nDowntown\nWaterfronts\nParks\n\nNeighborhood conditions:\n\nCrime rates\nSchool quality\nSocioeconomic characteristics\n\n\n\n\n\n\n\n\nImportant\n\n\n\nLocation matters!\nA house’s price is determined not only by its physical attributes but by where it is located."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#why-might-it-fail",
    "href": "weekly-notes/week-07-notes.html#why-might-it-fail",
    "title": "Xiao Yu - Week 7 Notes: Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "1,000 sq ft in Back Bay ≠ Roxbury\nSame attributes, very different prices\nReal estate adheres to the principle: “Location, location, location!”"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#how-can-we-improve-the-model",
    "href": "weekly-notes/week-07-notes.html#how-can-we-improve-the-model",
    "title": "Xiao Yu - Week 7 Notes: Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "Add spatial features\nUse neighborhood fixed effects\nInclude interaction terms to reflect how variables behave differently across places\n\n\n💡 Key idea: Spatial features allow regression models to capture real-world geography."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#market-segmentation",
    "href": "weekly-notes/week-07-notes.html#market-segmentation",
    "title": "Xiao Yu - Week 7 Notes: Model Diagnostics & Spatial Autocorrelation",
    "section": "1. Market Segmentation",
    "text": "1. Market Segmentation\nBoston operates as two distinct housing markets: - Luxury market: ~$1081/sq ft - Standard market: ~$96/sq ft"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#affordability-crisis",
    "href": "weekly-notes/week-07-notes.html#affordability-crisis",
    "title": "Xiao Yu - Week 7 Notes: Model Diagnostics & Spatial Autocorrelation",
    "section": "2. Affordability Crisis",
    "text": "2. Affordability Crisis\n\nInteraction effects amplify inequality\nLarger homes in wealthy areas become disproportionately expensive"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#policy-design-implications",
    "href": "weekly-notes/week-07-notes.html#policy-design-implications",
    "title": "Xiao Yu - Week 7 Notes: Model Diagnostics & Spatial Autocorrelation",
    "section": "3. Policy Design Implications",
    "text": "3. Policy Design Implications\n\nProperty taxes should reflect spatial variation\nHousing policy must be neighborhood-specific (not one-size-fits-all)"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#what-this-means-for-housing-prices",
    "href": "weekly-notes/week-07-notes.html#what-this-means-for-housing-prices",
    "title": "Xiao Yu - Week 7 Notes: Model Diagnostics & Spatial Autocorrelation",
    "section": "What This Means for Housing Prices",
    "text": "What This Means for Housing Prices\n\nCrime in nearby streets influences a property’s value more than crime occurring far away.\nProximity to parks and green space increases property desirability.\nImmediate neighborhood conditions largely define the housing market segment a property belongs to.\n\n\n\n\n\n\n\nNote\n\n\n\nSpatial relationship matters because households make decisions based on their local context, not the entire city."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#three-common-validation-approaches",
    "href": "weekly-notes/week-07-notes.html#three-common-validation-approaches",
    "title": "Xiao Yu - Week 7 Notes: Model Diagnostics & Spatial Autocorrelation",
    "section": "Three Common Validation Approaches",
    "text": "Three Common Validation Approaches\n\n\n\n\n\n\n\n\n\nMethod\nDescription\nAdvantages\nLimitations\n\n\n\n\nTrain/Test Split\nSplit data into 80% training, 20% testing\nSimple and fast\nResults depend heavily on single split (unstable)\n\n\nk-Fold CV\nDivide into k subsets; rotate test and train sets\nMore reliable performance estimate\nMore computation\n\n\nLOOCV (Leave-One-Out)\nSpecial case of k-fold CV with k = n\nUses almost all data for training\nExtremely slow for large datasets"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#why-use-cross-validation",
    "href": "weekly-notes/week-07-notes.html#why-use-cross-validation",
    "title": "Xiao Yu - Week 7 Notes: Model Diagnostics & Spatial Autocorrelation",
    "section": "Why Use Cross-Validation?",
    "text": "Why Use Cross-Validation?\n\nMeasures how well the model predicts new, unseen data\nMore objective than in-sample ( R^2 )\nHelps detect overfitting by testing generalizability\n\n\n\n\n\n\n\nImportant\n\n\n\nCross-validation ensures your model performs well in real-world scenarios, not just on the training dataset."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html",
    "href": "weekly-notes/week-06-notes.html",
    "title": "Xiao Yu - Week 6 Notes: Spatial Machine Learning & Advanced Regression",
    "section": "",
    "text": "Location variation (North End vs. Roxbury vs. Back Bay)\nProximity to:\n\nDowntown\nWaterfronts\nParks\n\nNeighborhood conditions:\n\nCrime rates\nSchool quality\nSocioeconomic characteristics\n\n\n\n\n\n\n\n\nImportant\n\n\n\nLocation matters!\nA house’s price is determined not only by its physical attributes but by where it is located.\n\n\n\n\n\n\n1,000 sq ft in Back Bay ≠ Roxbury\nSame attributes, very different prices\nReal estate adheres to the principle: “Location, location, location!”\n\n\n\n\n\nAdd spatial features\nUse neighborhood fixed effects\nInclude interaction terms to reflect how variables behave differently across places\n\n\n💡 Key idea: Spatial features allow regression models to capture real-world geography."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#what-does-a-basic-linear-model-ignore",
    "href": "weekly-notes/week-06-notes.html#what-does-a-basic-linear-model-ignore",
    "title": "Xiao Yu - Week 6 Notes: Spatial Machine Learning & Advanced Regression",
    "section": "",
    "text": "Location variation (North End vs. Roxbury vs. Back Bay)\nProximity to:\n\nDowntown\nWaterfronts\nParks\n\nNeighborhood conditions:\n\nCrime rates\nSchool quality\nSocioeconomic characteristics\n\n\n\n\n\n\n\n\nImportant\n\n\n\nLocation matters!\nA house’s price is determined not only by its physical attributes but by where it is located."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#why-might-it-fail",
    "href": "weekly-notes/week-06-notes.html#why-might-it-fail",
    "title": "Xiao Yu - Week 6 Notes: Spatial Machine Learning & Advanced Regression",
    "section": "",
    "text": "1,000 sq ft in Back Bay ≠ Roxbury\nSame attributes, very different prices\nReal estate adheres to the principle: “Location, location, location!”"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#how-can-we-improve-the-model",
    "href": "weekly-notes/week-06-notes.html#how-can-we-improve-the-model",
    "title": "Xiao Yu - Week 6 Notes: Spatial Machine Learning & Advanced Regression",
    "section": "",
    "text": "Add spatial features\nUse neighborhood fixed effects\nInclude interaction terms to reflect how variables behave differently across places\n\n\n💡 Key idea: Spatial features allow regression models to capture real-world geography."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#market-segmentation",
    "href": "weekly-notes/week-06-notes.html#market-segmentation",
    "title": "Xiao Yu - Week 6 Notes: Spatial Machine Learning & Advanced Regression",
    "section": "1. Market Segmentation",
    "text": "1. Market Segmentation\nBoston operates as two distinct housing markets: - Luxury market: ~$1081/sq ft - Standard market: ~$96/sq ft"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#affordability-crisis",
    "href": "weekly-notes/week-06-notes.html#affordability-crisis",
    "title": "Xiao Yu - Week 6 Notes: Spatial Machine Learning & Advanced Regression",
    "section": "2. Affordability Crisis",
    "text": "2. Affordability Crisis\n\nInteraction effects amplify inequality\nLarger homes in wealthy areas become disproportionately expensive"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#policy-design-implications",
    "href": "weekly-notes/week-06-notes.html#policy-design-implications",
    "title": "Xiao Yu - Week 6 Notes: Spatial Machine Learning & Advanced Regression",
    "section": "3. Policy Design Implications",
    "text": "3. Policy Design Implications\n\nProperty taxes should reflect spatial variation\nHousing policy must be neighborhood-specific (not one-size-fits-all)"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#what-this-means-for-housing-prices",
    "href": "weekly-notes/week-06-notes.html#what-this-means-for-housing-prices",
    "title": "Xiao Yu - Week 6 Notes: Spatial Machine Learning & Advanced Regression",
    "section": "What This Means for Housing Prices",
    "text": "What This Means for Housing Prices\n\nCrime in nearby streets influences a property’s value more than crime occurring far away.\nProximity to parks and green space increases property desirability.\nImmediate neighborhood conditions largely define the housing market segment a property belongs to.\n\n\n\n\n\n\n\nNote\n\n\n\nSpatial relationship matters because households make decisions based on their local context, not the entire city."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#three-common-validation-approaches",
    "href": "weekly-notes/week-06-notes.html#three-common-validation-approaches",
    "title": "Xiao Yu - Week 6 Notes: Spatial Machine Learning & Advanced Regression",
    "section": "Three Common Validation Approaches",
    "text": "Three Common Validation Approaches\n\n\n\n\n\n\n\n\n\nMethod\nDescription\nAdvantages\nLimitations\n\n\n\n\nTrain/Test Split\nSplit data into 80% training, 20% testing\nSimple and fast\nResults depend heavily on single split (unstable)\n\n\nk-Fold CV\nDivide into k subsets; rotate test and train sets\nMore reliable performance estimate\nMore computation\n\n\nLOOCV (Leave-One-Out)\nSpecial case of k-fold CV with k = n\nUses almost all data for training\nExtremely slow for large datasets"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#why-use-cross-validation",
    "href": "weekly-notes/week-06-notes.html#why-use-cross-validation",
    "title": "Xiao Yu - Week 6 Notes: Spatial Machine Learning & Advanced Regression",
    "section": "Why Use Cross-Validation?",
    "text": "Why Use Cross-Validation?\n\nMeasures how well the model predicts new, unseen data\nMore objective than in-sample ( R^2 )\nHelps detect overfitting by testing generalizability\n\n\n\n\n\n\n\nImportant\n\n\n\nCross-validation ensures your model performs well in real-world scenarios, not just on the training dataset."
  }
]